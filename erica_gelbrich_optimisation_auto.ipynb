{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae33f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_L    = torch.from_numpy(mu_U_ll_hat)\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat)\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "# Given estimates (mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "hat_mu_L    =  torch.from_numpy(mu_U_ll_hat) \n",
    "hat_Sigma_L =  torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "hat_mu_H    =  torch.from_numpy(mu_U_hl_hat)\n",
    "hat_Sigma_H =  torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "lambda_L =.2\n",
    "lambda_H =.3\n",
    "eta      = .01\n",
    "max_iter = 10\n",
    "\n",
    "Sigma_L = hat_Sigma_L\n",
    "Sigma_H = hat_Sigma_H\n",
    "\n",
    "\n",
    "T = torch.exp(torch.from_numpy(np.random.randn(2, 3)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb5e43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary functions using PyTorch for automatic differentiation\n",
    "def F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "    term1 = 0\n",
    "    term2 = 0\n",
    "    term3 = 0\n",
    "\n",
    "    # Loop to compute the sum of terms\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  # Convert to float32\n",
    "        V_i = T @ L_i  # Matrix multiplication, ensure V_i is float32\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  # Convert to float32\n",
    "\n",
    "        term1 += torch.norm(torch.matmul(V_i, mu_L) - torch.matmul(H_i, mu_H))**2 + torch.trace(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))) + torch.trace(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)))\n",
    "\n",
    "    term2 = lambda_L * (epsilon**2 - torch.norm(mu_L - hat_mu_L)**2 - torch.norm(sqrtm_svd(Sigma_L) - sqrtm_svd(hat_Sigma_L))**2)\n",
    "    term3 = lambda_H * (delta**2 - torch.norm(mu_H - hat_mu_H)**2 - torch.norm(sqrtm_svd(Sigma_H) - sqrtm_svd(hat_Sigma_H))**2)\n",
    "\n",
    "    return term1 / n + term2 + term3\n",
    "\n",
    "# Proximal operator for Sigma_L (using soft-thresholding)\n",
    "def prox_Sigma_L(Sigma_L, lambda_L, LLmodels, HLmodels, Sigma_H):\n",
    "    # Using the Frobenius norm as a soft-thresholding operator for Sigma_L\n",
    "    prox = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "        V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "        prox_i          = prox_operator(sqrtm_V_Sigma_V, lambda_L)\n",
    "        ll_term         = torch.linalg.pinv(V_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.pinv(V_i).T\n",
    "\n",
    "        H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "        hl_term         = torch.norm(sqrtm_H_Sigma_H, p='fro') \n",
    "       \n",
    "        prox += ll_term * hl_term\n",
    "\n",
    "    prox *= (2 / n)\n",
    "    prox = diagonalize(prox)\n",
    "    return prox\n",
    "\n",
    "# Proximal operator for Sigma_H (using soft-thresholding)\n",
    "def prox_Sigma_H(Sigma_H, lambda_H, LLmodels, HLmodels, Sigma_L):\n",
    "    prox = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "       \n",
    "        H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "        prox_i          = prox_operator(sqrtm_H_Sigma_H, lambda_H)\n",
    "        hl_term         = torch.linalg.inv(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.inv(H_i).T\n",
    "        #hl_term        = torch.inverse(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.inverse(H_i).T\n",
    "\n",
    "        V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "        ll_term         = torch.norm(sqrtm_V_Sigma_V, p='fro') \n",
    "        \n",
    "        prox     += ll_term * hl_term\n",
    "\n",
    "    prox *= (2 / n)\n",
    "\n",
    "    prox = diagonalize(prox)\n",
    "    return prox\n",
    "\n",
    "# Proximal operator of a matrix frobenious norm\n",
    "def prox_operator(A, lambda_param):\n",
    "    frobenius_norm = torch.norm(A, p='fro')\n",
    "    scaling_factor = torch.max(1 - lambda_param / frobenius_norm, torch.zeros_like(frobenius_norm))\n",
    "    return scaling_factor * A\n",
    "\n",
    "def diagonalize(A):\n",
    "    # Get eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = torch.linalg.eig(A)  \n",
    "    eigvals_real     = eigvals.real  \n",
    "    eigvals_real     = torch.sqrt(eigvals_real)  # Take the square root of the eigenvalues\n",
    "\n",
    "    return torch.diag(eigvals_real)\n",
    "\n",
    "def sqrtm_svd(A):\n",
    "    # Compute the SVD of A\n",
    "    U, S, V = torch.svd(A)\n",
    "    \n",
    "    # Take the square root of the singular values\n",
    "    S_sqrt = torch.sqrt(torch.clamp(S, min=0.0))  # Ensure non-negative singular values\n",
    "    \n",
    "    # Reconstruct the square root matrix\n",
    "    sqrt_A = U @ torch.diag(S_sqrt) @ V.T\n",
    "    \n",
    "    return sqrt_A\n",
    "\n",
    "def sqrtm_eig(A):\n",
    "    eigvals, eigvecs = torch.linalg.eig(A)\n",
    "    eigvals_real = eigvals.real\n",
    "    \n",
    "    # Ensure eigenvalues are non-negative for the square root to be valid\n",
    "    eigvals_sqrt = torch.sqrt(torch.clamp(eigvals_real, min=0.0))  # Square root of non-negative eigenvalues\n",
    "\n",
    "    # Reconstruct the square root of the matrix using the eigenvectors\n",
    "    # Make sure the eigenvectors are also real\n",
    "    eigvecs_real = eigvecs.real\n",
    "    \n",
    "    # Reconstruct the matrix square root\n",
    "    sqrt_A = eigvecs_real @ torch.diag(eigvals_sqrt) @ eigvecs_real.T\n",
    "    \n",
    "    return sqrt_A\n",
    "\n",
    "\n",
    "# Optimization loop using autograd and PyProximal (maximize using gradient ascent)\n",
    "def optimize(LLmodels, HLmodels, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    mu_L.requires_grad_(True)  # Enable autograd for mu_L\n",
    "    Sigma_L_half.requires_grad_(True)  # Enable autograd for Sigma_L\n",
    "    mu_H.requires_grad_(True)  # Enable autograd for mu_H\n",
    "    Sigma_H_half.requires_grad_(True)  # Enable autograd for Sigma_H\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        print(f\"Iteration {t}\")\n",
    "        \n",
    "        objective = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "        objective.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Ascent for mu_H\n",
    "            \n",
    "            print(f\"Sigma_L: {Sigma_L.grad}\")\n",
    "            Sigma_L_half += eta * Sigma_L.grad  # Ascent for Sigma_L\n",
    "            Sigma_H += eta * Sigma_H.grad  # Ascent for Sigma_H\n",
    "            Sigma_L = prox_Sigma_L(Sigma_L_half, lambda_L, LLmodels, HLmodels, Sigma_H)\n",
    "            print(Sigma_L)  \n",
    "            Sigma_H = prox_Sigma_H(Sigma_H_half, lambda_H, LLmodels, HLmodels, Sigma_L)\n",
    "            \n",
    "            #Zero the gradients after the update\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "            Sigma_L.grad.zero_()\n",
    "            Sigma_H.grad.zero_()\n",
    "\n",
    "            # if mu_L.grad is not None:\n",
    "            #     mu_L.grad.zero_()\n",
    "            # if mu_H.grad is not None:\n",
    "            #     mu_H.grad.zero_()\n",
    "            # if Sigma_L.grad is not None:\n",
    "            #     Sigma_L.grad.zero_()\n",
    "            # if Sigma_H.grad is not None:\n",
    "            #     Sigma_H.grad.zero_()\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2796382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "hat_Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "hat_mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "hat_Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = hat_mu_L.shape[0]\n",
    "h = hat_mu_H.shape[0]\n",
    "\n",
    "\n",
    "# Gelbrich initialization\n",
    "ll_moments      = mut.sample_moments_U(mu_hat = mu_U_ll_hat, Sigma_hat = Sigma_U_ll_hat, bound = epsilon, num_envs = 1)\n",
    "mu_L0, Sigma_L0 = ll_moments[0]\n",
    "#mu_L0, Sigma_L0 = torch.from_numpy(mu_L0), torch.from_numpy(Sigma_L0)\n",
    "\n",
    "hl_moments      = mut.sample_moments_U(mu_hat = mu_U_hl_hat, Sigma_hat = Sigma_U_hl_hat, bound = delta, num_envs = 1)\n",
    "mu_H0, Sigma_H0 = hl_moments[0]\n",
    "#mu_H0, Sigma_H0 = torch.from_numpy(mu_H0), torch.from_numpy(Sigma_H0)\n",
    "\n",
    "\n",
    "T = torch.exp(torch.from_numpy(np.random.randn(2, 3)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f245aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Sigma_L: tensor([[0.4711, 1.2888, 1.0865],\n",
      "        [1.2888, 5.5278, 3.1481],\n",
      "        [1.0865, 3.1481, 2.5457]])\n",
      "Updated Sigma_L: tensor([[0.0066, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6441, 0.0000],\n",
      "        [0.0000, 0.0000, 2.2323]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mu_H    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mu_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m Sigma_H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Sigma_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 6\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H \u001b[38;5;241m=\u001b[39m optimize(LLmodels, HLmodels, mu_L\u001b[38;5;241m.\u001b[39mfloat(), Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), mu_H\u001b[38;5;241m.\u001b[39mfloat(), Sigma_H\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      7\u001b[0m                                          hat_mu_L\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), hat_mu_H\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_H\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      8\u001b[0m                                            epsilon, delta, lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.7\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.8\u001b[39m, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(LLmodels, HLmodels, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter)\u001b[0m\n\u001b[1;32m     31\u001b[0m     mu_L\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     32\u001b[0m     mu_H\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m---> 33\u001b[0m     Sigma_L\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     34\u001b[0m     Sigma_H\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "mu_L    = torch.from_numpy(mu_L0).float()\n",
    "Sigma_L = torch.from_numpy(Sigma_L0).float()\n",
    "mu_H    = torch.from_numpy(mu_H0).float()\n",
    "Sigma_H = torch.from_numpy(Sigma_H0).float()\n",
    "\n",
    "mu_L, Sigma_L, mu_H, Sigma_H = optimize(LLmodels, HLmodels, mu_L.float(), Sigma_L.float(), mu_H.float(), Sigma_H.float(),\n",
    "                                         hat_mu_L.float(), hat_Sigma_L.float(), hat_mu_H.float(), hat_Sigma_H.float(),\n",
    "                                           epsilon, delta, lambda_L=.7, lambda_H=.8, eta=.01, max_iter=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
