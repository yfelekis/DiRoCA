{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae33f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_L    = torch.from_numpy(mu_U_ll_hat)\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat)\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "# Given estimates (mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "hat_mu_L    =  torch.from_numpy(mu_U_ll_hat) \n",
    "hat_Sigma_L =  torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "hat_mu_H    =  torch.from_numpy(mu_U_hl_hat)\n",
    "hat_Sigma_H =  torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "lambda_L =.2\n",
    "lambda_H =.3\n",
    "eta      = .01\n",
    "max_iter = 10\n",
    "\n",
    "Sigma_L = hat_Sigma_L\n",
    "Sigma_H = hat_Sigma_H\n",
    "\n",
    "\n",
    "T = torch.exp(torch.from_numpy(np.random.randn(2, 3)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb5e43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary functions using PyTorch for automatic differentiation\n",
    "def F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "    term1 = 0\n",
    "    term2 = 0\n",
    "    term3 = 0\n",
    "\n",
    "    # Loop to compute the sum of terms\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  # Convert to float32\n",
    "        V_i = T @ L_i  # Matrix multiplication, ensure V_i is float32\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  # Convert to float32\n",
    "\n",
    "        term1 += torch.norm(torch.matmul(V_i, mu_L) - torch.matmul(H_i, mu_H))**2 + torch.trace(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))) + torch.trace(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)))\n",
    "\n",
    "    term2 = lambda_L * (epsilon**2 - torch.norm(mu_L - hat_mu_L)**2 - torch.norm(sqrtm_svd(Sigma_L) - sqrtm_svd(hat_Sigma_L))**2)\n",
    "    term3 = lambda_H * (delta**2 - torch.norm(mu_H - hat_mu_H)**2 - torch.norm(sqrtm_svd(Sigma_H) - sqrtm_svd(hat_Sigma_H))**2)\n",
    "\n",
    "    return term1 / n + term2 + term3\n",
    "\n",
    "# Proximal operator for Sigma_L (using soft-thresholding)\n",
    "def prox_Sigma_L(Sigma_L, lambda_L, LLmodels, HLmodels, Sigma_H):\n",
    "    # Using the Frobenius norm as a soft-thresholding operator for Sigma_L\n",
    "    prox = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "        V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "        prox_i          = prox_operator(sqrtm_V_Sigma_V, lambda_L)\n",
    "        ll_term         = torch.linalg.pinv(V_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.pinv(V_i).T\n",
    "\n",
    "        H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "        hl_term         = torch.norm(sqrtm_H_Sigma_H, p='fro') \n",
    "       \n",
    "        prox += ll_term * hl_term\n",
    "\n",
    "    prox *= (2 / n)\n",
    "    prox = diagonalize(prox)\n",
    "    return prox\n",
    "\n",
    "# Proximal operator for Sigma_H (using soft-thresholding)\n",
    "def prox_Sigma_H(Sigma_H, lambda_H, LLmodels, HLmodels, Sigma_L):\n",
    "    prox = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "       \n",
    "        H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "        prox_i          = prox_operator(sqrtm_H_Sigma_H, lambda_H)\n",
    "        hl_term         = torch.linalg.inv(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.inv(H_i).T\n",
    "        #hl_term        = torch.inverse(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.inverse(H_i).T\n",
    "\n",
    "        V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "        ll_term         = torch.norm(sqrtm_V_Sigma_V, p='fro') \n",
    "        \n",
    "        prox     += ll_term * hl_term\n",
    "\n",
    "    prox *= (2 / n)\n",
    "\n",
    "    prox = diagonalize(prox)\n",
    "    return prox\n",
    "\n",
    "# Proximal operator of a matrix frobenious norm\n",
    "def prox_operator(A, lambda_param):\n",
    "    frobenius_norm = torch.norm(A, p='fro')\n",
    "    scaling_factor = torch.max(1 - lambda_param / frobenius_norm, torch.zeros_like(frobenius_norm))\n",
    "    return scaling_factor * A\n",
    "\n",
    "def diagonalize(A):\n",
    "    # Get eigenvalues and eigenvectors\n",
    "    eigvals, eigvecs = torch.linalg.eig(A)  \n",
    "    eigvals_real     = eigvals.real  \n",
    "    eigvals_real     = torch.sqrt(eigvals_real)  # Take the square root of the eigenvalues\n",
    "\n",
    "    return torch.diag(eigvals_real)\n",
    "\n",
    "def sqrtm_svd(A):\n",
    "    # Compute the SVD of A\n",
    "    U, S, V = torch.svd(A)\n",
    "    \n",
    "    # Take the square root of the singular values\n",
    "    S_sqrt = torch.sqrt(torch.clamp(S, min=0.0))  # Ensure non-negative singular values\n",
    "    \n",
    "    # Reconstruct the square root matrix\n",
    "    sqrt_A = U @ torch.diag(S_sqrt) @ V.T\n",
    "    \n",
    "    return sqrt_A\n",
    "\n",
    "def sqrtm_eig(A):\n",
    "    eigvals, eigvecs = torch.linalg.eig(A)\n",
    "    eigvals_real = eigvals.real\n",
    "    \n",
    "    # Ensure eigenvalues are non-negative for the square root to be valid\n",
    "    eigvals_sqrt = torch.sqrt(torch.clamp(eigvals_real, min=0.0))  # Square root of non-negative eigenvalues\n",
    "\n",
    "    # Reconstruct the square root of the matrix using the eigenvectors\n",
    "    # Make sure the eigenvectors are also real\n",
    "    eigvecs_real = eigvecs.real\n",
    "    \n",
    "    # Reconstruct the matrix square root\n",
    "    sqrt_A = eigvecs_real @ torch.diag(eigvals_sqrt) @ eigvecs_real.T\n",
    "    \n",
    "    return sqrt_A\n",
    "\n",
    "\n",
    "# Optimization loop using autograd and PyProximal (maximize using gradient ascent)\n",
    "def optimize(LLmodels, HLmodels, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    mu_L.requires_grad_(True)  # Enable autograd for mu_L\n",
    "    Sigma_L_half.requires_grad_(True)  # Enable autograd for Sigma_L\n",
    "    mu_H.requires_grad_(True)  # Enable autograd for mu_H\n",
    "    Sigma_H_half.requires_grad_(True)  # Enable autograd for Sigma_H\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        print(f\"Iteration {t}\")\n",
    "        \n",
    "        objective = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "        objective.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Ascent for mu_H\n",
    "            \n",
    "            print(f\"Sigma_L: {Sigma_L.grad}\")\n",
    "            Sigma_L_half += eta * Sigma_L.grad  # Ascent for Sigma_L\n",
    "            Sigma_H += eta * Sigma_H.grad  # Ascent for Sigma_H\n",
    "            Sigma_L = prox_Sigma_L(Sigma_L_half, lambda_L, LLmodels, HLmodels, Sigma_H)\n",
    "            print(Sigma_L)  \n",
    "            Sigma_H = prox_Sigma_H(Sigma_H_half, lambda_H, LLmodels, HLmodels, Sigma_L)\n",
    "            \n",
    "            #Zero the gradients after the update\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "            Sigma_L.grad.zero_()\n",
    "            Sigma_H.grad.zero_()\n",
    "\n",
    "            # if mu_L.grad is not None:\n",
    "            #     mu_L.grad.zero_()\n",
    "            # if mu_H.grad is not None:\n",
    "            #     mu_H.grad.zero_()\n",
    "            # if Sigma_L.grad is not None:\n",
    "            #     Sigma_L.grad.zero_()\n",
    "            # if Sigma_H.grad is not None:\n",
    "            #     Sigma_H.grad.zero_()\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2796382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "hat_Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "hat_mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "hat_Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = hat_mu_L.shape[0]\n",
    "h = hat_mu_H.shape[0]\n",
    "\n",
    "\n",
    "# Gelbrich initialization\n",
    "ll_moments      = mut.sample_moments_U(mu_hat = mu_U_ll_hat, Sigma_hat = Sigma_U_ll_hat, bound = epsilon, num_envs = 1)\n",
    "mu_L0, Sigma_L0 = ll_moments[0]\n",
    "#mu_L0, Sigma_L0 = torch.from_numpy(mu_L0), torch.from_numpy(Sigma_L0)\n",
    "\n",
    "hl_moments      = mut.sample_moments_U(mu_hat = mu_U_hl_hat, Sigma_hat = Sigma_U_hl_hat, bound = delta, num_envs = 1)\n",
    "mu_H0, Sigma_H0 = hl_moments[0]\n",
    "#mu_H0, Sigma_H0 = torch.from_numpy(mu_H0), torch.from_numpy(Sigma_H0)\n",
    "\n",
    "\n",
    "T = torch.exp(torch.from_numpy(np.random.randn(2, 3)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max):\n",
    "\n",
    "    for t in range(num_steps_max): \n",
    "        #print('mu_L before update:', mu_L)\n",
    "        mu_L         = update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "        # print('mu_L after update:', mu_L)\n",
    "        # print('mu_H before update:', mu_H)\n",
    "        mu_H         = update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "        # print('mu_H after update:', mu_H)\n",
    "\n",
    "        # print('Sigma_L before update:', Sigma_L)\n",
    "        Sigma_L_half = update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "        Sigma_L      = update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "        # print('Sigma_L after update:', Sigma_L)\n",
    "        \n",
    "        # print('Sigma_H before update:', Sigma_H)\n",
    "        Sigma_H_half = update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "        Sigma_H      = update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "        # print('Sigma_H after update:', Sigma_H)\n",
    "        \n",
    "        mu_L, Sigma_L, mu_H, Sigma_H = enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "        # print('mu_L after constraints:', mu_L)\n",
    "        # print('Sigma_L after constraints:', Sigma_L)\n",
    "        # print('mu_H after constraints:', mu_H)\n",
    "        # print('Sigma_H after constraints:', Sigma_H)\n",
    "        # print( )\n",
    "        # Compute the objective function for the current iteration\n",
    "        obj = 0\n",
    "        \n",
    "        for i, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "            V_i = T @ L_i.float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "                        \n",
    "            L_i_mu_L = V_i @ mu_L\n",
    "            H_i_mu_H = H_i @ mu_H\n",
    "            term1 = torch.norm(L_i_mu_L.float() - H_i_mu_H.float())**2\n",
    "            \n",
    "            V_Sigma_V = V_i.float() @ Sigma_L.float() @ V_i.T.float()\n",
    "            H_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "\n",
    "            term2 = torch.trace(V_Sigma_V)\n",
    "            term3 = torch.trace(H_Sigma_H)\n",
    "            \n",
    "            sqrtVSV = oput.sqrtm_svd(V_Sigma_V)\n",
    "            sqrtHSH = oput.sqrtm_svd(H_Sigma_H)\n",
    "\n",
    "            #term4 = -2*torch.trace(oput.sqrtm_svd(sqrtHSH @ V_Sigma_V @ sqrtHSH))\n",
    "            term4 = -2*torch.norm(oput.sqrtm_svd(sqrtVSV) @ oput.sqrtm_svd(sqrtHSH), 'nuc')\n",
    "            \n",
    "            obj = obj + (term1 + term2 + term3 + term4)\n",
    "        \n",
    "        obj = obj/i\n",
    "        \n",
    "        print(f\"Max step {t+1}/{num_steps_max}, Objective: {obj.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "def optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T):\n",
    "\n",
    "    objective_T = 0  # Initialize the objective for this step\n",
    "\n",
    "    for step in range(num_steps_min):\n",
    "        objective_T = 0  # Reset objective at the start of each step\n",
    "        for n, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "            L_i_mu_L = L_i @ mu_L  \n",
    "            H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "            term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "            term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "            term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "            \n",
    "            L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "            H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "\n",
    "            # Using the SVD square root term\n",
    "            term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "            objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "        objective_T = objective_T/n\n",
    "\n",
    "        optimizer_T.zero_grad() # Clear previous gradients\n",
    "        objective_T.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "        optimizer_T.step()      # Update T using the optimizer\n",
    "\n",
    "        print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "\n",
    "    return objective_T, T  # Return both the objective and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f245aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Sigma_L: tensor([[0.4711, 1.2888, 1.0865],\n",
      "        [1.2888, 5.5278, 3.1481],\n",
      "        [1.0865, 3.1481, 2.5457]])\n",
      "Updated Sigma_L: tensor([[0.0066, 0.0000, 0.0000],\n",
      "        [0.0000, 1.6441, 0.0000],\n",
      "        [0.0000, 0.0000, 2.2323]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mu_H    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mu_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m Sigma_H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Sigma_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 6\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H \u001b[38;5;241m=\u001b[39m optimize(LLmodels, HLmodels, mu_L\u001b[38;5;241m.\u001b[39mfloat(), Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), mu_H\u001b[38;5;241m.\u001b[39mfloat(), Sigma_H\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      7\u001b[0m                                          hat_mu_L\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), hat_mu_H\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_H\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      8\u001b[0m                                            epsilon, delta, lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.7\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.8\u001b[39m, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 33\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(LLmodels, HLmodels, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter)\u001b[0m\n\u001b[1;32m     31\u001b[0m     mu_L\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     32\u001b[0m     mu_H\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[0;32m---> 33\u001b[0m     Sigma_L\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     34\u001b[0m     Sigma_H\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mzero_()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "mu_L    = torch.from_numpy(mu_L0).float()\n",
    "Sigma_L = torch.from_numpy(Sigma_L0).float()\n",
    "mu_H    = torch.from_numpy(mu_H0).float()\n",
    "Sigma_H = torch.from_numpy(Sigma_H0).float()\n",
    "\n",
    "mu_L, Sigma_L, mu_H, Sigma_H = optimize(LLmodels, HLmodels, mu_L.float(), Sigma_L.float(), mu_H.float(), Sigma_H.float(),\n",
    "                                         hat_mu_L.float(), hat_Sigma_L.float(), hat_mu_H.float(), hat_Sigma_H.float(),\n",
    "                                           epsilon, delta, lambda_L=.7, lambda_H=.8, eta=.01, max_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79585378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wass_mean_barycenter(struc_matrices, mu):\n",
    "\n",
    "    n = len(struc_matrices)\n",
    "\n",
    "    mu_barycenter = np.sum([S @ mu for S in struc_matrices], axis=0) / n\n",
    "\n",
    "\n",
    "    return mu_barycenter\n",
    "    \n",
    "    # Initialize the covariance matrix\n",
    "    Sigma_barycenter = Sigma  # Start with the shared covariance matrix\n",
    "    \n",
    "    # Iterate to refine Sigma_barycenter\n",
    "    for iteration in range(max_iter):\n",
    "        Sigma_barycenter_half = sqrtm(Sigma_barycenter)  # Compute sqrt(Σ_barycenter)\n",
    "        sum_term = np.zeros_like(Sigma)\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Calculate the transformed covariance L_i * Sigma_L * L_i^T\n",
    "            struc_i_transformed = struc_matrices[i] @ Sigma @ struc_matrices[i].T\n",
    "            # Compute the square root of the term\n",
    "            term = sqrtm(Sigma_barycenter_half @ struc_i_transformed @ Sigma_barycenter_half)\n",
    "            sum_term += term\n",
    "        \n",
    "        # Update Sigma_barycenter (take the average)\n",
    "        new_Sigma_barycenter = sum_term / n\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_Sigma_barycenter - Sigma_barycenter) < tol:\n",
    "            break\n",
    "        \n",
    "        # Update for the next iteration\n",
    "        Sigma_barycenter = new_Sigma_barycenter\n",
    "    \n",
    "    return mu_barycenter, Sigma_barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26910c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_matrices = []  # List of L_i matrices\n",
    "for iota in Ill:\n",
    "    L_matrices.append(LLmodels[iota].compute_mechanism())\n",
    "\n",
    "H_matrices = []  # List of H_i matrices\n",
    "for eta in Ihl:\n",
    "    H_matrices.append(HLmodels[eta].compute_mechanism())\n",
    "\n",
    "mu_bary_L, Sigma_bary_L = oput.compute_gauss_barycenter(L_matrices, mu_U_ll_hat, Sigma_U_ll_hat)\n",
    "mu_bary_H, Sigma_bary_H = oput.compute_gauss_barycenter(H_matrices, mu_U_hl_hat, Sigma_U_hl_hat)\n",
    "\n",
    "print(\"Low-level barycenter Mean:\", mu_bary_L)\n",
    "print(\"Low-level barycenter Covariance:\", Sigma_bary_L)\n",
    "print( )\n",
    "print(\"High-level barycenter Mean:\", mu_bary_H)\n",
    "print(\"High-level barycenter Covariance:\", Sigma_bary_H)\n",
    "\n",
    "V                 = oput.sample_projection(mu_U_ll_hat.shape[0], mu_U_hl_hat.shape[0], use_stiefel=False)\n",
    "mu_bary_L_proj    = V @ mu_bary_L\n",
    "Sigma_bary_L_proj = V @ Sigma_bary_L @ V.T\n",
    "\n",
    "monge, A = oput.monge_map(mu_bary_L_proj, Sigma_bary_L_proj, mu_bary_H, Sigma_bary_H)\n",
    "T        = V.T @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4d9d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wass_mean_barycenter(struc_matrices, mu):\n",
    "    mu_barycenter = np.sum([S @ mu for S in struc_matrices], axis=0) / len(struc_matrices)\n",
    "    \n",
    "    return mu_barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6be68fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_matrices = []  # List of L_i matrices\n",
    "for iota in Ill:\n",
    "    L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "    V_i = T @ L_i\n",
    "    L_matrices.append(V_i)\n",
    "\n",
    "H_matrices = []  # List of H_i matrices\n",
    "for eta in Ihl:\n",
    "    H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "    H_matrices.append(H_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b4139f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3555, 2.0766, 0.8979],\n",
       "         [0.5018, 0.7747, 1.1855]]),\n",
       " tensor([[0.3555, 2.0766, 0.8979],\n",
       "         [0.5018, 0.7747, 1.1855]]),\n",
       " tensor([[0.3555, 1.9770, 0.8763],\n",
       "         [0.5018, 0.6341, 1.1551]]),\n",
       " tensor([[0.3555, 2.0766, 0.8979],\n",
       "         [0.5018, 0.7747, 1.1855]]),\n",
       " tensor([[0.3555, 1.9770, 0.8763],\n",
       "         [0.5018, 0.6341, 1.1551]]),\n",
       " tensor([[0.3555, 1.9770, 0.8763],\n",
       "         [0.5018, 0.6341, 1.1551]])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_L_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57ec1410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0042843 , -0.00863504])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wass_mean_barycenter(H_matrices, mu_U_hl_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddd5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, iota in enumerate(Ill):\n",
    "    L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "    V_i = T @ L_i  \n",
    "    mu_bary += V_i @ mu_U_ll_hat\n",
    "\n",
    "mu_bary = mu_bary / n\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fd89511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monge(m_1, S_1, m_2, S_2):\n",
    "    inner      = torch.matmul(oput.sqrtm_svd(S_1), torch.matmul(S_2, oput.sqrtm_svd(S_1)))\n",
    "    sqrt_inner = oput.sqrtm_svd(inner)\n",
    "    A          = torch.matmul(torch.inverse(oput.sqrtm_svd(S_1)), torch.matmul(sqrt_inner, torch.inverse(oput.sqrtm_svd(S_1))))\n",
    "   \n",
    "    # Define the Monge map as a function τ(x) = m_2 + A(x - m_1)\n",
    "    def tau(x):\n",
    "        return m_2 + A @ (x - m_1)\n",
    "\n",
    "    return tau, A\n",
    "\n",
    "def compute_monge_matrix(S_1, S_2):\n",
    "    return torch.inverse(oput.sqrtm_svd(S_1)) @ oput.sqrtm_svd(oput.sqrtm_svd(S_1) @ S_2 @ oput.sqrtm_svd(S_1)) @ torch.inverse(oput.sqrtm_svd(S_1))\n",
    "\n",
    "def update_S_bary(Sigma, models, S_bary):\n",
    "    S_bary_new = torch.zeros_like(Sigma)\n",
    "    for n, struc_mat in enumerate(models):\n",
    "        S_bary_new += oput.sqrtm_svd(oput.sqrtm_svd(S_bary) @ struc_mat @ Sigma @ struc_mat.T @ oput.sqrtm_svd(S_bary))\n",
    "    \n",
    "    return S_bary_new /n\n",
    "\n",
    "def compute_L_matrices(T, LLmodels, Ill):\n",
    "    for iota in Ill:\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        L_matrices.append(V_i)\n",
    "\n",
    "    return L_matrices\n",
    "\n",
    "def compute_H_matrices(HLmodels, Ihl):\n",
    "    for eta in Ihl:\n",
    "        H_i = torch.from_numpy(HLmodels[eta].compute_mechanism()).float()  \n",
    "        H_matrices.append(H_i)\n",
    "\n",
    "    return H_matrices\n",
    "\n",
    "def update_mu_bary(struc_matrices, mu):\n",
    "    struc_matrices_tensor = torch.stack(struc_matrices)\n",
    "    mu_barycenter         = torch.sum(struc_matrices_tensor @ mu, dim=0) / len(struc_matrices)\n",
    "\n",
    "    return mu_barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "29983d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H, LLmodels, HLmodels, Ill, Ihl, max_iter=10, tol=1e-6):\n",
    "\n",
    "    # Initialize the optimal transport map tau and the transformation matrix T\n",
    "    tau = lambda x: x\n",
    "    T   = torch.randn(Sigma_H.shape[0], Sigma_L.shape[0])\n",
    "    print(T.shape)\n",
    "    # Initialize the structural matrices    \n",
    "    L_matrices = compute_L_matrices(T, LLmodels, Ill)\n",
    "    H_matrices = compute_H_matrices(HLmodels, Ihl)\n",
    "\n",
    "    # Initilize the barycenteric means and covariances\n",
    "    mu_bary_L = update_mu_bary(L_matrices, mu_L)\n",
    "    mu_bary_H = update_mu_bary(H_matrices, mu_H)\n",
    "\n",
    "    S_bary_L = torch.sum(torch.stack([S @ Sigma_L @ S.T for S in L_matrices]), dim=0) / len(L_matrices)\n",
    "    S_bary_H = torch.sum(torch.stack([S @ Sigma_H @ S.T for S in H_matrices]), dim=0) / len(H_matrices)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Save previous values to check for convergence\n",
    "        mu_bary_L_old = mu_bary_L.clone()\n",
    "        mu_bary_H_old = mu_bary_H.clone()\n",
    "        S_bary_L_old = S_bary_L.clone()\n",
    "        S_bary_H_old = S_bary_H.clone()\n",
    "        T_old = T.clone()\n",
    "\n",
    "        tau, T     = monge(mu_bary_L, S_bary_L, mu_bary_H, S_bary_H)\n",
    "        print(T.shape)\n",
    "        L_matrices = compute_L_matrices(T, LLmodels, Ill)\n",
    "\n",
    "        mu_bary_L  = update_mu_bary(L_matrices, mu_L)\n",
    "        mu_bary_H  = update_mu_bary(H_matrices, mu_H)\n",
    "        \n",
    "        S_bary_L   = update_S_bary(Sigma_L, L_matrices, S_bary_L)\n",
    "        S_bary_H   = update_S_bary(Sigma_H, H_matrices, S_bary_H)\n",
    "\n",
    "        \n",
    "        # Check for convergence\n",
    "        if check_convergence(mu_bary_L_old, mu_bary_H_old, S_bary_L_old, S_bary_H_old, T_old,\n",
    "                             mu_bary_L, mu_bary_H, S_bary_L, S_bary_H, T, tol):\n",
    "            print(f\"Converged after {_+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        return tau, T, mu_bary_L, S_bary_L, mu_bary_H, S_bary_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f5868415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the convergence check function\n",
    "def check_convergence(mu_bary_L_old, mu_bary_H_old, S_bary_L_old, S_bary_H_old, T_old, \n",
    "                      mu_bary_L, mu_bary_H, S_bary_L, S_bary_H, T, tol):\n",
    "    # Frobenius norm of the difference in barycenter means\n",
    "    mu_diff_L = torch.norm(mu_bary_L - mu_bary_L_old, p='fro')\n",
    "    mu_diff_H = torch.norm(mu_bary_H - mu_bary_H_old, p='fro')\n",
    "    \n",
    "    # Frobenius norm of the difference in barycenter covariances\n",
    "    S_diff_L = torch.norm(S_bary_L - S_bary_L_old, p='fro')\n",
    "    S_diff_H = torch.norm(S_bary_H - S_bary_H_old, p='fro')\n",
    "    \n",
    "    # Frobenius norm of the difference in the transformation matrix\n",
    "    T_diff = torch.norm(T - T_old, p='fro')\n",
    "    \n",
    "    # Check if all differences are below the tolerance\n",
    "    if mu_diff_L < tol and mu_diff_H < tol and S_diff_L < tol and S_diff_H < tol and T_diff < tol:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "752bee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x2 and 3x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the barycentric optimization function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tau, T, mu_bary_L, S_bary_L, mu_bary_H, S_bary_H \u001b[38;5;241m=\u001b[39m barycentric_optimization(\n\u001b[1;32m      3\u001b[0m     mu_L, mu_H, Sigma_L, Sigma_H, LLmodels, HLmodels, Ill, Ihl, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print final results\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal barycenter for L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS_bary_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[150], line 32\u001b[0m, in \u001b[0;36mbarycentric_optimization\u001b[0;34m(mu_L, mu_H, Sigma_L, Sigma_H, LLmodels, HLmodels, Ill, Ihl, max_iter, tol)\u001b[0m\n\u001b[1;32m     30\u001b[0m tau, T     \u001b[38;5;241m=\u001b[39m monge(mu_bary_L, S_bary_L, mu_bary_H, S_bary_H)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(T\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 32\u001b[0m L_matrices \u001b[38;5;241m=\u001b[39m compute_L_matrices(T, LLmodels, Ill)\n\u001b[1;32m     34\u001b[0m mu_bary_L  \u001b[38;5;241m=\u001b[39m update_mu_bary(L_matrices, mu_L)\n\u001b[1;32m     35\u001b[0m mu_bary_H  \u001b[38;5;241m=\u001b[39m update_mu_bary(H_matrices, mu_H)\n",
      "Cell \u001b[0;32mIn[146], line 30\u001b[0m, in \u001b[0;36mcompute_L_matrices\u001b[0;34m(T, LLmodels, Ill)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iota \u001b[38;5;129;01min\u001b[39;00m Ill:\n\u001b[1;32m     29\u001b[0m     L_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(LLmodels[iota]\u001b[38;5;241m.\u001b[39mcompute_mechanism())\u001b[38;5;241m.\u001b[39mfloat()  \n\u001b[0;32m---> 30\u001b[0m     V_i \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m@\u001b[39m L_i  \n\u001b[1;32m     31\u001b[0m     L_matrices\u001b[38;5;241m.\u001b[39mappend(V_i)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L_matrices\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x2 and 3x3)"
     ]
    }
   ],
   "source": [
    "tau, T, mu_bary_L, S_bary_L, mu_bary_H, S_bary_H = barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H, LLmodels, HLmodels, Ill, Ihl, max_iter=10, tol=1e-6)\n",
    "\n",
    "print(f\"Final optimal map T: {T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a11a091",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
