{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp \n",
    "from src.CBN import CausalBayesianNetwork as CBN\n",
    "import modularised_utils as mut\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import wasserstein_distance_nd\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5279c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1_gnd'\n",
    "#experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bf6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01341509",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll      = mut.load_samples(experiment)[None][0]\n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1]\n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "\n",
    "omega = mut.load_omega_map(experiment)\n",
    "\n",
    "num_llvars      = Dll.shape[1]\n",
    "num_hlvars      = Dhl.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fef938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "ll_coeffs = mut.get_mle_coefficients_gmm(Dll, Gll, weights=None, n_components=num_llvars)\n",
    "hl_coeffs = mut.get_mle_coefficients_gmm(Dhl, Ghl, weights=None, n_components=num_hlvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966329bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)\n",
    "\n",
    "A_ll = mut.generate_perturbed_datasets(D = U_ll_hat, bound = epsilon, num_envs = ll_num_envs) #Low-level: A_epsilon\n",
    "A_hl = mut.generate_perturbed_datasets(D = U_hl_hat, bound = delta, num_envs = hl_num_envs) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced89cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ced86cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 2/100 [00:03<02:27,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 3 iterations.\n",
      "Optimized T: [[1.52829830e-02 4.73341307e-01 1.25829370e-11]\n",
      " [2.26566373e-02 5.95564466e-01 3.49693126e-12]]\n",
      "Optimized Theta: [[0.35147087 0.47811262 0.37771377]\n",
      " [0.19655297 0.04600236 0.4856553 ]\n",
      " [0.24309754 0.21838081 0.27136206]\n",
      " ...\n",
      " [0.38433809 0.07658456 0.07209259]\n",
      " [0.27574949 0.16564704 0.40556579]\n",
      " [0.48183323 0.12395806 0.41800125]]\n",
      "Optimized Phi: [[0.58498078 0.11461105]\n",
      " [0.12657994 0.23646488]\n",
      " [0.46489155 0.52252822]\n",
      " ...\n",
      " [0.17574303 0.17407436]\n",
      " [0.07207346 0.05444351]\n",
      " [0.16153937 0.13609068]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "U_L = U_ll_hat\n",
    "U_H = U_hl_hat\n",
    "\n",
    "num_samples, n = U_L.shape\n",
    "num_samples, m = U_H.shape\n",
    "\n",
    "epsilon = 0.5  # Radius of the Wasserstein ball for the low-level model\n",
    "delta   = 0.5 # Radius of the Wasserstein ball for the high-level model\n",
    "alpha   = 0.0001 # Learning rate for ascent steps in Theta and Phi\n",
    "\n",
    "# Initialize variables\n",
    "T     = np.random.rand(m, n)\n",
    "Theta = np.random.rand(num_samples, n)\n",
    "Phi   = np.random.rand(num_samples, m)\n",
    "\n",
    "# Project onto Frobenius ball function\n",
    "def project_onto_frobenius_ball(matrix, radius):\n",
    "    norm = np.linalg.norm(matrix, 'fro')\n",
    "    if norm > radius:\n",
    "        return matrix * (radius / norm)\n",
    "    return matrix\n",
    "\n",
    "# Update function for T \n",
    "def update_T(U_L, U_H, Theta, Phi):\n",
    "    T_var = cp.Variable((m, n), nonneg=True)\n",
    "    objective = 0\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota]._compute_reduced_form() \n",
    "        Hi = HLmodels[omega[iota]]._compute_reduced_form()\n",
    "        A  = T_var @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob      = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "    return T_var.value\n",
    "\n",
    "# Gradient ascent step for Theta\n",
    "def ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha):\n",
    "    gradient = np.zeros_like(Theta)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota]._compute_reduced_form() \n",
    "        Hi = HLmodels[omega[iota]]._compute_reduced_form()\n",
    "        A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += ((T @ Li).T @ A).T  # Compute gradient wrt Theta\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Theta += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Theta, np.sqrt(num_samples * epsilon**2))\n",
    "\n",
    "# Gradient ascent step for Phi\n",
    "def ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha):\n",
    "    gradient = np.zeros_like(Phi)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota]._compute_reduced_form() \n",
    "        Hi = HLmodels[omega[iota]]._compute_reduced_form()\n",
    "        A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += (Hi @ A).T  # Compute gradient wrt Phi\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Phi += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Phi, np.sqrt(num_samples * delta**2))\n",
    "\n",
    "# Main optimization loop\n",
    "max_iters = 100\n",
    "tol = 1e-5\n",
    "\n",
    "for iteration in tqdm(range(max_iters)):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Minimize wrt T\n",
    "    T = update_T(U_L, U_H, Theta, Phi)\n",
    "\n",
    "    # Maximize wrt Theta and Phi using gradient ascent\n",
    "    Theta = ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha)\n",
    "    Phi   = ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7d3003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated T =  [[1.52829830e-02 4.73341307e-01 1.25829370e-11]\n",
      " [2.26566373e-02 5.95564466e-01 3.49693126e-12]], \n",
      " \n",
      "Ground Truth T = [[1 2 1]\n",
      " [0 1 0]] \n",
      " \n",
      "Frobenius Distance = 0\n"
     ]
    }
   ],
   "source": [
    "# Tau  = mut.load_T(experiment)\n",
    "# #Tau = torch.from_numpy(Tau).float()\n",
    "# diff = torch.norm(T-Tau, 'fro')\n",
    "\n",
    "# print(f\"Estimated T =  {T}, \\n \\nGround Truth T = {Tau} \\n \\nFrobenius Distance = {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c944c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = np.array([1.5, 2.5, 3.5])\n",
    "mapped_point = T @ x_sample\n",
    "print(f'{x_sample} maps to {mapped_point}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT CVXPY\n",
    "U_L = U_ll_hat\n",
    "U_H = U_hl_hat\n",
    "\n",
    "num_samples, n = U_L.shape\n",
    "num_samples, m = U_H.shape\n",
    "\n",
    "# Parameters\n",
    "epsilon = 1.0\n",
    "delta = 1.0\n",
    "alpha = 0.01  # Learning rate for ascent steps in Theta and Phi\n",
    "learning_rate_T = 0.001  # Learning rate for descent step in T\n",
    "\n",
    "# Initialize primal and dual variables\n",
    "T = np.random.rand(m, n)\n",
    "Theta = np.random.rand(num_samples, n)\n",
    "Phi = np.random.rand(num_samples, m)\n",
    "\n",
    "# Define a function to project onto Frobenius ball\n",
    "def project_onto_frobenius_ball(matrix, radius):\n",
    "    norm = np.linalg.norm(matrix, 'fro')\n",
    "    if norm > radius:\n",
    "        return matrix * (radius / norm)\n",
    "    return matrix\n",
    "\n",
    "# Define a function to project onto the non-negative orthant\n",
    "def project_onto_non_negative(matrix):\n",
    "    return np.maximum(matrix, 0)\n",
    "\n",
    "# Gradient descent step for T\n",
    "def descent_step_T(U_L, U_H, T, Theta, Phi, learning_rate_T):\n",
    "    gradient = np.zeros_like(T)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "        \n",
    "        # Compute gradient with respect to T\n",
    "        gradient += A @ (Li @ (U_L.T + Theta.T)).T\n",
    "        \n",
    "    gradient /= num_samples\n",
    "    T = T - learning_rate_T * gradient  # Gradient descent step\n",
    "    return project_onto_non_negative(T)  # Ensure non-negativity\n",
    "\n",
    "# Gradient ascent step for Theta\n",
    "def ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha):\n",
    "    gradient = np.zeros_like(Theta)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += ((T @ Li).T @ A).T  # Compute gradient wrt Theta\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Theta += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Theta, np.sqrt(num_samples * epsilon**2))\n",
    "\n",
    "# Gradient ascent step for Phi\n",
    "def ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha):\n",
    "    gradient = np.zeros_like(Phi)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += (Hi @ A).T  # Compute gradient wrt Phi\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Phi += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Phi, np.sqrt(num_samples * delta**2))\n",
    "\n",
    "# Main optimization loop\n",
    "max_iters = 100\n",
    "tol = 1e-4\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Gradient descent step for T\n",
    "    T = descent_step_T(U_L, U_H, T, Theta, Phi, learning_rate_T)\n",
    "\n",
    "    # Gradient ascent steps for Theta and Phi\n",
    "    Theta = ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha)\n",
    "    Phi = ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7df312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstraction_errors             = {}\n",
    "# abstraction_env_errors         = {}\n",
    "# max_env_avg_interv_error_value = -np.inf\n",
    "# max_env_avg_interv_error_key   = None\n",
    "\n",
    "# for lenv in A_ll:\n",
    "#     for henv in A_hl:\n",
    "#         total_ui_error = 0\n",
    "#         num_distros    = len(Ill)\n",
    "\n",
    "#         T  = mut.sample_stoch_matrix(num_hlvars, num_llvars) # sample the abstraction map/matrix\n",
    "\n",
    "#         for iota in Ill:\n",
    "#             llcm   = LLmodels[iota]\n",
    "#             hlcm   = HLmodels[omega[iota]]\n",
    "#             llmech = llcm.compute_mechanism()\n",
    "#             hlmech = hlcm.compute_mechanism()\n",
    "\n",
    "#             lefthh = T @ (llmech @ lenv.T)\n",
    "#             righthh = hlmech @ henv.T\n",
    "#             #print(rig)\n",
    "#             error = wasserstein_distance_nd(lefthh, righthh)\n",
    "#             #error = mut.mat_jsd_distance(T@(llmech @ lenv.T), hlmech @ henv.T)\n",
    "#             #error = mut.mat_ot_wasserstein_distance(T@(llmech @ lenv.T), hlmech @ henv.T)\n",
    "#             #error  = mut.mat_wasserstein_distance(T@(llmech @ lenv.T), hlmech @ henv.T)\n",
    "            \n",
    "#             #print(error,'\\n')\n",
    "#             total_ui_error += error\n",
    "\n",
    "#         avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "#         if avg_interv_error > max_env_avg_interv_error_value:\n",
    "#             max_env_avg_interv_error_value = avg_interv_error\n",
    "#             max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "#         abstraction_errors[str(T)] = avg_interv_error\n",
    "#         #abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error\n",
    "\n",
    "# max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "# max_error = abstraction_errors[max_tau]\n",
    "\n",
    "# print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "# max_lenv = max_env_avg_interv_error_key[0]\n",
    "# max_henv = max_env_avg_interv_error_key[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d2c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
