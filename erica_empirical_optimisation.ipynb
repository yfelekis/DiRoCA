{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local modules\n",
    "import modularised_utils as mut\n",
    "import opt_utils as oput\n",
    "import evaluation_utils as evut\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5279c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment       = 'synth1'\n",
    "abduction        = False\n",
    "coeff_estimation = False\n",
    "\n",
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon, delta           = params.radius[experiment]\n",
    "ll_num_envs, hl_num_envs = params.n_envs[experiment]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples, num_hlsamples  = params.n_samples[experiment]\n",
    "\n",
    "# Load ground truth abstraction\n",
    "Tau = mut.load_T(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bf6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll_obs  = mut.load_samples(experiment)[None][0] \n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "l        = len(Gll.nodes())\n",
    "\n",
    "Dhl_obs  = mut.load_samples(experiment)[None][1] \n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "h        = len(Ghl.nodes())\n",
    "\n",
    "omega    = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01341509",
   "metadata": {},
   "outputs": [],
   "source": [
    "if coeff_estimation == True:\n",
    "    ll_coeffs = mut.get_coefficients(Dll_obs, Gll)\n",
    "    hl_coeffs = mut.get_coefficients(Dhl_obs, Ghl) \n",
    "else:\n",
    "    ll_coeffs = mut.load_coeffs(experiment, 'LL')\n",
    "    hl_coeffs = mut.load_coeffs(experiment, 'HL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a04da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if abduction == True:\n",
    "    U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll_obs, Gll, ll_coeffs)\n",
    "    U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl_obs, Ghl, hl_coeffs)\n",
    "else:\n",
    "    U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.load_exogenous(experiment, 'LL')\n",
    "    U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.load_exogenous(experiment, 'HL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966329bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ll = mut.generate_perturbed_datasets(D = U_ll_hat, bound = epsilon, num_envs = ll_num_envs) #Low-level: A_epsilon\n",
    "A_hl = mut.generate_perturbed_datasets(D = U_hl_hat, bound = delta, num_envs = hl_num_envs) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0f0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ced86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_L = U_ll_hat\n",
    "# U_H = U_hl_hat\n",
    "\n",
    "# num_samples, n = U_L.shape\n",
    "# num_samples, m = U_H.shape\n",
    "\n",
    "# epsilon = 0.5  # Radius of the Wasserstein ball for the low-level model\n",
    "# delta   = 0.5 # Radius of the Wasserstein ball for the high-level model\n",
    "# alpha   = 0.0001 # Learning rate for ascent steps in Theta and Phi\n",
    "\n",
    "# # Initialize variables\n",
    "# T     = np.random.rand(m, n)\n",
    "# Theta = np.random.rand(num_samples, n)\n",
    "# Phi   = np.random.rand(num_samples, m)\n",
    "\n",
    "# # Project onto Frobenius ball function\n",
    "# def project_onto_frobenius_ball(matrix, radius):\n",
    "#     norm = np.linalg.norm(matrix, 'fro')\n",
    "#     if norm > radius:\n",
    "#         return matrix * (radius / norm)\n",
    "#     return matrix\n",
    "\n",
    "# # Update function for T \n",
    "# def update_T(U_L, U_H, Theta, Phi):\n",
    "#     T_var = cp.Variable((m, n), nonneg=True)\n",
    "#     objective = 0\n",
    "#     for iota in Ill:\n",
    "#         Li = LLmodels[iota].F() \n",
    "#         Hi = HLmodels[omega[iota]].F()\n",
    "#         A  = T_var @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "#         objective += cp.norm(A, \"fro\")**2\n",
    "\n",
    "#     objective = cp.Minimize(objective / num_samples)\n",
    "#     prob      = cp.Problem(objective)\n",
    "#     prob.solve()\n",
    "#     return T_var.value\n",
    "\n",
    "# # Gradient ascent step for Theta\n",
    "# def ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha):\n",
    "#     gradient = np.zeros_like(Theta)\n",
    "#     for iota in Ill:\n",
    "#         Li = LLmodels[iota].F() \n",
    "#         Hi = HLmodels[omega[iota]].F()\n",
    "#         A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "#         gradient += ((T @ Li).T @ A).T  # Compute gradient wrt Theta\n",
    "\n",
    "#     gradient /= num_samples\n",
    "#     Theta += alpha * gradient  # Ascent step\n",
    "#     return project_onto_frobenius_ball(Theta, np.sqrt(num_samples * epsilon**2))\n",
    "\n",
    "# # Gradient ascent step for Phi\n",
    "# def ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha):\n",
    "#     gradient = np.zeros_like(Phi)\n",
    "#     for iota in Ill:\n",
    "#         Li = LLmodels[iota]._compute_reduced_form() \n",
    "#         Hi = HLmodels[omega[iota]]._compute_reduced_form()\n",
    "#         A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "#         gradient += (Hi @ A).T  # Compute gradient wrt Phi\n",
    "\n",
    "#     gradient /= num_samples\n",
    "#     Phi += alpha * gradient  # Ascent step\n",
    "#     return project_onto_frobenius_ball(Phi, np.sqrt(num_samples * delta**2))\n",
    "\n",
    "# # Main optimization loop\n",
    "# max_iters = 100\n",
    "# tol = 1e-5\n",
    "\n",
    "# for iteration in tqdm(range(max_iters)):\n",
    "#     T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "#     # Minimize wrt T\n",
    "#     T = update_T(U_L, U_H, Theta, Phi)\n",
    "\n",
    "#     # Maximize wrt Theta and Phi using gradient ascent\n",
    "#     Theta = ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha)\n",
    "#     Phi   = ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha)\n",
    "\n",
    "#     # Check for convergence\n",
    "#     if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "#         np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "#         np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "#         print(f\"Converged in {iteration + 1} iterations.\")\n",
    "#         break\n",
    "\n",
    "# # Final optimized values of T, Theta, and Phi\n",
    "# print(\"Optimized T:\", T)\n",
    "# print(\"Optimized Theta:\", Theta)\n",
    "# print(\"Optimized Phi:\", Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d28864cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection onto Frobenius ball\n",
    "def project_onto_frobenius_ball(matrix, radius):\n",
    "    norm = torch.norm(matrix, p='fro')\n",
    "    if norm > radius:\n",
    "        return matrix * (radius / norm)\n",
    "    return matrix\n",
    "\n",
    "# Objective function\n",
    "def objective(U_L, U_H, T, Theta, Phi, L, H):\n",
    "    loss_iota = 0\n",
    "    for iota in Ill:\n",
    "        L_i       = torch.from_numpy(LLmodels[iota].F).float()\n",
    "        H_i       = torch.from_numpy(HLmodels[omega[iota]].F).float()\n",
    "        pert_L_i  = U_L + Theta\n",
    "        pert_H_i  = U_H + Phi\n",
    "       \n",
    "        loss_iota = loss_iota + torch.norm(T @ L_i @ pert_L_i.T - H_i @ pert_H_i.T, p='fro')**2\n",
    "\n",
    "    loss = loss_iota/len(Ill)\n",
    "\n",
    "    return loss                                                          \n",
    "\n",
    "def run_empirical_optimization(theta_hatL, theta_hatH, initial_theta,\n",
    "                               epsilon, delta, eta_min, eta_max,\n",
    "                               num_stpes_min, num_stpes_max, max_iter, tol, seed,\n",
    "                               robust_L, robust_H, plot_epochs, display_results):\n",
    "\n",
    "    torch.manual_seed(seed) \n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    erica      = robust_L or robust_H\n",
    "   \n",
    "    U_L = torch.from_numpy(theta_hatL['U_L']).float()\n",
    "    U_H = torch.from_numpy(theta_hatH['U_H']).float()\n",
    "    num_samples = U_L.shape[0]\n",
    "    l = U_L.shape[1]\n",
    "    h = U_H.shape[1]\n",
    "\n",
    "# Initialize variables with requires_grad=True\n",
    "    T = torch.randn(h, l, requires_grad=True)\n",
    "    if initial_theta == 'random':\n",
    "        Theta = torch.randn(num_samples, l, requires_grad=True)\n",
    "        Phi = torch.randn(num_samples, h, requires_grad=True)\n",
    "    elif initial_theta == 'projected':\n",
    "        Theta = torch.randn(num_samples, l, requires_grad=True)\n",
    "        Phi = torch.randn(num_samples, h, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "            Theta.data = project_onto_frobenius_ball(Theta, torch.sqrt(torch.tensor(num_samples) * epsilon**2))\n",
    "            Phi.data = project_onto_frobenius_ball(Phi, torch.sqrt(torch.tensor(num_samples) * delta**2))\n",
    "# Create optimizers\n",
    "    optimizer_T = torch.optim.SGD([T], lr=eta_min)\n",
    "    optimizer_theta = torch.optim.SGD([Theta], lr=eta_max)\n",
    "    optimizer_phi = torch.optim.SGD([Phi], lr=eta_max)\n",
    "\n",
    "    prev_objective = float('inf')\n",
    "    epoch_objectives = {'T_objectives_overall': [], 'theta_objectives_overall': []}\n",
    "\n",
    "    for iteration in tqdm(range(max_iter)):\n",
    "        # Minimization step for T\n",
    "        for _ in range(num_stpes_min):\n",
    "            optimizer_T.zero_grad()\n",
    "            loss_min = objective(U_L, U_H, T, Theta, Phi, LLmodels, HLmodels)\n",
    "            loss_min.backward()\n",
    "            optimizer_T.step()\n",
    "        \n",
    "        # Maximization step for Theta and Phi\n",
    "        for _ in range(num_stpes_max):\n",
    "            optimizer_theta.zero_grad()\n",
    "            optimizer_phi.zero_grad()\n",
    "            \n",
    "            loss_max = objective(U_L, U_H, T, Theta, Phi, LLmodels, HLmodels)\n",
    "            loss_max.backward()\n",
    "            \n",
    "            # Update using optimizers\n",
    "            optimizer_theta.step()\n",
    "            optimizer_phi.step()\n",
    "            \n",
    "            # Project back onto constraint sets\n",
    "            with torch.no_grad():\n",
    "                Theta.data = project_onto_frobenius_ball(Theta, torch.sqrt(torch.tensor(num_samples) * epsilon**2))\n",
    "                Phi.data = project_onto_frobenius_ball(Phi, torch.sqrt(torch.tensor(num_samples) * delta**2))\n",
    "        \n",
    "        # Check convergence\n",
    "        with torch.no_grad():\n",
    "            current_objective = objective(U_L, U_H, T, Theta, Phi, LLmodels, HLmodels).item()\n",
    "            objective_change = abs(prev_objective - current_objective)\n",
    "            if objective_change < tol:\n",
    "                print(f\"Converged in {iteration + 1} iterations.\")\n",
    "                break\n",
    "            prev_objective = current_objective\n",
    "\n",
    "        if plot_epochs:\n",
    "            oput.plot_epoch_objectives(epoch_objectives, erica)\n",
    "\n",
    "\n",
    "    \n",
    "    U_L_final = U_L + Theta\n",
    "    U_H_final = U_H + Phi\n",
    "\n",
    "    paramsL      = {'U_L': U_L_final.detach().numpy(), 'radius': epsilon}\n",
    "    paramsH      = {'U_H': U_H_final.detach().numpy(), 'radius': delta}\n",
    "    T            = T.detach().numpy()\n",
    "    end_time     = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    if display_results == True:\n",
    "        oput.print_results(T, paramsL, paramsH, elapsed_time)\n",
    "\n",
    "\n",
    "    return T, Theta, Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19769f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = .9\n",
    "delta   = .8\n",
    "\n",
    "theta_hatL   = {'U_L': U_ll_hat, 'radius': epsilon}\n",
    "theta_hatH   = {'U_H': U_hl_hat, 'radius': delta}\n",
    "\n",
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70bbd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.58it/s]\n"
     ]
    }
   ],
   "source": [
    "params_empirical = {\n",
    "                    'theta_hatL': theta_hatL,      # Initial low-level parameters\n",
    "                    'theta_hatH': theta_hatH,      # Initial high-level parameters\n",
    "                    'initial_theta': 'random',     # Added: initialization method for Theta/Phi\n",
    "                    'epsilon': 0.5,               # Low-level radius\n",
    "                    'delta': 0.5,                 # High-level radius\n",
    "                    'eta_min': 0.001,             # Added: minimum learning rate\n",
    "                    'eta_max': 0.01,              # Added: maximum learning rate\n",
    "                    'num_stpes_min': 4,           # Steps for minimization\n",
    "                    'num_stpes_max': 3,           # Steps for maximization\n",
    "                    'max_iter': 10,               # Maximum iterations\n",
    "                    'tol': 1e-5,                  # Convergence tolerance\n",
    "                    'seed': seed,                 # Random seed\n",
    "                    'robust_L': True,             # Added: robustness flag for low-level\n",
    "                    'robust_H': True,             # Added: robustness flag for high-level\n",
    "                    'plot_epochs': False,         # Added: whether to plot epoch objectives\n",
    "                    'display_results': False      # Whether to display results\n",
    "                   }\n",
    "\n",
    "results = run_empirical_optimization(**params_empirical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4b3e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[nan, nan, nan],\n",
       "        [nan, nan, nan]], dtype=float32),\n",
       " tensor([[nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan],\n",
       "         [nan, nan, nan]], requires_grad=True),\n",
       " tensor([[nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         ...,\n",
       "         [nan, nan],\n",
       "         [nan, nan],\n",
       "         [nan, nan]], requires_grad=True))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a9fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0d7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7d3003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated T =  [[1.52829830e-02 4.73341307e-01 1.25829370e-11]\n",
      " [2.26566373e-02 5.95564466e-01 3.49693126e-12]], \n",
      " \n",
      "Ground Truth T = [[1 2 1]\n",
      " [0 1 0]] \n",
      " \n",
      "Frobenius Distance = 0\n"
     ]
    }
   ],
   "source": [
    "# Tau  = mut.load_T(experiment)\n",
    "# #Tau = torch.from_numpy(Tau).float()\n",
    "# diff = torch.norm(T-Tau, 'fro')\n",
    "\n",
    "# print(f\"Estimated T =  {T}, \\n \\nGround Truth T = {Tau} \\n \\nFrobenius Distance = {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c944c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = np.array([1.5, 2.5, 3.5])\n",
    "mapped_point = T @ x_sample\n",
    "print(f'{x_sample} maps to {mapped_point}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WITHOUT CVXPY\n",
    "U_L = U_ll_hat\n",
    "U_H = U_hl_hat\n",
    "\n",
    "num_samples, n = U_L.shape\n",
    "num_samples, m = U_H.shape\n",
    "\n",
    "# Parameters\n",
    "epsilon = 1.0\n",
    "delta = 1.0\n",
    "alpha = 0.01  # Learning rate for ascent steps in Theta and Phi\n",
    "learning_rate_T = 0.001  # Learning rate for descent step in T\n",
    "\n",
    "# Initialize primal and dual variables\n",
    "T = np.random.rand(m, n)\n",
    "Theta = np.random.rand(num_samples, n)\n",
    "Phi = np.random.rand(num_samples, m)\n",
    "\n",
    "# Define a function to project onto Frobenius ball\n",
    "def project_onto_frobenius_ball(matrix, radius):\n",
    "    norm = np.linalg.norm(matrix, 'fro')\n",
    "    if norm > radius:\n",
    "        return matrix * (radius / norm)\n",
    "    return matrix\n",
    "\n",
    "# Define a function to project onto the non-negative orthant\n",
    "def project_onto_non_negative(matrix):\n",
    "    return np.maximum(matrix, 0)\n",
    "\n",
    "# Gradient descent step for T\n",
    "def descent_step_T(U_L, U_H, T, Theta, Phi, learning_rate_T):\n",
    "    gradient = np.zeros_like(T)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "        \n",
    "        # Compute gradient with respect to T\n",
    "        gradient += A @ (Li @ (U_L.T + Theta.T)).T\n",
    "        \n",
    "    gradient /= num_samples\n",
    "    T = T - learning_rate_T * gradient  # Gradient descent step\n",
    "    return project_onto_non_negative(T)  # Ensure non-negativity\n",
    "\n",
    "# Gradient ascent step for Theta\n",
    "def ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha):\n",
    "    gradient = np.zeros_like(Theta)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += ((T @ Li).T @ A).T  # Compute gradient wrt Theta\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Theta += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Theta, np.sqrt(num_samples * epsilon**2))\n",
    "\n",
    "# Gradient ascent step for Phi\n",
    "def ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha):\n",
    "    gradient = np.zeros_like(Phi)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += (Hi @ A).T  # Compute gradient wrt Phi\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Phi += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Phi, np.sqrt(num_samples * delta**2))\n",
    "\n",
    "# Main optimization loop\n",
    "max_iters = 100\n",
    "tol = 1e-4\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Gradient descent step for T\n",
    "    T = descent_step_T(U_L, U_H, T, Theta, Phi, learning_rate_T)\n",
    "\n",
    "    # Gradient ascent steps for Theta and Phi\n",
    "    Theta = ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, alpha)\n",
    "    Phi = ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, alpha)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d2c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
