{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ca742a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import joblib\n",
    "import pickle \n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import seaborn as sns\n",
    "import random \n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from src.CBN import CausalBayesianNetwork as CBN\n",
    "import modularised_utils as mut\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "import evaluation_utils as evut\n",
    "import opt_utils as oput\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cc3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_base = joblib.load('batteries/scms/M_WMG_bins_5_avg_2.pkl')\n",
    "M_abst = joblib.load('batteries/scms/M_LRCS_bins_5.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46c5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-level interventions (new style)\n",
    "iota0 = None\n",
    "iota1 = ops.Intervention({'CG': 75.})\n",
    "iota2 = ops.Intervention({'CG': 110.})\n",
    "iota3 = ops.Intervention({'CG': 180.})\n",
    "iota4 = ops.Intervention({'CG': 200.})\n",
    "\n",
    "# Abstract-level interventions (new style)\n",
    "iota0_prime = None\n",
    "iota1_prime = ops.Intervention({'CG': 75.})\n",
    "iota2_prime = ops.Intervention({'CG': 100.})\n",
    "iota3_prime = ops.Intervention({'CG': 200.})\n",
    "\n",
    "# Mapping\n",
    "omega = {\n",
    "    iota0: iota0_prime,\n",
    "    iota1: iota1_prime,\n",
    "    iota2: iota2_prime,\n",
    "    iota3: iota3_prime,\n",
    "    iota4: iota3_prime\n",
    "}\n",
    "\n",
    "Ill = list(set(omega.keys()))\n",
    "Ihl = list(set(omega.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e25336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = joblib.load('batteries/dfs/df_WMG_bins_5_avg_2.pkl')\n",
    "df_abst = joblib.load('batteries/dfs/df_LRCS_bins_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaba6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.drop(df_base.columns[[1,2]], axis=1, inplace=True)\n",
    "df_base.replace({75:0, 110:1, 150:2, 170:3, 180:4, 200:5}, inplace=True)\n",
    "\n",
    "df_abst.drop(df_abst.columns[[1]], axis=1, inplace=True)\n",
    "df_abst.replace({75:0, 100:1, 200:2}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451f8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to match graph\n",
    "df_base = df_base.rename(columns={\n",
    "    'binned ML_avg0': 'ML0',\n",
    "    'binned ML_avg1': 'ML1'\n",
    "})\n",
    "# Rename columns to match graph\n",
    "df_abst = df_abst.rename(columns={\n",
    "    'Comma gap (µm)': 'CG', 'binned ML': 'ML'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea008d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gll = nx.DiGraph()\n",
    "# Gll.add_nodes_from(M_base.nodes())\n",
    "# Gll.add_edges_from(M_base.edges())\n",
    "# Ghl = nx.DiGraph()\n",
    "# Ghl.add_nodes_from(M_abst.nodes())\n",
    "# Ghl.add_edges_from(M_abst.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81100130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, convert your data to numpy array if it's not already\n",
    "# if isinstance(df_base, str):\n",
    "#     # If df_base is a path to a file\n",
    "#     df_base = np.loadtxt(df_base)  # or pd.read_csv(df_base).values\n",
    "# elif isinstance(df_base, pd.DataFrame):\n",
    "#     # If df_base is a pandas DataFrame\n",
    "#     df_base = df_base.values\n",
    "# else:\n",
    "#     # Ensure it's a numpy array\n",
    "#     df_base = np.array(df_base)\n",
    "\n",
    "# # Now you can use get_coefficients\n",
    "# ll_endogenous_coeff_dict = mut.get_coefficients(df_base, Gll)\n",
    "\n",
    "# # Same for high-level data\n",
    "# if isinstance(df_abst, str):\n",
    "#     df_abst = np.loadtxt(df_abst)\n",
    "# elif isinstance(df_abst, pd.DataFrame):\n",
    "#     df_abst = df_abst.values\n",
    "# else:\n",
    "#     df_abst = np.array(df_abst)\n",
    "\n",
    "# hl_endogenous_coeff_dict = mut.get_coefficients(df_abst, Ghl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12d315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_ll, ll_mu_hat, ll_Sigma_hat = mut.lan_abduction(df_base, Gll, ll_endogenous_coeff_dict)\n",
    "# U_hl, hl_mu_hat, hl_Sigma_hat = mut.lan_abduction(df_abst, Ghl, hl_endogenous_coeff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163802c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = {}\n",
    "\n",
    "for child in M_base.nodes():\n",
    "    parents = M_base.get_parents(child)\n",
    "    if not parents:\n",
    "        continue  # skip root nodes\n",
    "    X = df_base[parents].values\n",
    "    y = df_base[child].values\n",
    "    coef = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    for i, parent in enumerate(parents):\n",
    "        ll_coeffs[(parent, child)] = coef[i]\n",
    "\n",
    "Gll = CBN(list(ll_coeffs.keys()))\n",
    "\n",
    "hl_coeffs = {}\n",
    "\n",
    "for child in M_abst.nodes():\n",
    "    parents = M_abst.get_parents(child)\n",
    "    if not parents:\n",
    "        continue  # skip root nodes\n",
    "    X = df_abst[parents].values\n",
    "    y = df_abst[child].values\n",
    "    coef = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    for i, parent in enumerate(parents):\n",
    "        hl_coeffs[(parent, child)] = coef[i]\n",
    "\n",
    "Ghl = CBN(list(hl_coeffs.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc918853",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = min(df_base.shape[0], df_abst.shape[0])\n",
    "df_base = df_base[:min_samples]\n",
    "df_abst = df_abst[:min_samples]\n",
    "\n",
    "df_base= df_base.to_numpy()\n",
    "df_abst= df_abst.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c72360",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_llsamples = df_base.shape[0]\n",
    "num_hlsamples = df_abst.shape[0]\n",
    "l = len(Gll.nodes())\n",
    "h = len(Ghl.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d37b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(df_base, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(df_abst, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed952751",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aac533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_matrices = oput.compute_struc_matrices(LLmodels, Ill)\n",
    "H_matrices = oput.compute_struc_matrices(HLmodels, Ihl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da0a8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_radius(N, eta, c1=1.0, c2=1.0, alpha=2.0, m=3):\n",
    "    \"\"\"\n",
    "    Compute epsilon_N(eta) for empirical Wasserstein case.\n",
    "\n",
    "    Parameters:\n",
    "    - N: int, number of samples\n",
    "    - eta: float, confidence level (0 < eta < 1)\n",
    "    - c1: float, constant from theorem (default 1.0, adjust if needed)\n",
    "    - c2: float, constant from theorem (default 1.0, adjust if needed)\n",
    "    - alpha: float, light-tail exponent (P[exp(||ξ||^α)] ≤ A)\n",
    "    - m: int, ambient dimension\n",
    "\n",
    "    Returns:\n",
    "    - epsilon: float, the concentration radius\n",
    "    \"\"\"\n",
    "    assert 0 < eta < 1, \"eta must be in (0,1)\"\n",
    "    threshold = np.log(c1 / eta) / c2\n",
    "    if N >= threshold:\n",
    "        exponent = min(1/m, 0.5)\n",
    "    else:\n",
    "        exponent = 1 / alpha\n",
    "\n",
    "    epsilon = (np.log(c1 / eta) / (c2 * N)) ** exponent\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bdb4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_bound = round(compute_empirical_radius(N=num_llsamples, eta=0.05, c1=1000.0, c2=1.0, alpha=2.0, m=l), 3)\n",
    "hl_bound = round(compute_empirical_radius(N=num_hlsamples, eta=0.05, c1=1000.0, c2=1.0, alpha=2.0, m=h), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9ae69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon, delta = ll_bound, hl_bound\n",
    "\n",
    "eta_max = 0.001\n",
    "eta_min = 0.001\n",
    "\n",
    "max_iter = 1000\n",
    "num_steps_min = 5\n",
    "num_steps_max = 5\n",
    "\n",
    "robust_L = True \n",
    "robust_H = True\n",
    "\n",
    "initialization = 'random'\n",
    "\n",
    "tol  = 1e-5\n",
    "seed = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "526c1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_erica = {\n",
    "                        'U_L': U_ll_hat,\n",
    "                        'U_H': U_hl_hat,\n",
    "                        'L_models': LLmodels,\n",
    "                        'H_models': HLmodels,\n",
    "                        'omega': omega,\n",
    "                        'epsilon': epsilon,\n",
    "                        'delta': delta,\n",
    "                        'eta_min': eta_min,\n",
    "                        'eta_max': eta_max,\n",
    "                        'num_steps_min': num_steps_min,\n",
    "                        'num_steps_max': num_steps_max,\n",
    "                        'max_iter': max_iter,\n",
    "                        'tol': tol,\n",
    "                        'seed': seed,\n",
    "                        'robust_L': robust_L,\n",
    "                        'robust_H': robust_H,\n",
    "                        'initialization': initialization,\n",
    "                        'experiment': 'battery_discrete'\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ca15028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ε=δ = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 70.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ε=δ = 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 66.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ε=δ = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 832/1000 [00:11<00:02, 69.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 833\n",
      "Training for ε=δ = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 68.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for ε=δ = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 70.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed. T matrices stored in trained_results dictionary.\n",
      "Available ε=δ values: ['T_8', 'T_0.537-0.393', 'T_1', 'T_2', 'T_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define different epsilon=delta values\n",
    "eps_delta_values     = [8, ll_bound, 1, 2, 4]\n",
    "diroca_train_results_empirical = {}\n",
    "\n",
    "# For each epsilon=delta value\n",
    "for eps_delta in eps_delta_values:\n",
    "    print(f\"Training for ε=δ = {eps_delta}\")\n",
    "    # Update theta parameters\n",
    "    if eps_delta == ll_bound:\n",
    "        opt_params_erica['epsilon'] = ll_bound\n",
    "        opt_params_erica['delta']   = hl_bound\n",
    "    \n",
    "    else:\n",
    "        opt_params_erica['epsilon'] = eps_delta\n",
    "        opt_params_erica['delta']   = eps_delta\n",
    "    \n",
    "    # Run ERICA optimization\n",
    "    params_empirical, T_empirical = oput.run_empirical_erica_optimization(**opt_params_erica)\n",
    "    \n",
    "    # Store results including optimization parameters and transformation matrix\n",
    "    if eps_delta == ll_bound:\n",
    "        diroca_train_results_empirical['T_'+str(ll_bound)+'-'+str(hl_bound)] = {\n",
    "                                                    'optimization_params': params_empirical,\n",
    "                                                    'T_matrix': T_empirical\n",
    "                                                }\n",
    "    else:\n",
    "        diroca_train_results_empirical['T_'+str(eps_delta)] = {\n",
    "                                                    'optimization_params': params_empirical,\n",
    "                                                    'T_matrix': T_empirical\n",
    "                                                }\n",
    "\n",
    "print(\"\\nTraining completed. T matrices stored in trained_results dictionary.\")\n",
    "print(\"Available ε=δ values:\", list(diroca_train_results_empirical.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89c2b773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 850.93it/s]\n"
     ]
    }
   ],
   "source": [
    "params_enrico, T_enrico = oput.run_empirical_erica_optimization(**{**opt_params_erica, 'robust_L': False, 'robust_H': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58c4424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diroca_train_results_empirical['T_0.00'] = {\n",
    "                                'optimization_params': params_enrico,\n",
    "                                'T_matrix': T_enrico\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca07a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_bary = {\n",
    "                        'U_ll_hat':U_ll_hat,\n",
    "                        'U_hl_hat':U_hl_hat,\n",
    "                        'L_matrices':L_matrices,\n",
    "                        'H_matrices':H_matrices,\n",
    "                        'max_iter':1000,\n",
    "                        'tol':tol,\n",
    "                        'seed':seed\n",
    "                    }\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ae0e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3410.31it/s]\n"
     ]
    }
   ],
   "source": [
    "T_bary = oput.run_empirical_bary_optim(**opt_params_bary)\n",
    "params_bary = {'L':{}, 'H':{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "042aeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diroca_train_results_empirical['T_b'] = {\n",
    "                                'optimization_params': params_bary,\n",
    "                                'T_matrix': T_bary\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdb53936",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params_smooth = {\n",
    "                        'U_L': U_ll_hat,\n",
    "                        'U_H': U_hl_hat,\n",
    "                        'L_models': LLmodels,\n",
    "                        'H_models': HLmodels,\n",
    "                        'omega': omega,\n",
    "                        'eta_min': eta_min,\n",
    "                        'num_steps_min': num_steps_min,\n",
    "                        'max_iter': 300,\n",
    "                        'tol': tol,\n",
    "                        'seed': seed,\n",
    "                        'noise_sigma': 0.1,\n",
    "                        'num_noise_samples': 10\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e3110da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:10<00:00, 27.54it/s]\n"
     ]
    }
   ],
   "source": [
    "params_smooth, T_smooth = oput.run_empirical_smooth_optimization(**opt_params_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd9c6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "diroca_train_results_empirical['T_s'] = {\n",
    "                                'optimization_params': params_smooth,\n",
    "                                'T_matrix': T_smooth\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07be452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linabs_results = evut.run_abs_lingam_complete(df_base, df_abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "969ae0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "diroca_train_results_empirical['T_pa'] = {'optimization_params':{'L':{'pert_U':U_ll_hat},'H':{'pert_U':U_hl_hat}}, 'T_matrix': linabs_results['Perfect']['T'].T}\n",
    "diroca_train_results_empirical['T_na'] = {'optimization_params':{'L':{'pert_U':U_ll_hat},'H':{'pert_U':U_hl_hat}}, 'T_matrix': linabs_results['Noisy']['T'].T}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b833bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'battery_discrete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5da17060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/battery_discrete/diroca_train_results_empirical.pkl']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(diroca_train_results_empirical, f\"data/{experiment}/diroca_train_results_empirical.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5dc0fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downstream_evaluation(T, df_base, df_abst, noise_level, noise_in):\n",
    "#     from sklearn.linear_model import Lasso\n",
    "#     from sklearn.model_selection import LeaveOneGroupOut\n",
    "#     from sklearn.metrics import mean_squared_error\n",
    "#     import numpy as np\n",
    "\n",
    "#     from sklearn.linear_model import Lasso\n",
    "#     from sklearn.model_selection import LeaveOneGroupOut\n",
    "#     from sklearn.metrics import mean_squared_error\n",
    "#     import numpy as np\n",
    "\n",
    "#     # Define fixed hyperparameters\n",
    "#     lasso_params = {'alpha': 0.1, 'max_iter': 1000, 'tol': 0.01}\n",
    "    \n",
    "#     # Copy inputs\n",
    "#     df_base_noisy = df_base.copy()\n",
    "#     df_abst_noisy = df_abst.copy()\n",
    "#     df_abst_noisy = df_abst_noisy.astype(float)\n",
    "\n",
    "#     if noise_in == 'both':\n",
    "#         df_base_noisy += np.random.normal(0, noise_level, df_base.shape)\n",
    "#         df_abst_noisy += np.random.normal(0, noise_level, df_abst.shape)\n",
    "#     elif noise_in == 'base':\n",
    "#         df_base_noisy += np.random.normal(0, noise_level, df_base.shape)\n",
    "#         df_abst_noisy = df_abst\n",
    "#     elif noise_in =='abst':\n",
    "#         df_abst_noisy += np.random.normal(0, noise_level, df_abst.shape)\n",
    "#         df_base_noisy = df_base\n",
    "#     elif noise_in == 'none':\n",
    "#         df_base_noisy, df_abst_noisy = df_base, df_abst\n",
    "    \n",
    "\n",
    "#     # Generate transformed and real abstract samples\n",
    "#     tau_samples = T @ df_base_noisy.T\n",
    "#     abst_samples = df_abst_noisy.T\n",
    "\n",
    "#     # Step 1: Transpose to get (N, dim)\n",
    "#     X_real = abst_samples.T\n",
    "#     X_gen  = tau_samples.T\n",
    "\n",
    "#     # Step 2: Define target labels and intervention groupings\n",
    "#     y = df_abst[:, 1]\n",
    "#     groups = df_abst[:, 0]\n",
    "#     # y = df_abst_noisy[:, \"binned ML\"]\n",
    "#     # groups = df_abst_noisy[:, \"Comma gap (µm)\"]\n",
    "\n",
    "\n",
    "#     assert X_real.shape[0] == len(y) == len(groups), \"Mismatch in number of samples\"\n",
    "\n",
    "#     # Step 3: Combine real and generated data\n",
    "#     X_all = np.concatenate([X_real, X_gen], axis=0)\n",
    "#     y_all = np.concatenate([y, y], axis=0)\n",
    "#     groups_all  = np.concatenate([groups, groups], axis=0)\n",
    "\n",
    "#     logo = LeaveOneGroupOut()\n",
    "\n",
    "#     # Mode 1: Real → Real\n",
    "#     mse_real = []\n",
    "#     for train_idx, test_idx in logo.split(X_real, y, groups=groups):\n",
    "#         model = Lasso().fit(X_real[train_idx], y[train_idx])\n",
    "#         y_pred = model.predict(X_real[test_idx])\n",
    "#         mse_real.append(mean_squared_error(y[test_idx], y_pred))\n",
    "\n",
    "#    # Mode 2: Augmented → Real\n",
    "#     mse_aug = []\n",
    "#     for train_idx, test_idx in logo.split(X_real, y, groups=groups):\n",
    "#         # Use noisy y for training with transformed data\n",
    "#         y_train = df_abst_noisy[train_idx, 1]\n",
    "#         model = Lasso().fit(X_gen[train_idx], y_train)\n",
    "#         y_pred = model.predict(X_real[test_idx])\n",
    "#         mse_aug.append(mean_squared_error(y[test_idx], y_pred))\n",
    "\n",
    "#     # Mode 3: Real + Augmented → Real\n",
    "#     mse_mix = []\n",
    "#     for test_group in np.unique(groups):\n",
    "#         test_mask = (groups == test_group)\n",
    "#         test_idx_real = np.where(test_mask)[0]\n",
    "\n",
    "#         train_mask_real = (groups != test_group)\n",
    "#         train_idx_real = np.where(train_mask_real)[0]\n",
    "#         train_idx_gen = np.arange(len(y)) + len(y)\n",
    "#         train_idx_all = np.concatenate([train_idx_real, train_idx_gen])\n",
    "\n",
    "#         model = Lasso(**lasso_params).fit(X_all[train_idx_all], y_all[train_idx_all])\n",
    "\n",
    "#         y_pred = model.predict(X_real[test_idx_real])\n",
    "#         mse_mix.append(mean_squared_error(y[test_idx_real], y_pred))\n",
    "\n",
    "#     return {\n",
    "#         \"Real\": (np.mean(mse_real), np.std(mse_real)),\n",
    "#         \"Aug\": (np.mean(mse_aug), np.std(mse_aug)),\n",
    "#         \"AugReal\": (np.mean(mse_mix), np.std(mse_mix))\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "405f39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_evaluation_paper(T, df_base, df_abst):\n",
    "    \"\"\"\n",
    "    Implements the paper's evaluation methodology with three scenarios\n",
    "    \"\"\"\n",
    "    # Get unique Comma Gap values\n",
    "    comma_gaps = np.unique(df_abst[:, 0])\n",
    "    \n",
    "    # Scenario 1: Before abstraction (Real → Real)\n",
    "    mse_real = []\n",
    "    for cg in comma_gaps:\n",
    "        # Split data\n",
    "        test_mask = (df_abst[:, 0] == cg)\n",
    "        train_mask = ~test_mask\n",
    "        \n",
    "        # Train data\n",
    "        X_train = df_abst[train_mask, 0].reshape(-1, 1)  # Comma Gap as feature\n",
    "        y_train = df_abst[train_mask, 1]  # Mass Loading as target\n",
    "        \n",
    "        # Test data\n",
    "        X_test = df_abst[test_mask, 0].reshape(-1, 1)\n",
    "        y_test = df_abst[test_mask, 1]\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_real.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Scenario 2: After abstraction with support (Aug → Real)\n",
    "    mse_aug = []\n",
    "    # Generate transformed samples\n",
    "    tau_samples = T @ df_base.T\n",
    "    enhanced_data = np.concatenate([df_abst, tau_samples.T])\n",
    "    \n",
    "    for cg in comma_gaps:\n",
    "        test_mask = (df_abst[:, 0] == cg)\n",
    "        train_mask_abst = ~test_mask\n",
    "        train_mask_full = np.concatenate([train_mask_abst, np.ones(len(tau_samples.T), dtype=bool)])\n",
    "        \n",
    "        # Train data (including transformed samples)\n",
    "        X_train = enhanced_data[train_mask_full, 0].reshape(-1, 1)\n",
    "        y_train = enhanced_data[train_mask_full, 1]\n",
    "        \n",
    "        # Test data (only original LRCS)\n",
    "        X_test = df_abst[test_mask, 0].reshape(-1, 1)\n",
    "        y_test = df_abst[test_mask, 1]\n",
    "        \n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_aug.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Scenario 3: After abstraction without support (Real+Aug → Real)\n",
    "    mse_mix = []\n",
    "    for cg in comma_gaps:\n",
    "        test_mask_abst = (df_abst[:, 0] == cg)\n",
    "        test_mask_tau = (tau_samples.T[:, 0] == cg)\n",
    "        \n",
    "        train_mask_abst = ~test_mask_abst\n",
    "        train_mask_tau = ~test_mask_tau\n",
    "        \n",
    "        # Combine masks for training\n",
    "        train_data = np.concatenate([\n",
    "            df_abst[train_mask_abst],\n",
    "            tau_samples.T[train_mask_tau]\n",
    "        ])\n",
    "        \n",
    "        X_train = train_data[:, 0].reshape(-1, 1)\n",
    "        y_train = train_data[:, 1]\n",
    "        \n",
    "        # Test only on LRCS data\n",
    "        X_test = df_abst[test_mask_abst, 0].reshape(-1, 1)\n",
    "        y_test = df_abst[test_mask_abst, 1]\n",
    "        \n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_mix.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"Real\": (np.mean(mse_real), np.std(mse_real)),\n",
    "        \"Aug\": (np.mean(mse_aug), np.std(mse_aug)),\n",
    "        \"AugReal\": (np.mean(mse_mix), np.std(mse_mix))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa2b3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Method          Real                      Aug                       AugReal                  \n",
      "================================================================================\n",
      "T_8             5.556 ± 2.388             3.144 ± 2.736             3.144 ± 2.736            \n",
      "T_0.537-0.393   5.556 ± 2.388             4.840 ± 6.185             4.840 ± 6.185            \n",
      "T_1             5.556 ± 2.388             6.949 ± 9.273             6.949 ± 9.273            \n",
      "T_2             5.556 ± 2.388             5.288 ± 7.175             5.288 ± 7.175            \n",
      "T_4             5.556 ± 2.388             2.907 ± 1.907             2.907 ± 1.907            \n",
      "T_0.00          5.556 ± 2.388             6.031 ± 5.746             6.031 ± 5.746            \n",
      "T_b             5.556 ± 2.388             7.029 ± 5.610             7.029 ± 5.610            \n",
      "T_s             5.556 ± 2.388             4.584 ± 3.946             4.584 ± 3.946            \n",
      "T_pa            5.556 ± 2.388             3.310 ± 2.536             3.310 ± 2.536            \n",
      "T_na            5.556 ± 2.388             3.310 ± 2.536             3.679 ± 3.053            \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Method':<15} {'Real':<25} {'Aug':<25} {'AugReal':<25}\")\n",
    "# print(f\"{'Method':<15} {'Real':<25} \")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "for method in list(diroca_train_results_empirical.keys()):\n",
    "    T = diroca_train_results_empirical[method]['T_matrix']\n",
    "    d = downstream_evaluation_paper(T, df_base, df_abst)#, noise_level=0.0, noise_in='both')\n",
    "    \n",
    "    real_str = f\"{d['Real'][0]:.3f} ± {d['Real'][1]:.3f}\"\n",
    "    aug_str = f\"{d['Aug'][0]:.3f} ± {d['Aug'][1]:.3f}\"\n",
    "    augreal_str = f\"{d['AugReal'][0]:.3f} ± {d['AugReal'][1]:.3f}\"\n",
    "    \n",
    "    print(f\"{method:<15} {real_str:<25} {aug_str:<25} {augreal_str:<25}\")\n",
    "    # print(f\"{method:<15} {real_str:<25} \")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d68226ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_evaluation_paper_with_noise(T, df_base, df_abst, noise_level=0.0, noise_in='none'):\n",
    "    \"\"\"\n",
    "    Implements the paper's evaluation methodology with three scenarios, with added noise functionality\n",
    "    \"\"\"\n",
    "    # Create noisy copies of the data and convert to float\n",
    "    df_base_noisy = df_base.copy().astype(float)\n",
    "    df_abst_noisy = df_abst.copy().astype(float)\n",
    "    \n",
    "    # Add noise according to specified parameters\n",
    "    if noise_in == 'both':\n",
    "        df_base_noisy += np.random.normal(0, noise_level, df_base.shape)\n",
    "        df_abst_noisy += np.random.normal(0, noise_level, df_abst.shape)\n",
    "    elif noise_in == 'base':\n",
    "        df_base_noisy += np.random.normal(0, noise_level, df_base.shape)\n",
    "        df_abst_noisy = df_abst.copy().astype(float)\n",
    "    elif noise_in == 'abst':\n",
    "        df_abst_noisy += np.random.normal(0, noise_level, df_abst.shape)\n",
    "        df_base_noisy = df_base.copy().astype(float)\n",
    "    \n",
    "    # Get unique Comma Gap values\n",
    "    comma_gaps = np.unique(df_abst_noisy[:, 0])\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # Scenario 1: Before abstraction (Real → Real)\n",
    "    mse_real = []\n",
    "    for cg in comma_gaps:\n",
    "        test_mask = (df_abst_noisy[:, 0] == cg)\n",
    "        train_mask = ~test_mask\n",
    "        \n",
    "        X_train = df_abst_noisy[train_mask, 0].reshape(-1, 1)\n",
    "        y_train = df_abst_noisy[train_mask, 1]\n",
    "        \n",
    "        X_test = df_abst_noisy[test_mask, 0].reshape(-1, 1)\n",
    "        y_test = df_abst_noisy[test_mask, 1]\n",
    "        \n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_real.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Scenario 2: After abstraction with support (Aug → Real)\n",
    "    mse_aug = []\n",
    "    # Generate transformed samples using noisy base data\n",
    "    tau_samples = T @ df_base_noisy.T\n",
    "    enhanced_data = np.concatenate([df_abst_noisy, tau_samples.T])\n",
    "    \n",
    "    for cg in comma_gaps:\n",
    "        test_mask = (df_abst_noisy[:, 0] == cg)\n",
    "        train_mask_abst = ~test_mask\n",
    "        train_mask_full = np.concatenate([train_mask_abst, np.ones(len(tau_samples.T), dtype=bool)])\n",
    "        \n",
    "        X_train = enhanced_data[train_mask_full, 0].reshape(-1, 1)\n",
    "        y_train = enhanced_data[train_mask_full, 1]\n",
    "        \n",
    "        X_test = df_abst_noisy[test_mask, 0].reshape(-1, 1)\n",
    "        y_test = df_abst_noisy[test_mask, 1]\n",
    "        \n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_aug.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Scenario 3: After abstraction without support (Real+Aug → Real)\n",
    "    mse_mix = []\n",
    "    for cg in comma_gaps:\n",
    "        test_mask_abst = (df_abst_noisy[:, 0] == cg)\n",
    "        test_mask_tau = (tau_samples.T[:, 0] == cg)\n",
    "        \n",
    "        train_mask_abst = ~test_mask_abst\n",
    "        train_mask_tau = ~test_mask_tau\n",
    "        \n",
    "        train_data = np.concatenate([\n",
    "            df_abst_noisy[train_mask_abst],\n",
    "            tau_samples.T[train_mask_tau]\n",
    "        ])\n",
    "        \n",
    "        X_train = train_data[:, 0].reshape(-1, 1)\n",
    "        y_train = train_data[:, 1]\n",
    "        \n",
    "        X_test = df_abst_noisy[test_mask_abst, 0].reshape(-1, 1)\n",
    "        y_test = df_abst_noisy[test_mask_abst, 1]\n",
    "        \n",
    "        model = Lasso().fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse_mix.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"Real\": (np.mean(mse_real), np.std(mse_real)),\n",
    "        \"Aug\": (np.mean(mse_aug), np.std(mse_aug)),\n",
    "        \"AugReal\": (np.mean(mse_mix), np.std(mse_mix))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a16e831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Method          Real                      Aug                       AugReal                  \n",
      "================================================================================\n",
      "T_8             1.361 ± 2.088             2.350 ± 1.129             2.350 ± 1.129            \n",
      "T_0.537-0.393   1.349 ± 2.015             4.167 ± 2.334             4.167 ± 2.334            \n",
      "T_1             1.403 ± 2.069             1.239 ± 0.709             1.239 ± 0.709            \n",
      "T_2             1.369 ± 2.036             0.693 ± 0.420             0.693 ± 0.420            \n",
      "T_4             1.367 ± 2.075             1.928 ± 0.980             1.928 ± 0.980            \n",
      "T_0.00          1.396 ± 2.104             4.069 ± 2.134             4.069 ± 2.134            \n",
      "T_b             1.312 ± 2.058             3.626 ± 1.656             3.626 ± 1.656            \n",
      "T_s             1.349 ± 2.005             3.047 ± 1.507             3.047 ± 1.507            \n",
      "T_pa            1.374 ± 1.992             1.353 ± 1.921             1.353 ± 1.921            \n",
      "T_na            1.386 ± 2.163             1.364 ± 2.137             1.364 ± 2.137            \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"{'Method':<15} {'Real':<25} {'Aug':<25} {'AugReal':<25}\")\n",
    "# print(f\"{'Method':<15} {'Real':<25} \")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "for method in list(diroca_train_results_empirical.keys()):\n",
    "    T = diroca_train_results_empirical[method]['T_matrix']\n",
    "    d = downstream_evaluation_paper_with_noise(T, df_base, df_abst, noise_level=0.1, noise_in='both')\n",
    "    \n",
    "    real_str = f\"{d['Real'][0]:.3f} ± {d['Real'][1]:.3f}\"\n",
    "    aug_str = f\"{d['Aug'][0]:.3f} ± {d['Aug'][1]:.3f}\"\n",
    "    augreal_str = f\"{d['AugReal'][0]:.3f} ± {d['AugReal'][1]:.3f}\"\n",
    "    \n",
    "    print(f\"{method:<15} {real_str:<25} {aug_str:<25} {augreal_str:<25}\")\n",
    "    # print(f\"{method:<15} {real_str:<25} \")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f46b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402ace3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da093c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27054b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db11fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2fbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6afed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca7abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452ac5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892d7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62804f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913ab54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2e777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78dca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1b45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883f6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cff6b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_frobenius_ball(params, radius_type, boundary, rand_rad):\n",
    "    \"\"\"\n",
    "    Initialize a matrix inside the Frobenius ball with ||X||_F^2 <= N*epsilon^2\n",
    "    \"\"\"\n",
    "    num_samples, num_vars = params['pert_U'].shape\n",
    "    if radius_type == 'worst_case':\n",
    "        radius = params['radius_worst']\n",
    "\n",
    "    elif radius_type == 'hat_case':\n",
    "        radius = params['radius']\n",
    "\n",
    "    elif radius_type == 'random':\n",
    "        radius = rand_rad\n",
    "        \n",
    "    matrix           = np.random.randn(num_samples, num_vars)  \n",
    "    squared_norm     = np.linalg.norm(matrix, 'fro')**2\n",
    "    max_squared_norm = num_samples * radius**2\n",
    "    \n",
    "    if boundary == True:\n",
    "        scaling_factor = np.sqrt(max_squared_norm / squared_norm) * np.random.rand(1)\n",
    "    else:\n",
    "        scaling_factor = np.sqrt(max_squared_norm / squared_norm)\n",
    "    return matrix * scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adb73c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pertubation(pert_type, pert_level, experiment, rad=None):\n",
    "    \n",
    "    params = load_empirical_optimization_params(experiment, pert_level)\n",
    "    \n",
    "    if pert_type == 'sample_radius':\n",
    "        P = sample_from_frobenius_ball(params, 'hat_case', boundary=False, rand_rad=rad)\n",
    "    \n",
    "    elif pert_type == 'sample_radius_worst':\n",
    "        P = sample_from_frobenius_ball(params, 'worst_case', boundary=False, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'boundary_worst':\n",
    "        P = params['pert_U']\n",
    "        \n",
    "    elif pert_type == 'boundary_hat':\n",
    "        P = sample_from_frobenius_ball(params, 'hat_case', boundary=True, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'random_hat':\n",
    "        P = sample_from_frobenius_ball(params, 'hat_case', boundary=False, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'random_worst':\n",
    "        P = sample_from_frobenius_ball(params, 'worst_case', boundary=False, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'boundary_random_hat':\n",
    "        P = sample_from_frobenius_ball(params, 'hat_case', boundary=True, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'boundary_random_worst':\n",
    "        P = sample_from_frobenius_ball(params, 'worst_case', boundary=True, rand_rad=rad)\n",
    "\n",
    "    elif pert_type == 'random_normal':\n",
    "        P = np.random.randn(*params['pert_U'].shape)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3ee78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downstream_evaluation_with_structured_noise(\n",
    "    T, df_base, df_abst,\n",
    "    noise_type='gaussian',  # or 'frobenius'\n",
    "    noise_level=0.1,\n",
    "    noise_in='base',  # 'base', 'abst', or 'both'\n",
    "    pert_type=None,\n",
    "    experiment=None,\n",
    "    radius_L=None,\n",
    "    radius_H=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate abstraction robustness with either Gaussian or Frobenius-ball structured noise.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    df_base_noisy = df_base.copy().astype(float)\n",
    "    df_abst_noisy = df_abst.copy().astype(float)\n",
    "\n",
    "    if noise_type == 'gaussian':\n",
    "        if noise_in in ['base', 'both']:\n",
    "            df_base_noisy += np.random.normal(0, noise_level, df_base.shape)\n",
    "        if noise_in in ['abst', 'both']:\n",
    "            df_abst_noisy += np.random.normal(0, noise_level, df_abst.shape)\n",
    "\n",
    "    elif noise_type == 'frobenius':\n",
    "\n",
    "        if noise_in in ['base', 'both']:\n",
    "            assert pert_type is not None and experiment is not None\n",
    "            pert_L = generate_pertubation(pert_type, 'L', experiment, rad=radius_L)\n",
    "            df_base_noisy += pert_L\n",
    "        if noise_in in ['abst', 'both']:\n",
    "            assert pert_type is not None and experiment is not None\n",
    "            pert_H = generate_pertubation(pert_type, 'H', experiment, rad=radius_H)\n",
    "            df_abst_noisy += pert_H\n",
    "\n",
    "    tau_samples = T @ df_base_noisy.T\n",
    "    abst_samples = df_abst_noisy.T\n",
    "\n",
    "    X_real = abst_samples.T\n",
    "    X_gen = tau_samples.T\n",
    "\n",
    "    y = df_abst_noisy[:, 1]\n",
    "    groups = df_abst_noisy[:, 0]\n",
    "\n",
    "    assert X_real.shape[0] == len(y) == len(groups), \"Mismatch in sample counts\"\n",
    "\n",
    "    X_all = np.concatenate([X_real, X_gen], axis=0)\n",
    "    y_all = np.concatenate([y, y], axis=0)\n",
    "    groups_all = np.concatenate([groups, groups], axis=0)\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    mse_real = []\n",
    "    for train_idx, test_idx in logo.split(X_real, y, groups=groups):\n",
    "        model = Lasso().fit(X_real[train_idx], y[train_idx])\n",
    "        y_pred = model.predict(X_real[test_idx])\n",
    "        mse_real.append(mean_squared_error(y[test_idx], y_pred))\n",
    "\n",
    "    mse_aug = []\n",
    "    for train_idx, test_idx in logo.split(X_real, y, groups=groups):\n",
    "        model = Lasso().fit(X_gen[train_idx], y[train_idx])\n",
    "        y_pred = model.predict(X_real[test_idx])\n",
    "        mse_aug.append(mean_squared_error(y[test_idx], y_pred))\n",
    "\n",
    "    mse_mix = []\n",
    "    for test_group in np.unique(groups):\n",
    "        test_mask = (groups == test_group)\n",
    "        test_idx_real = np.where(test_mask)[0]\n",
    "\n",
    "        train_mask_real = (groups != test_group)\n",
    "        train_idx_real = np.where(train_mask_real)[0]\n",
    "        train_idx_gen = np.arange(len(y)) + len(y)\n",
    "        train_idx_all = np.concatenate([train_idx_real, train_idx_gen])\n",
    "\n",
    "        model = Lasso().fit(X_all[train_idx_all], y_all[train_idx_all])\n",
    "        y_pred = model.predict(X_real[test_idx_real])\n",
    "        mse_mix.append(mean_squared_error(y[test_idx_real], y_pred))\n",
    "\n",
    "    return {\n",
    "        \"Real→Real\": (np.mean(mse_real), np.std(mse_real)),\n",
    "        \"Aug→Real\": (np.mean(mse_aug), np.std(mse_aug)),\n",
    "        \"Real+Aug→Real\": (np.mean(mse_mix), np.std(mse_mix))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29e50549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_empirical_optimization_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m downstream_evaluation_with_structured_noise(T, df_base, df_abst, noise_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m'\u001b[39m, noise_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, noise_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Frobenius-ball structured noise (requires experiment + params)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m downstream_evaluation_with_structured_noise(\n\u001b[1;32m      6\u001b[0m     T, df_base, df_abst,\n\u001b[1;32m      7\u001b[0m     noise_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrobenius\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     noise_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     pert_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_worst\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyExperiment\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     radius_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m     radius_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn[68], line 33\u001b[0m, in \u001b[0;36mdownstream_evaluation_with_structured_noise\u001b[0;34m(T, df_base, df_abst, noise_type, noise_level, noise_in, pert_type, experiment, radius_L, radius_H)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise_in \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pert_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m experiment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     pert_L \u001b[38;5;241m=\u001b[39m generate_pertubation(pert_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m, experiment, rad\u001b[38;5;241m=\u001b[39mradius_L)\n\u001b[1;32m     34\u001b[0m     df_base_noisy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pert_L\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise_in \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[67], line 3\u001b[0m, in \u001b[0;36mgenerate_pertubation\u001b[0;34m(pert_type, pert_level, experiment, rad)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_pertubation\u001b[39m(pert_type, pert_level, experiment, rad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     params \u001b[38;5;241m=\u001b[39m load_empirical_optimization_params(experiment, pert_level)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pert_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_radius\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m         P \u001b[38;5;241m=\u001b[39m sample_from_frobenius_ball(params, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhat_case\u001b[39m\u001b[38;5;124m'\u001b[39m, boundary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rand_rad\u001b[38;5;241m=\u001b[39mrad)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_empirical_optimization_params' is not defined"
     ]
    }
   ],
   "source": [
    "# Gaussian noise in base-level only\n",
    "downstream_evaluation_with_structured_noise(T, df_base, df_abst, noise_type='gaussian', noise_level=0.5, noise_in='base')\n",
    "\n",
    "# Frobenius-ball structured noise (requires experiment + params)\n",
    "downstream_evaluation_with_structured_noise(\n",
    "    T, df_base, df_abst,\n",
    "    noise_type='frobenius',\n",
    "    noise_in='both',\n",
    "    pert_type='random_worst',\n",
    "    experiment='MyExperiment',\n",
    "    radius_L=3,\n",
    "    radius_H=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83169eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
