{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35fe35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/bin/sh: brew: command not found\n",
      "\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "import utilities as ut\n",
    "import modularised_utils as mut\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dbee5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded for 'lilucas'.\n"
     ]
    }
   ],
   "source": [
    "experiment = 'lilucas'\n",
    "setting    = 'empirical'\n",
    "\n",
    "if setting == 'gaussian':\n",
    "    path = f\"data/{experiment}/results\"\n",
    "\n",
    "elif setting == 'empirical':\n",
    "    path = f\"data/{experiment}/results_empirical\"\n",
    "\n",
    "saved_folds = joblib.load(f\"data/{experiment}/cv_folds.pkl\")\n",
    "\n",
    "# Load the original data dictionary\n",
    "all_data      = ut.load_all_data(experiment)\n",
    "\n",
    "Dll_samples   = all_data['LLmodel']['data']\n",
    "Dhl_samples   = all_data['HLmodel']['data']\n",
    "I_ll_relevant = all_data['LLmodel']['intervention_set']\n",
    "omega         = all_data['abstraction_data']['omega']\n",
    "ll_var_names  = list(all_data['LLmodel']['graph'].nodes())\n",
    "hl_var_names  = list(all_data['HLmodel']['graph'].nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "34efba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Methods available for evaluation:\n",
      "  - Abslin_p\n",
      "  - Abslin_n\n",
      "  - DiRoCA_star\n",
      "  - DIROCA_1\n",
      "  - DIROCA_2\n",
      "  - DIROCA_4\n",
      "  - DIROCA_8\n",
      "  - GradCA\n",
      "  - BARYCA\n"
     ]
    }
   ],
   "source": [
    "# Load dictionaries containing the results for each optimization method\n",
    "if setting == 'gaussian':\n",
    "    diroca_results = joblib.load(f\"{path}/diroca_cv_results.pkl\")\n",
    "    gradca_results = joblib.load(f\"{path}/gradca_cv_results.pkl\")\n",
    "    baryca_results = joblib.load(f\"{path}/baryca_cv_results.pkl\")\n",
    "\n",
    "elif setting == 'empirical':\n",
    "    diroca_results = joblib.load(f\"{path}/diroca_cv_results_empirical.pkl\")\n",
    "    gradca_results = joblib.load(f\"{path}/gradca_cv_results_empirical.pkl\")\n",
    "    baryca_results = joblib.load(f\"{path}/baryca_cv_results_empirical.pkl\")\n",
    "    abslingam_results = joblib.load(f\"{path}/abslingam_cv_results_empirical.pkl\")\n",
    "\n",
    "results_to_evaluate = {}\n",
    "\n",
    "if setting == 'empirical':\n",
    "    if abslingam_results:\n",
    "        first_fold_key = list(abslingam_results.keys())[0]\n",
    "        for style in abslingam_results[first_fold_key].keys():\n",
    "            method_name = f\"Abs-LiNGAM ({style})\"\n",
    "            new_abslingam_dict = {}\n",
    "            for fold_key, fold_results in abslingam_results.items():\n",
    "                if style in fold_results:\n",
    "                    new_abslingam_dict[fold_key] = {style: fold_results[style]}\n",
    "            results_to_evaluate[method_name] = new_abslingam_dict\n",
    "    \n",
    "    def create_diroca_label(run_id):\n",
    "        \"\"\"Parses a run_id and creates a simplified label if epsilon and delta are equal.\"\"\"\n",
    "        # Use regular expression to find numbers for epsilon and delta\n",
    "        matches = re.findall(r'(\\d+\\.?\\d*)', run_id)\n",
    "        if len(matches) == 2:\n",
    "            eps, delta = matches\n",
    "            # If they are the same, use the simplified format\n",
    "            if eps == delta:\n",
    "                # Handle integer conversion for clean labels like '1' instead of '1.0'\n",
    "                val = int(float(eps)) if float(eps).is_integer() else float(eps)\n",
    "                return f\"DIROCA (eps_delta_{val})\"\n",
    "        # Otherwise, or if parsing fails, use the full original name\n",
    "        return f\"DIROCA ({run_id})\"\n",
    "\n",
    "    # Unpack each DIROCA hyperparameter run with the new, clean label\n",
    "    if diroca_results:\n",
    "        first_fold_key = list(diroca_results.keys())[0]\n",
    "        for run_id in diroca_results[first_fold_key].keys():\n",
    "            method_name = create_diroca_label(run_id) # Use the new helper to create the name\n",
    "            new_diroca_dict = {}\n",
    "            for fold_key, fold_results in diroca_results.items():\n",
    "                if run_id in fold_results:\n",
    "                    new_diroca_dict[fold_key] = {run_id: fold_results[run_id]}\n",
    "            results_to_evaluate[method_name] = new_diroca_dict\n",
    "\n",
    "    results_to_evaluate['GradCA'] = gradca_results\n",
    "    results_to_evaluate['BARYCA'] = baryca_results\n",
    "\n",
    "elif setting == 'gaussian':\n",
    "    results_to_evaluate['GradCA'] = gradca_results\n",
    "    results_to_evaluate['BARYCA'] = baryca_results\n",
    "\n",
    "    if diroca_results:\n",
    "        first_fold_key = list(diroca_results.keys())[0]\n",
    "        diroca_run_ids = list(diroca_results[first_fold_key].keys())\n",
    "\n",
    "        # create a separate entry for each DIROCA run\n",
    "        for run_id in diroca_run_ids:\n",
    "            method_name = f\"DIROCA ({run_id})\"\n",
    "            \n",
    "            new_diroca_dict = {}\n",
    "            for fold_key, fold_results in diroca_results.items():\n",
    "                # For each fold grab the data for the current run_id\n",
    "                if run_id in fold_results:\n",
    "                    new_diroca_dict[fold_key] = {run_id: fold_results[run_id]}\n",
    "            \n",
    "            results_to_evaluate[method_name] = new_diroca_dict\n",
    "\n",
    "label_map_gaussian = {\n",
    "                        'DIROCA (eps_delta_0.111)': 'DiRoCA_star',\n",
    "                        'DIROCA (eps_delta_1)': 'DIROCA_1',\n",
    "                        'DIROCA (eps_delta_2)': 'DIROCA_2',\n",
    "                        'DIROCA (eps_delta_4)': 'DIROCA_4',\n",
    "                        'DIROCA (eps_delta_8)': 'DIROCA_8',\n",
    "                        'GradCA': 'GradCA',\n",
    "                        'BARYCA': 'BARYCA'\n",
    "                    }\n",
    "\n",
    "label_map_empirical = {\n",
    "                        'DIROCA (eps_0.328_delta_0.107)': 'DiRoCA_star',\n",
    "                        'DIROCA (eps_delta_1)': 'DIROCA_1',\n",
    "                        'DIROCA (eps_delta_2)': 'DIROCA_2',\n",
    "                        'DIROCA (eps_delta_4)': 'DIROCA_4',\n",
    "                        'DIROCA (eps_delta_8)': 'DIROCA_8',\n",
    "                        'GradCA': 'GradCA',\n",
    "                        'BARYCA': 'BARYCA',\n",
    "                        'Abs-LiNGAM (Perfect)': 'Abslin_p',\n",
    "                        'Abs-LiNGAM (Noisy)': 'Abslin_n'\n",
    "                    }\n",
    "\n",
    "if setting == 'empirical':\n",
    "    results_to_evaluate = {label_map_empirical.get(key, key): value for key, value in results_to_evaluate.items()}\n",
    "\n",
    "elif setting == 'gaussian':\n",
    "    results_to_evaluate = {label_map_gaussian.get(key, key): value for key, value in results_to_evaluate.items()}\n",
    "\n",
    "print(\"\\nMethods available for evaluation:\")\n",
    "for key in results_to_evaluate.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a3075",
   "metadata": {},
   "source": [
    "## Ï‰ contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cbb91294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_omega_map(original_omega, num_misalignments):\n",
    "    \n",
    "    \"\"\"Randomly re-wires a subset of entries in the omega map.\"\"\"\n",
    "    \n",
    "    omega_keys = [k for k in original_omega.keys() if k is not None]\n",
    "    omega_vals = [original_omega[k] for k in omega_keys if original_omega[k] is not None]\n",
    "\n",
    "    contaminated_omega = original_omega.copy()\n",
    "    \n",
    "    # Bound the number of misalignments by the number of eligible keys.\n",
    "    num_to_corrupt = min(num_misalignments, len(omega_keys))\n",
    "    # Randomly select keys to corrupt.\n",
    "    to_corrupt = random.sample(omega_keys, k=num_to_corrupt)\n",
    "    \n",
    "    # Create a random permutation of available targets (ensuring change)\n",
    "    # Use the set of targets from eligible keys.\n",
    "    all_targets = list(set(omega_vals))\n",
    "\n",
    "    for key in to_corrupt:\n",
    "        original_target = original_omega[key]\n",
    "        # Only corrupt if there's an alternative available.\n",
    "        available_targets = [t for t in all_targets if t != original_target]\n",
    "        if available_targets:\n",
    "            new_target = random.choice(available_targets)\n",
    "            contaminated_omega[key] = new_target\n",
    "            \n",
    "    return contaminated_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb8b1a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(6, 6)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(max_misalignments, max_misalignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0b7c75f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega-misspecification evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Misalignment Level:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Misalignment Level: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Omega-Misspecification Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_misalignments = len(I_ll_relevant) \n",
    "misalignment_levels = [max_misalignments] #range(0, max_misalignments)\n",
    "\n",
    "# Number of random contaminations to average over for each setting\n",
    "num_trials = 5\n",
    "\n",
    "omega_spec_records = []\n",
    "print(\"Omega-misspecification evaluation\")\n",
    "\n",
    "for num_misalignments in tqdm(misalignment_levels, desc=\"Misalignment Level\"):\n",
    "    for trial in range(num_trials):\n",
    "        rng_seed = 10_000 * num_misalignments + trial\n",
    "        random.seed(rng_seed)\n",
    "        np.random.seed(rng_seed % (2**32 - 1))  \n",
    "\n",
    "        # Create a new scrambled omega map\n",
    "        omega_cont = contaminate_omega_map(omega, num_misalignments)\n",
    "        \n",
    "        for i, fold_info in enumerate(saved_folds):\n",
    "            for method_name, results_dict in results_to_evaluate.items():\n",
    "                fold_results = results_dict.get(f'fold_{i}', {})\n",
    "                for run_key, run_data in fold_results.items():\n",
    "\n",
    "                    if 'DIROCA' in method_name:\n",
    "                        method_label = method_name\n",
    "                    else:\n",
    "                        method_label = method_name\n",
    "\n",
    "                    T_learned = run_data['T_matrix']\n",
    "                    test_indices = run_data['test_indices']\n",
    "                    \n",
    "                    errors_per_intervention = []\n",
    "                    for iota in I_ll_relevant:\n",
    "                        Dll_test = Dll_samples[iota][test_indices]\n",
    "                        # Use the contaminated omega map\n",
    "                        Dhl_test = Dhl_samples[omega_cont[iota]][test_indices]\n",
    "                        \n",
    "                        if setting == 'gaussian':\n",
    "                            error = ut.calculate_abstraction_error(T_learned, Dll_test, Dhl_test)\n",
    "                        elif setting == 'empirical':\n",
    "                            error = ut.calculate_empirical_error(T_learned, Dll_test, Dhl_test)\n",
    "                            \n",
    "                        if not np.isnan(error): errors_per_intervention.append(error)\n",
    "                    \n",
    "                    avg_error = np.mean(errors_per_intervention) if errors_per_intervention else np.nan\n",
    "                    \n",
    "                    record = {\n",
    "                                'method': method_label, \n",
    "                                'misalignments': num_misalignments,\n",
    "                                'trial': trial,\n",
    "                                'fold': i,\n",
    "                                'error': avg_error\n",
    "                            }\n",
    "                    omega_spec_records.append(record)\n",
    "\n",
    "omega_spec_df = pd.DataFrame(omega_spec_records)\n",
    "print(\"\\n\\n--- Omega-Misspecification Evaluation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7c4a5a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Overall Performance (Averaged Across All Misalignment Levels)\n",
      "=================================================================\n",
      "Method/Run                          | Mean Â± Std\n",
      "=================================================================\n",
      "GradCA                              | 311.6708 Â± 2.7129\n",
      "DIROCA_4                            | 313.4190 Â± 8.0703\n",
      "DIROCA_8                            | 313.5909 Â± 8.1378\n",
      "DIROCA_2                            | 317.2651 Â± 7.6389\n",
      "DIROCA_1                            | 334.4559 Â± 3.5699\n",
      "Abslin_n                            | 355.5886 Â± 2.8270\n",
      "Abslin_p                            | 400.5373 Â± 2.3690\n",
      "DiRoCA_star                         | 471.1535 Â± 4.3108\n",
      "BARYCA                              | 561.7655 Â± 4.1679\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Overall Performance (Averaged Across All Misalignment Levels)\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Method/Run':<35} | {'Mean Â± Std'}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "summary = omega_spec_df.groupby('method')['error'].agg(['mean', 'std', 'count'])\n",
    "summary['sem'] = summary['std'] #/ np.sqrt(summary['count'])\n",
    "# summary['ci95'] = 1.96 * summary['sem']\n",
    "summary['ci95'] = summary['sem']\n",
    "\n",
    "\n",
    "for method_name, row in summary.sort_values('mean').iterrows():\n",
    "    print(f\"{method_name:<35} | {row['mean']:.4f} Â± {row['ci95']:.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1444dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29c32eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best method: GradCA (mean error: 311.7912)\n",
      "\n",
      "Paired t-tests (best vs others):\n",
      "============================================================\n",
      "GradCA vs Abslin_n                       | p=0.0000 | diff=-44.4233 | Significant\n",
      "GradCA vs Abslin_p                       | p=0.0000 | diff=-89.6323 | Significant\n",
      "GradCA vs BARYCA                         | p=0.0000 | diff=-249.1023 | Significant\n",
      "GradCA vs DIROCA_1                       | p=0.0000 | diff=-22.4724 | Significant\n",
      "GradCA vs DIROCA_2                       | p=0.0000 | diff=-5.2820 | Significant\n",
      "GradCA vs DIROCA_4                       | p=0.0000 | diff=-1.3772 | Significant\n",
      "GradCA vs DIROCA_8                       | p=0.0000 | diff=-1.5516 | Significant\n",
      "GradCA vs DiRoCA_star                    | p=0.0000 | diff=-158.4792 | Significant\n"
     ]
    }
   ],
   "source": [
    "# Paired t-tests for contamination analysis\n",
    "from scipy.stats import ttest_rel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the best method (lowest mean error)\n",
    "best_method = summary['mean'].idxmin()\n",
    "best_mean = summary.loc[best_method, 'mean']\n",
    "\n",
    "print(f\"\\nBest method: {best_method} (mean error: {best_mean:.4f})\")\n",
    "print(\"\\nPaired t-tests (best vs others):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get data for best method\n",
    "best_data = omega_spec_df[omega_spec_df['method'] == best_method]['error'].values\n",
    "\n",
    "# Compare against all other methods\n",
    "for method in summary.index:\n",
    "    if method != best_method:\n",
    "        other_data = omega_spec_df[omega_spec_df['method'] == method]['error'].values\n",
    "        \n",
    "        # Ensure same number of observations for paired test\n",
    "        min_len = min(len(best_data), len(other_data))\n",
    "        if min_len > 0:\n",
    "            t_stat, p_value = ttest_rel(best_data[:min_len], other_data[:min_len])\n",
    "            other_mean = summary.loc[method, 'mean']\n",
    "            diff = best_mean - other_mean\n",
    "            \n",
    "            print(f\"{best_method} vs {method:<30} | p={p_value:.4f} | diff={diff:.4f} | {'Significant' if p_value < 0.05 else 'Not significant'}\")\n",
    "        else:\n",
    "            print(f\"{best_method} vs {method:<30} | No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af78037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad1e5ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omega-misspecification evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Misalignment Level: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:03<00:00,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Omega-Misspecification Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ======================================================================\n",
    "# 1. The Corrected Helper Function\n",
    "# ======================================================================\n",
    "def contaminate_omega_map(original_omega, num_misalignments, seed=None):\n",
    "    \"\"\"\n",
    "    Randomly re-wires a subset of entries in the omega map using a specific seed\n",
    "    for reproducible randomness.\n",
    "    \"\"\"\n",
    "    # Create a local random number generator for this specific run\n",
    "    rng = random.Random(seed)\n",
    "    \n",
    "    omega_keys = [k for k in original_omega.keys() if k is not None]\n",
    "    omega_vals = [original_omega[k] for k in omega_keys if original_omega[k] is not None]\n",
    "\n",
    "    contaminated_omega = original_omega.copy()\n",
    "    \n",
    "    num_to_corrupt = min(num_misalignments, len(omega_keys))\n",
    "    \n",
    "    # Use the local generator for sampling\n",
    "    to_corrupt = rng.sample(omega_keys, k=num_to_corrupt)\n",
    "    \n",
    "    all_targets = list(set(omega_vals))\n",
    "\n",
    "    for key in to_corrupt:\n",
    "        original_target = original_omega[key]\n",
    "        available_targets = [t for t in all_targets if t != original_target]\n",
    "        if available_targets:\n",
    "            # Use the local generator for choice\n",
    "            new_target = rng.choice(available_targets)\n",
    "            contaminated_omega[key] = new_target\n",
    "            \n",
    "    return contaminated_omega\n",
    "\n",
    "# ======================================================================\n",
    "# 2. The Corrected Evaluation Loop\n",
    "# ======================================================================\n",
    "max_misalignments = len(I_ll_relevant) -1\n",
    "misalignment_levels = np.linspace(0, max_misalignments, 11, dtype=int)\n",
    "num_trials = 3 # You can now increase this to get variance\n",
    "\n",
    "omega_spec_records = []\n",
    "print(\"Omega-misspecification evaluation\")\n",
    "\n",
    "for num_misalignments in tqdm(misalignment_levels, desc=\"Misalignment Level\"):\n",
    "    for trial in range(num_trials):\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Pass the trial number as the seed to get a different map for each trial\n",
    "        omega_cont = contaminate_omega_map(omega, num_misalignments, seed=trial)\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        for i, fold_info in enumerate(saved_folds):\n",
    "            for method_name, results_dict in results_to_evaluate.items():\n",
    "                fold_results = results_dict.get(f'fold_{i}', {})\n",
    "                for run_key, run_data in fold_results.items():\n",
    "\n",
    "                    if 'DIROCA' in method_name:\n",
    "                        method_label = method_name\n",
    "                    else:\n",
    "                        method_label = method_name\n",
    "\n",
    "                    T_learned = run_data['T_matrix']\n",
    "                    test_indices = run_data['test_indices']\n",
    "                    \n",
    "                    errors_per_intervention = []\n",
    "                    for iota in I_ll_relevant:\n",
    "                        Dll_test = Dll_samples[iota][test_indices]\n",
    "                        # Use the contaminated omega map\n",
    "                        Dhl_test = Dhl_samples[omega_cont[iota]][test_indices]\n",
    "                        \n",
    "                        if setting == 'gaussian':\n",
    "                            error = ut.calculate_abstraction_error(T_learned, Dll_test, Dhl_test)\n",
    "                        elif setting == 'empirical':\n",
    "                            error = ut.calculate_empirical_error(T_learned, Dll_test, Dhl_test)\n",
    "                            \n",
    "                        if not np.isnan(error): errors_per_intervention.append(error)\n",
    "                    \n",
    "                    avg_error = np.mean(errors_per_intervention) if errors_per_intervention else np.nan\n",
    "\n",
    "                    record = {\n",
    "                        'method': method_name, \n",
    "                        'misalignments': num_misalignments,\n",
    "                        'trial': trial,\n",
    "                        'fold': i,\n",
    "                        'error': avg_error\n",
    "                    }\n",
    "                    omega_spec_records.append(record)\n",
    "\n",
    "omega_spec_df = pd.DataFrame(omega_spec_records)\n",
    "print(\"\\n\\n--- Omega-Misspecification Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f2105cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Overall Performance (Averaged Across All Misalignment Levels)\n",
      "=================================================================\n",
      "Method/Run                          | Mean Â± Std\n",
      "=================================================================\n",
      "GradCA                              | 5.3669 Â± 0.0918\n",
      "DIROCA_2                            | 5.4990 Â± 0.0553\n",
      "BARYCA                              | 5.7128 Â± 0.0691\n",
      "DiRoCA_star                         | 5.7774 Â± 0.0988\n",
      "DIROCA_4                            | 6.7642 Â± 0.0776\n",
      "DIROCA_8                            | 6.7642 Â± 0.0776\n",
      "DIROCA_1                            | 6.8994 Â± 0.0813\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Overall Performance (Averaged Across All Misalignment Levels)\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Method/Run':<35} | {'Mean Â± Std'}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "summary = omega_spec_df.groupby('method')['error'].agg(['mean', 'std', 'count'])\n",
    "summary['sem'] = summary['std'] #/ np.sqrt(summary['count'])\n",
    "# summary['ci95'] = 1.96 * summary['sem']\n",
    "summary['ci95'] = summary['sem']\n",
    "\n",
    "\n",
    "for method_name, row in summary.sort_values('mean').iterrows():\n",
    "    print(f\"{method_name:<35} | {row['mean']:.4f} Â± {row['ci95']:.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bcb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9461ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a631595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c89b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e787b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5076f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8c876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
