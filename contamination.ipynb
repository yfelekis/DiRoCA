{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35fe35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "import utilities as ut\n",
    "import modularised_utils as mut\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded for 'slc'.\n"
     ]
    }
   ],
   "source": [
    "experiment = 'slc'\n",
    "setting    = 'empirical'\n",
    "\n",
    "if setting == 'gaussian':\n",
    "    path = f\"data/{experiment}/results\"\n",
    "\n",
    "elif setting == 'empirical':\n",
    "    path = f\"data/{experiment}/results_empirical\"\n",
    "\n",
    "saved_folds = joblib.load(f\"data/{experiment}/cv_folds.pkl\")\n",
    "\n",
    "# Load the original data dictionary\n",
    "all_data      = ut.load_all_data(experiment)\n",
    "\n",
    "Dll_samples   = all_data['LLmodel']['data']\n",
    "Dhl_samples   = all_data['HLmodel']['data']\n",
    "I_ll_relevant = all_data['LLmodel']['intervention_set']\n",
    "omega         = all_data['abstraction_data']['omega']\n",
    "ll_var_names  = list(all_data['LLmodel']['graph'].nodes())\n",
    "hl_var_names  = list(all_data['HLmodel']['graph'].nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34efba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Methods available for evaluation:\n",
      "  - Abslin_p\n",
      "  - Abslin_n\n",
      "  - DIROCA (eps_0.107_delta_0.035)\n",
      "  - DIROCA_1\n",
      "  - DIROCA_2\n",
      "  - DIROCA_4\n",
      "  - DIROCA_8\n",
      "  - GradCA\n",
      "  - BARYCA\n"
     ]
    }
   ],
   "source": [
    "# Load dictionaries containing the results for each optimization method\n",
    "if setting == 'gaussian':\n",
    "    diroca_results = joblib.load(f\"{path}/diroca_cv_results.pkl\")\n",
    "    gradca_results = joblib.load(f\"{path}/gradca_cv_results.pkl\")\n",
    "    baryca_results = joblib.load(f\"{path}/baryca_cv_results.pkl\")\n",
    "\n",
    "elif setting == 'empirical':\n",
    "    diroca_results = joblib.load(f\"{path}/diroca_cv_results_empirical.pkl\")\n",
    "    gradca_results = joblib.load(f\"{path}/gradca_cv_results_empirical.pkl\")\n",
    "    baryca_results = joblib.load(f\"{path}/baryca_cv_results_empirical.pkl\")\n",
    "    abslingam_results = joblib.load(f\"{path}/abslingam_cv_results_empirical.pkl\")\n",
    "\n",
    "results_to_evaluate = {}\n",
    "\n",
    "if setting == 'empirical':\n",
    "    if abslingam_results:\n",
    "        first_fold_key = list(abslingam_results.keys())[0]\n",
    "        for style in abslingam_results[first_fold_key].keys():\n",
    "            method_name = f\"Abs-LiNGAM ({style})\"\n",
    "            new_abslingam_dict = {}\n",
    "            for fold_key, fold_results in abslingam_results.items():\n",
    "                if style in fold_results:\n",
    "                    new_abslingam_dict[fold_key] = {style: fold_results[style]}\n",
    "            results_to_evaluate[method_name] = new_abslingam_dict\n",
    "    \n",
    "    def create_diroca_label(run_id):\n",
    "        \"\"\"Parses a run_id and creates a simplified label if epsilon and delta are equal.\"\"\"\n",
    "        # Use regular expression to find numbers for epsilon and delta\n",
    "        matches = re.findall(r'(\\d+\\.?\\d*)', run_id)\n",
    "        if len(matches) == 2:\n",
    "            eps, delta = matches\n",
    "            # If they are the same, use the simplified format\n",
    "            if eps == delta:\n",
    "                # Handle integer conversion for clean labels like '1' instead of '1.0'\n",
    "                val = int(float(eps)) if float(eps).is_integer() else float(eps)\n",
    "                return f\"DIROCA (eps_delta_{val})\"\n",
    "        # Otherwise, or if parsing fails, use the full original name\n",
    "        return f\"DIROCA ({run_id})\"\n",
    "\n",
    "    # Unpack each DIROCA hyperparameter run with the new, clean label\n",
    "    if diroca_results:\n",
    "        first_fold_key = list(diroca_results.keys())[0]\n",
    "        for run_id in diroca_results[first_fold_key].keys():\n",
    "            method_name = create_diroca_label(run_id) # Use the new helper to create the name\n",
    "            new_diroca_dict = {}\n",
    "            for fold_key, fold_results in diroca_results.items():\n",
    "                if run_id in fold_results:\n",
    "                    new_diroca_dict[fold_key] = {run_id: fold_results[run_id]}\n",
    "            results_to_evaluate[method_name] = new_diroca_dict\n",
    "\n",
    "    results_to_evaluate['GradCA'] = gradca_results\n",
    "    results_to_evaluate['BARYCA'] = baryca_results\n",
    "\n",
    "elif setting == 'gaussian':\n",
    "    results_to_evaluate['GradCA'] = gradca_results\n",
    "    results_to_evaluate['BARYCA'] = baryca_results\n",
    "\n",
    "    if diroca_results:\n",
    "        first_fold_key = list(diroca_results.keys())[0]\n",
    "        diroca_run_ids = list(diroca_results[first_fold_key].keys())\n",
    "\n",
    "        # create a separate entry for each DIROCA run\n",
    "        for run_id in diroca_run_ids:\n",
    "            method_name = f\"DIROCA ({run_id})\"\n",
    "            \n",
    "            new_diroca_dict = {}\n",
    "            for fold_key, fold_results in diroca_results.items():\n",
    "                # For each fold grab the data for the current run_id\n",
    "                if run_id in fold_results:\n",
    "                    new_diroca_dict[fold_key] = {run_id: fold_results[run_id]}\n",
    "            \n",
    "            results_to_evaluate[method_name] = new_diroca_dict\n",
    "\n",
    "label_map_gaussian = {\n",
    "                        'DIROCA (eps_delta_0.111)': 'DiRoCA_star',\n",
    "                        'DIROCA (eps_delta_1)': 'DIROCA_1',\n",
    "                        'DIROCA (eps_delta_2)': 'DIROCA_2',\n",
    "                        'DIROCA (eps_delta_4)': 'DIROCA_4',\n",
    "                        'DIROCA (eps_delta_8)': 'DIROCA_8',\n",
    "                        'GradCA': 'GradCA',\n",
    "                        'BARYCA': 'BARYCA'\n",
    "                    }\n",
    "\n",
    "label_map_empirical = {\n",
    "                        'DIROCA (eps_0.328_delta_0.107)': 'DiRoCA_star',\n",
    "                        'DIROCA (eps_delta_1)': 'DIROCA_1',\n",
    "                        'DIROCA (eps_delta_2)': 'DIROCA_2',\n",
    "                        'DIROCA (eps_delta_4)': 'DIROCA_4',\n",
    "                        'DIROCA (eps_delta_8)': 'DIROCA_8',\n",
    "                        'GradCA': 'GradCA',\n",
    "                        'BARYCA': 'BARYCA',\n",
    "                        'Abs-LiNGAM (Perfect)': 'Abslin_p',\n",
    "                        'Abs-LiNGAM (Noisy)': 'Abslin_n'\n",
    "                    }\n",
    "\n",
    "if setting == 'empirical':\n",
    "    results_to_evaluate = {label_map_empirical.get(key, key): value for key, value in results_to_evaluate.items()}\n",
    "\n",
    "elif setting == 'gaussian':\n",
    "    results_to_evaluate = {label_map_gaussian.get(key, key): value for key, value in results_to_evaluate.items()}\n",
    "\n",
    "print(\"\\nMethods available for evaluation:\")\n",
    "for key in results_to_evaluate.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c72ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c662be07",
   "metadata": {},
   "source": [
    "## F-contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93a802d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_data(data, strength, contamination_type, num_segments=10, seed=None):\n",
    "    \"\"\"\n",
    "    Applies a specified contamination to data samples to simulate model misspecification.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The original data samples.\n",
    "        strength (float): The magnitude of the contamination.\n",
    "        contamination_type (str): 'piecewise', 'multiplicative', or 'nonlinear'.\n",
    "        num_segments (int): Number of segments for the 'piecewise' type.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The contaminated data.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data_cont = data.copy()\n",
    "    \n",
    "    if contamination_type == \"multiplicative\":\n",
    "        # Apply element-wise multiplicative noise\n",
    "        noise = rng.uniform(low=1.0 - strength, high=1.0 + strength, size=data.shape)\n",
    "        data_cont *= noise\n",
    "  \n",
    "    elif contamination_type == \"nonlinear\":\n",
    "        # Apply a sine-based non-linear distortion\n",
    "        data_cont += strength * np.sin(data_cont)\n",
    "  \n",
    "    elif contamination_type == \"piecewise\":\n",
    "        # Apply piecewise contamination to each column (variable)\n",
    "        for col_idx in range(data.shape[1]):\n",
    "            column = data_cont[:, col_idx]\n",
    "            breakpoints = np.quantile(column, q=np.linspace(0, 1, num_segments + 1))\n",
    "            breakpoints[-1] += 1e-6 # Ensure the last element is included\n",
    "            \n",
    "            for i in range(num_segments):\n",
    "                factor = 1.0 + rng.uniform(low=-strength, high=strength)\n",
    "                mask = (column >= breakpoints[i]) & (column < breakpoints[i+1])\n",
    "                data_cont[mask, col_idx] *= factor\n",
    "  \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown contamination type: {contamination_type}\")\n",
    "  \n",
    "    return data_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65ae7a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F 'multiplicative' misspecification evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Contamination Strength: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- F-Misspecification Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contamination_type_to_run = 'multiplicative' # Options: 'piecewise', 'multiplicative', 'nonlinear'\n",
    "\n",
    "# Range of contamination strengths to test\n",
    "contamination_strengths = np.linspace(0, 1, 10) \n",
    "\n",
    "# Number of random contaminations to average over for each setting\n",
    "num_trials = 10\n",
    "\n",
    "f_spec_records = []\n",
    "print(f\"F '{contamination_type_to_run}' misspecification evaluation\")\n",
    "\n",
    "for strength in tqdm(contamination_strengths, desc=\"Contamination Strength\"):\n",
    "    for trial in range(num_trials):\n",
    "        for i, fold_info in enumerate(saved_folds):\n",
    "            for method_name, results_dict in results_to_evaluate.items():\n",
    "                fold_results = results_dict.get(f'fold_{i}', {})\n",
    "                for run_key, run_data in fold_results.items():\n",
    "\n",
    "                    if 'DIROCA' in method_name:\n",
    "                        method_label = method_name\n",
    "                    else:\n",
    "                        method_label = method_name\n",
    "\n",
    "                    T_learned = run_data['T_matrix']\n",
    "                    test_indices = run_data['test_indices']\n",
    "                    \n",
    "                    errors_per_intervention = []\n",
    "                    for iota in I_ll_relevant:\n",
    "                        Dll_test_clean = Dll_samples[iota][test_indices]\n",
    "                        Dhl_test_clean = Dhl_samples[omega[iota]][test_indices]\n",
    "                        \n",
    "                        Dll_test_cont = contaminate_data(Dll_test_clean, strength, contamination_type_to_run, seed=trial)\n",
    "                        Dhl_test_cont = contaminate_data(Dhl_test_clean, strength, contamination_type_to_run, seed=trial)\n",
    "                        if setting == 'gaussian':\n",
    "                            error = ut.calculate_abstraction_error(T_learned, Dll_test_cont, Dhl_test_cont)\n",
    "                        elif setting == 'empirical':\n",
    "                            error = ut.calculate_empirical_error(T_learned, Dll_test_cont, Dhl_test_cont)\n",
    "                        if not np.isnan(error):\n",
    "                            errors_per_intervention.append(error)\n",
    "                    \n",
    "                    avg_error = np.mean(errors_per_intervention) if errors_per_intervention else np.nan\n",
    "                    \n",
    "                    record = {\n",
    "                        'method': method_label, \n",
    "                        'contamination': strength,\n",
    "                        'trial': trial,\n",
    "                        'fold': i,\n",
    "                        'error': avg_error\n",
    "                    }\n",
    "                    f_spec_records.append(record)\n",
    "\n",
    "f_spec_df = pd.DataFrame(f_spec_records)\n",
    "print(\"--- F-Misspecification Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21a783b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Overall Performance (Averaged Across All 'multiplicative' Strengths)\n",
      "=================================================================\n",
      "Method/Run                          | Mean ± Std\n",
      "=================================================================\n",
      "GradCA                              | 63.1590 ± 5.4351\n",
      "DIROCA_1                            | 69.7332 ± 4.9434\n",
      "DIROCA_2                            | 70.7777 ± 4.0304\n",
      "DIROCA_4                            | 70.7777 ± 4.0304\n",
      "DIROCA_8                            | 70.7777 ± 4.0304\n",
      "BARYCA                              | 95.4702 ± 8.7866\n",
      "DIROCA (eps_0.107_delta_0.035)      | 97.6368 ± 9.0061\n",
      "Abslin_n                            | 101.4545 ± 8.6654\n",
      "Abslin_p                            | 103.8440 ± 10.8534\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(f\"Overall Performance (Averaged Across All '{contamination_type_to_run}' Strengths)\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Method/Run':<35} | {'Mean ± Std'}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "summary = f_spec_df.groupby('method')['error'].agg(['mean', 'std', 'count'])\n",
    "summary['sem'] = summary['std'] #/ np.sqrt(summary['count'])\n",
    "# summary['ci95'] = 1.96 * summary['sem']\n",
    "summary['ci95'] = summary['sem']\n",
    "\n",
    "for method_name, row in summary.sort_values('mean').iterrows():\n",
    "    print(f\"{method_name:<35} | {row['mean']:.4f} ± {row['ci95']:.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a3075",
   "metadata": {},
   "source": [
    "## ω contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbb91294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_omega_map(original_omega, num_misalignments):\n",
    "    \n",
    "    \"\"\"Randomly re-wires a subset of entries in the omega map.\"\"\"\n",
    "    \n",
    "    omega_keys = [k for k in original_omega.keys() if k is not None]\n",
    "    omega_vals = [original_omega[k] for k in omega_keys if original_omega[k] is not None]\n",
    "\n",
    "    contaminated_omega = original_omega.copy()\n",
    "    \n",
    "    # Bound the number of misalignments by the number of eligible keys.\n",
    "    num_to_corrupt = min(num_misalignments, len(omega_keys))\n",
    "    # Randomly select keys to corrupt.\n",
    "    to_corrupt = random.sample(omega_keys, k=num_to_corrupt)\n",
    "    \n",
    "    # Create a random permutation of available targets (ensuring change)\n",
    "    # Use the set of targets from eligible keys.\n",
    "    all_targets = list(set(omega_vals))\n",
    "\n",
    "    for key in to_corrupt:\n",
    "        original_target = original_omega[key]\n",
    "        # Only corrupt if there's an alternative available.\n",
    "        available_targets = [t for t in all_targets if t != original_target]\n",
    "        if available_targets:\n",
    "            new_target = random.choice(available_targets)\n",
    "            contaminated_omega[key] = new_target\n",
    "            \n",
    "    return contaminated_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c75f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Omega-misspecification evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Misalignment Level: 100%|██████████| 6/6 [00:02<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Omega-Misspecification Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_misalignments = len(I_ll_relevant) \n",
    "misalignment_levels = range(0, max_misalignments)\n",
    "\n",
    "# Number of random contaminations to average over for each setting\n",
    "num_trials = 20\n",
    "\n",
    "omega_spec_records = []\n",
    "print(\"Omega-misspecification evaluation\")\n",
    "\n",
    "for num_misalignments in tqdm(misalignment_levels, desc=\"Misalignment Level\"):\n",
    "    for trial in range(num_trials):\n",
    "        # Create a new scrambled omega map\n",
    "        omega_cont = contaminate_omega_map(omega, num_misalignments)\n",
    "        \n",
    "        for i, fold_info in enumerate(saved_folds):\n",
    "            for method_name, results_dict in results_to_evaluate.items():\n",
    "                fold_results = results_dict.get(f'fold_{i}', {})\n",
    "                for run_key, run_data in fold_results.items():\n",
    "\n",
    "                    if 'DIROCA' in method_name:\n",
    "                        method_label = method_name\n",
    "                    else:\n",
    "                        method_label = method_name\n",
    "\n",
    "                    T_learned = run_data['T_matrix']\n",
    "                    test_indices = run_data['test_indices']\n",
    "                    \n",
    "                    errors_per_intervention = []\n",
    "                    for iota in I_ll_relevant:\n",
    "                        Dll_test = Dll_samples[iota][test_indices]\n",
    "                        # Use the contaminated omega map\n",
    "                        Dhl_test = Dhl_samples[omega_cont[iota]][test_indices]\n",
    "                        \n",
    "                        if setting == 'gaussian':\n",
    "                            error = ut.calculate_abstraction_error(T_learned, Dll_test, Dhl_test)\n",
    "                        elif setting == 'empirical':\n",
    "                            error = ut.calculate_empirical_error(T_learned, Dll_test, Dhl_test)\n",
    "                            \n",
    "                        if not np.isnan(error): errors_per_intervention.append(error)\n",
    "                    \n",
    "                    avg_error = np.mean(errors_per_intervention) if errors_per_intervention else np.nan\n",
    "                    \n",
    "                    record = {\n",
    "                                'method': method_label, \n",
    "                                'misalignments': num_misalignments,\n",
    "                                'trial': trial,\n",
    "                                'fold': i,\n",
    "                                'error': avg_error\n",
    "                            }\n",
    "                    omega_spec_records.append(record)\n",
    "\n",
    "omega_spec_df = pd.DataFrame(omega_spec_records)\n",
    "print(\"\\n\\n--- Omega-Misspecification Evaluation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c4a5a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "Overall Performance (Averaged Across All Misalignment Levels)\n",
      "=================================================================\n",
      "Method/Run                          | Mean ± Std\n",
      "=================================================================\n",
      "GradCA                              | 58.1176 ± 2.5301\n",
      "DIROCA_2                            | 60.6816 ± 4.6320\n",
      "DIROCA_4                            | 60.6816 ± 4.6320\n",
      "DIROCA_8                            | 60.6816 ± 4.6320\n",
      "DIROCA_1                            | 61.0272 ± 3.3619\n",
      "BARYCA                              | 88.2573 ± 4.0329\n",
      "Abslin_p                            | 91.0618 ± 3.5338\n",
      "DIROCA (eps_0.107_delta_0.035)      | 91.7016 ± 4.8322\n",
      "Abslin_n                            | 93.7674 ± 3.3887\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"Overall Performance (Averaged Across All Misalignment Levels)\")\n",
    "print(\"=\"*65)\n",
    "print(f\"{'Method/Run':<35} | {'Mean ± Std'}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "summary = omega_spec_df.groupby('method')['error'].agg(['mean', 'std', 'count'])\n",
    "summary['sem'] = summary['std'] #/ np.sqrt(summary['count'])\n",
    "# summary['ci95'] = 1.96 * summary['sem']\n",
    "summary['ci95'] = summary['sem']\n",
    "\n",
    "\n",
    "for method_name, row in summary.sort_values('mean').iterrows():\n",
    "    print(f\"{method_name:<35} | {row['mean']:.4f} ± {row['ci95']:.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af78037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
