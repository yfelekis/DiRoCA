{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Local modules\n",
    "import modularised_utils as mut\n",
    "import opt_utils as oput \n",
    "import evaluation_utils as evut\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import params\n",
    "import random\n",
    "\n",
    "from math_utils import compute_wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3532da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'lucas6x3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "003375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_results = joblib.load(f\"data/{experiment}/diroca_train_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "326791cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_estimation = True\n",
    "\n",
    "Dll_obs = joblib.load(f\"data/{experiment}/Dll_obs_test.pkl\")\n",
    "Dhl_obs = joblib.load(f\"data/{experiment}/Dhl_obs_test.pkl\")\n",
    "\n",
    "LLmodels = joblib.load(f\"data/{experiment}/LLmodels.pkl\")\n",
    "HLmodels = joblib.load(f\"data/{experiment}/HLmodels.pkl\")\n",
    "\n",
    "num_llsamples, num_hlsamples  = Dll_obs.shape[0], Dhl_obs.shape[0]\n",
    "\n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "\n",
    "n_varsll, n_varshl = len(Gll.nodes()), len(Ghl.nodes())\n",
    "\n",
    "omega    = mut.load_omega_map(experiment)\n",
    "\n",
    "if coeff_estimation == True:\n",
    "    ll_coeffs = mut.get_coefficients(Dll_obs, Gll)\n",
    "    hl_coeffs = mut.get_coefficients(Dhl_obs, Ghl) \n",
    "else:\n",
    "    ll_coeffs = mut.load_coeffs(experiment, 'LL')\n",
    "    hl_coeffs = mut.load_coeffs(experiment, 'HL')\n",
    "\n",
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll_obs, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl_obs, Ghl, hl_coeffs)\n",
    "\n",
    "data = evut.generate_data(LLmodels, HLmodels, omega, num_llsamples, num_hlsamples, mu_U_ll_hat, Sigma_U_ll_hat, mu_U_hl_hat, Sigma_U_hl_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80fce0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_observ        = True \n",
    "test_interv        = True\n",
    "num_iter           = 100\n",
    "metric             = 'wass'\n",
    "\n",
    "if test_observ and test_interv:\n",
    "    test_data = data\n",
    "\n",
    "elif test_observ:\n",
    "    test_data = {None: data[None]}\n",
    "\n",
    "elif test_interv:\n",
    "    test_data = {k: v for k, v in data.items() if k is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74d9ca",
   "metadata": {},
   "source": [
    "## 0-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20534b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has to be learnt with coeff = True as well. check opt_ell.ipynb'\n",
    "if coeff_estimation == True:\n",
    "    \n",
    "    results_single = {method: {'errors': [], 'mean': 0, 'ci': 0} for method in T_results.keys()}\n",
    "\n",
    "    for name, res in T_results.items():\n",
    "        T = res['T_matrix']\n",
    "        errors = []  # Store errors for each intervention\n",
    "        scale_factor = 1/np.sqrt(len(Ill))\n",
    "        wass_total = 0\n",
    "        for iota in Ill:\n",
    "            L_i = LLmodels[iota].F\n",
    "            V_i = T @ L_i\n",
    "            H_i = HLmodels[omega[iota]].F\n",
    "\n",
    "            muV    = V_i @ mu_U_ll_hat\n",
    "            sigmaV = V_i @ Sigma_U_ll_hat @ V_i.T\n",
    "            muH    = H_i @ mu_U_hl_hat\n",
    "            sigmaH = H_i @ Sigma_U_hl_hat @ H_i.T\n",
    "\n",
    "\n",
    "            # Compute Wasserstein metric\n",
    "            wass_dist = np.sqrt(mut.compute_wasserstein(muV, sigmaV, muH, sigmaH))\n",
    "            errors.append(wass_dist)\n",
    "            wass_total += wass_dist\n",
    "\n",
    "        # Calculate mean and CI\n",
    "        mean_error = np.mean(errors)\n",
    "        std_error = np.std(errors)\n",
    "        ci = std_error\n",
    "\n",
    "        # Store all statistics\n",
    "        results_single[name] = {\n",
    "            'errors': errors,\n",
    "            'mean': mean_error,\n",
    "            'ci': ci\n",
    "        }\n",
    "\n",
    "    results_single = dict(sorted(results_single.items(), key=lambda x: x[1]['mean']))\n",
    "    ll_coeffs = mut.load_coeffs(experiment, 'LL')\n",
    "    hl_coeffs = mut.load_coeffs(experiment, 'HL')\n",
    "else:\n",
    "    print('No coeff estimation')\n",
    "    \n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'Method':<15} {'Error (mean ± CI)':<35}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for method, stats in results_single.items():\n",
    "    print(f\"{method:<15} {stats['mean']:>8.4f} ± {stats['ci']:<8.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df493378",
   "metadata": {},
   "source": [
    "## ρ-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b64a080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_values = np.arange(0.05, 100.05, 10).tolist()  \n",
    "sample_forms = ['boundary', 'sample']\n",
    "\n",
    "center   = 'worst'\n",
    "coverage_type = 'uniform'\n",
    "\n",
    "hat_dict = {'L': [mu_U_ll_hat, Sigma_U_ll_hat], 'H': [mu_U_hl_hat, Sigma_U_hl_hat]}\n",
    "\n",
    "worst = 'T_8'\n",
    "mu_worst_L    = T_results[worst]['optimization_params']['L']['mu_U']\n",
    "Sigma_worst_L = T_results[worst]['optimization_params']['L']['Sigma_U']\n",
    "mu_worst_H    = T_results[worst]['optimization_params']['H']['mu_U']\n",
    "Sigma_worst_H = T_results[worst]['optimization_params']['H']['Sigma_U']\n",
    "\n",
    "worst_dict = {'L': [mu_worst_L, Sigma_worst_L], 'H': [mu_worst_H, Sigma_worst_H]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the r_sigma values to sweep over\n",
    "sigma_values = np.linspace(0, 1, 100)\n",
    "methods_to_track = list(T_results.keys())\n",
    "\n",
    "# Storage for plotting and for mean/CI across all sigmas\n",
    "error_evolution = {method: [] for method in methods_to_track}\n",
    "mean_across_sigmas = {method: [] for method in methods_to_track}\n",
    "ci_across_sigmas = {method: [] for method in methods_to_track}\n",
    "\n",
    "for r_sigma in sigma_values:\n",
    "    #print(f\"Testing with r_sigma = {r_sigma}\")\n",
    "    # Generate shifted Gaussian families for this sigma\n",
    "    shift_family_L = mut.generate_shifted_gaussian_family(\n",
    "        mu_worst_L, Sigma_worst_L, 1, r_mu=0, r_sigma=r_sigma, coverage='rand', seed=None)\n",
    "    shift_family_H = mut.generate_shifted_gaussian_family(\n",
    "        mu_worst_H, Sigma_worst_H, 1, r_mu=0, r_sigma=r_sigma, coverage='rand', seed=None)\n",
    "\n",
    "    # Initialize results for this sigma\n",
    "    results = {method: [] for method in methods_to_track}\n",
    "\n",
    "    for shift_L, shift_H in zip(shift_family_L, shift_family_H):\n",
    "        noise_muL, noise_SigmaL = shift_L\n",
    "        noise_muH, noise_SigmaH = shift_H\n",
    "        noise_muL = noise_muL.numpy() if hasattr(noise_muL, 'numpy') else noise_muL\n",
    "        noise_muH = noise_muH.numpy() if hasattr(noise_muH, 'numpy') else noise_muH\n",
    "        noise_SigmaL = noise_SigmaL.numpy() if hasattr(noise_SigmaL, 'numpy') else noise_SigmaL\n",
    "        noise_SigmaH = noise_SigmaH.numpy() if hasattr(noise_SigmaH, 'numpy') else noise_SigmaH\n",
    "\n",
    "        for name in methods_to_track:\n",
    "            res = T_results[name]\n",
    "            T = res['T_matrix']\n",
    "            wass_total = 0\n",
    "            for iota in Ill:\n",
    "                L_i = LLmodels[iota].F\n",
    "                V_i = T @ L_i\n",
    "                H_i = HLmodels[omega[iota]].F\n",
    "                muV = V_i @ noise_muL\n",
    "                sigmaV = V_i @ noise_SigmaL @ V_i.T\n",
    "                muH = H_i @ noise_muH\n",
    "                sigmaH = H_i @ noise_SigmaH @ H_i.T\n",
    "                wass_dist = np.sqrt(compute_wasserstein(muV, sigmaV, muH, sigmaH))\n",
    "                wass_total += wass_dist\n",
    "            results[name].append(wass_total / len(Ill))\n",
    "\n",
    "    # Store mean and CI for each method for this sigma\n",
    "    for method in methods_to_track:\n",
    "        mean = np.mean(results[method])\n",
    "        std = np.std(results[method])\n",
    "        ci = std/10\n",
    "        error_evolution[method].append(mean)\n",
    "        mean_across_sigmas[method].append(mean)\n",
    "        ci_across_sigmas[method].append(ci)\n",
    "\n",
    "# Print mean and CI across all sigmas for each method\n",
    "print(f\"\\n{'Method':<15} {'Mean across sigmas ± 1.96*Std':<35}\")\n",
    "print(\"=\"*50)\n",
    "method_stats = []\n",
    "for method in methods_to_track:\n",
    "    mean_over_sigmas = np.mean(mean_across_sigmas[method])\n",
    "    std_over_sigmas = np.std(mean_across_sigmas[method])\n",
    "    ci_over_sigmas = std_over_sigmas\n",
    "    method_stats.append((method, mean_over_sigmas, ci_over_sigmas))\n",
    "# Sort by mean, descending (worst to best)\n",
    "method_stats.sort(key=lambda x: x[1], reverse=True)\n",
    "for method, mean, ci in method_stats:\n",
    "    print(f\"{method:<15} {mean:8.4f} ± {ci:<8.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f732879",
   "metadata": {},
   "source": [
    "## F-contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed9c2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_structural_matrix(M, contamination_fraction, contamination_type, num_segments=10, seed=None):\n",
    "   \"\"\"\n",
    "   Contaminates a linear transformation matrix M to break its strict linearity.\n",
    "  \n",
    "   Args:\n",
    "       M (np.ndarray): Original linear transformation matrix (n x m).\n",
    "       contamination_fraction (float): Magnitude of contamination (e.g., between 0.05 and 1.0).\n",
    "       contamination_type (str): Type of contamination to apply. Options are:\n",
    "                                 'multiplicative', 'nonlinear', or 'piecewise'.\n",
    "       num_segments (int): Number of segments for piecewise linear contamination (default: 3).\n",
    "       seed (int, optional): Random seed for reproducibility.\n",
    "      \n",
    "   Returns:\n",
    "       np.ndarray: The contaminated matrix.\n",
    "   \"\"\"\n",
    "   rng = np.random.default_rng(seed)\n",
    "   M_cont = M.copy() \n",
    "   n, m = M.shape\n",
    "\n",
    "\n",
    "   if contamination_type == \"multiplicative\":\n",
    "       # Apply element-wise multiplicative noise (preserving zeros below the main diagonal)\n",
    "       # Only perturb the upper-triangular part.\n",
    "       noise = rng.uniform(low=1.0 - contamination_fraction, high=1.0 + contamination_fraction, size=M.shape)\n",
    "       # Create a mask for the upper triangular (including diagonal)\n",
    "       mask = np.triu(np.ones_like(M))\n",
    "       M_cont = M * (1 - mask + mask * noise)\n",
    "  \n",
    "   elif contamination_type == \"nonlinear\":\n",
    "       # Apply a nonlinear function to L: for instance, add a sine-based perturbation.\n",
    "       M_cont = M + contamination_fraction * np.sin(M)\n",
    "  \n",
    "   elif contamination_type == \"piecewise\":\n",
    "       # Contaminate each row with a piecewise linear function.\n",
    "       def piecewise_contaminate_row(row, cont_frac, segments, rng):\n",
    "           n_elem = len(row)\n",
    "           # Choose random breakpoints among indices\n",
    "           if segments < 2:\n",
    "               return row  # nothing to do\n",
    "           breakpoints = np.sort(rng.integers(low=1, high=n_elem, size=segments - 1))\n",
    "           breakpoints = np.concatenate(([0], breakpoints, [n_elem]))\n",
    "           contaminated_row = np.empty_like(row)\n",
    "           # For each segment, assign a random multiplicative factor.\n",
    "           for j in range(len(breakpoints) - 1):\n",
    "               start = breakpoints[j]\n",
    "               end = breakpoints[j+1]\n",
    "               factor = 1.0 + rng.uniform(low=-cont_frac, high=cont_frac)\n",
    "               contaminated_row[start:end] = row[start:end] * factor\n",
    "           return contaminated_row\n",
    "      \n",
    "       # Apply the piecewise contamination row-by-row.\n",
    "       for i in range(n):\n",
    "           M_cont[i, :] = piecewise_contaminate_row(M[i, :], contamination_fraction, num_segments, rng)\n",
    "  \n",
    "   else:\n",
    "       raise ValueError(\"Unknown contamination type. Choose among 'multiplicative', 'nonlinear', or 'piecewise'.\")\n",
    "  \n",
    "   return M_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea99392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contamination levels to test\n",
    "contamination_levels = np.linspace(0.0, 1.0, 100)  \n",
    "for cont_type in ['piecewise']:\n",
    "    print(f\"Contamination type: {cont_type}\")\n",
    "    # Store results for plotting\n",
    "    plot_results = {method: {'means': [], 'stds': []} for method in T_results.keys()}\n",
    "\n",
    "\n",
    "    # Run experiment for each contamination level\n",
    "    for cont_frac in tqdm(contamination_levels):\n",
    "        abstraction_error = {name: [] for name in T_results.keys()}\n",
    "    \n",
    "        for _ in range(1):\n",
    "            noise_muL, noise_SigmaL = mu_U_ll_hat, Sigma_U_ll_hat\n",
    "            noise_muH, noise_SigmaH = mu_U_hl_hat, Sigma_U_hl_hat\n",
    "            \n",
    "            noise_muL    = noise_muL.numpy() if torch.is_tensor(noise_muL) else noise_muL\n",
    "            noise_muH    = noise_muH.numpy() if torch.is_tensor(noise_muH) else noise_muH\n",
    "            noise_SigmaL = noise_SigmaL.numpy() if torch.is_tensor(noise_SigmaL) else noise_SigmaL\n",
    "            noise_SigmaH = noise_SigmaH.numpy() if torch.is_tensor(noise_SigmaH) else noise_SigmaH\n",
    "\n",
    "            for name, res in T_results.items():\n",
    "                T = res['T_matrix']\n",
    "                total = 0\n",
    "                for iota in Ill:\n",
    "                    L_i = LLmodels[iota].F\n",
    "                    L_i = contaminate_structural_matrix(L_i, contamination_fraction=cont_frac, contamination_type=cont_type)\n",
    "                    V_i = T @ L_i\n",
    "                    H_i = HLmodels[omega[iota]].F\n",
    "                    H_i = contaminate_structural_matrix(H_i, contamination_fraction=cont_frac, contamination_type=cont_type)\n",
    "                    \n",
    "                    muV    = V_i @ noise_muL\n",
    "                    sigmaV = V_i @ noise_SigmaL @ V_i.T\n",
    "                    muH    = H_i @ noise_muH\n",
    "                    sigmaH = H_i @ noise_SigmaH @ H_i.T\n",
    "\n",
    "                    dist = np.sqrt(compute_wasserstein(muV, sigmaV, muH, sigmaH))\n",
    "\n",
    "                    total += dist\n",
    "\n",
    "\n",
    "                iter_avg = total / len(Ill)\n",
    "                abstraction_error[name].append(iter_avg)\n",
    "\n",
    "\n",
    "        # Store results for this contamination level\n",
    "        for method in T_results.keys():\n",
    "            mean_e = np.mean(abstraction_error[method])\n",
    "            std_e = np.std(abstraction_error[method])\n",
    "            plot_results[method]['means'].append(mean_e)\n",
    "            plot_results[method]['stds'].append(std_e)\n",
    "\n",
    "    # Compute averages across all contamination levels for each method\n",
    "    method_averages = {}\n",
    "\n",
    "    for method in T_results.keys():\n",
    "        # Get all means across contamination levels\n",
    "        all_means = plot_results[method]['means']\n",
    "        # Compute the mean and std across all contamination levels\n",
    "        overall_mean = np.mean(all_means)\n",
    "        overall_std = np.std(all_means)\n",
    "        method_averages[method] = (overall_mean, overall_std)\n",
    "\n",
    "    # Sort methods by average (worst to best)\n",
    "    sorted_methods = sorted(method_averages.items(), key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"AVERAGE WASSERSTEIN DISTANCE ACROSS ALL CONTAMINATION LEVELS (0.0 to 1.0)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Method':<15} {'Mean ± CI (95%)':<35}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    for method, (mean, std) in sorted_methods:\n",
    "        ci = std\n",
    "        print(f\"{method:<15} {mean:>8.4f} ± {ci:<8.4f}\")\n",
    "\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23a4128",
   "metadata": {},
   "source": [
    "## ω-contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cecbba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_omega_map(original_omega, num_misalignments):\n",
    "    \"\"\"\n",
    "    Randomly corrupt a subset of entries in the ω map to simulate mapping misspecification.\n",
    "    \n",
    "    Args:\n",
    "        original_omega (dict): Original intervention mapping.\n",
    "            For example: {None: None, iota1: H_i1, iota2: H_i1, iota3: H_i2, ...}\n",
    "        num_misalignments (int): Desired number of misaligned mappings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A new ω mapping with up to num_misalignments entries altered.\n",
    "    \"\"\"\n",
    "    # Exclude keys or values that are None if desired.\n",
    "    omega_keys = [k for k in original_omega.keys() if k is not None]\n",
    "    omega_vals = [original_omega[k] for k in omega_keys if original_omega[k] is not None]\n",
    "    \n",
    "    # Start with a copy of the original mapping.\n",
    "    contaminated_omega = original_omega.copy()\n",
    "    \n",
    "    # Bound the number of misalignments by the number of eligible keys.\n",
    "    num_to_corrupt = min(num_misalignments, len(omega_keys))\n",
    "    \n",
    "    # Randomly select keys to corrupt.\n",
    "    to_corrupt = random.sample(omega_keys, k=num_to_corrupt)\n",
    "    \n",
    "    # Create a random permutation of available targets (ensuring change)\n",
    "    # Use the set of targets from eligible keys.\n",
    "    all_targets = list(set(omega_vals))\n",
    "    \n",
    "    for key in to_corrupt:\n",
    "        original_target = original_omega[key]\n",
    "        # Only corrupt if there's an alternative available.\n",
    "        available_targets = [t for t in all_targets if t != original_target]\n",
    "        if available_targets:\n",
    "            new_target = random.choice(available_targets)\n",
    "            contaminated_omega[key] = new_target\n",
    "            \n",
    "    return contaminated_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b381ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contamination levels to test\n",
    "misalignment_levels = range(0, len(Ill))\n",
    "\n",
    "# Store results for plotting\n",
    "omega_plot_results = {method: {'means': [], 'stds': []} for method in T_results.keys()}\n",
    "\n",
    "\n",
    "# Run experiment for each contamination level\n",
    "for num_mis in tqdm(misalignment_levels):\n",
    "   abstraction_error = {name: [] for name in T_results.keys()}\n",
    "  \n",
    "   for _ in range(1):\n",
    "    noise_muL, noise_SigmaL = mu_U_ll_hat, Sigma_U_ll_hat\n",
    "    noise_muH, noise_SigmaH = mu_U_hl_hat, Sigma_U_hl_hat\n",
    "    \n",
    "    noise_muL    = noise_muL.numpy() if torch.is_tensor(noise_muL) else noise_muL\n",
    "    noise_muH    = noise_muH.numpy() if torch.is_tensor(noise_muH) else noise_muH\n",
    "    noise_SigmaL = noise_SigmaL.numpy() if torch.is_tensor(noise_SigmaL) else noise_SigmaL\n",
    "    noise_SigmaH = noise_SigmaH.numpy() if torch.is_tensor(noise_SigmaH) else noise_SigmaH\n",
    "\n",
    "    omega_cont = contaminate_omega_map(omega, num_mis)\n",
    "\n",
    "\n",
    "    for name, res in T_results.items():\n",
    "        T = res['T_matrix']\n",
    "        total = 0\n",
    "        for iota in Ill:\n",
    "            L_i = LLmodels[iota].F\n",
    "            V_i = T @ L_i\n",
    "            H_i = HLmodels[omega_cont[iota]].F\n",
    "            \n",
    "            muV    = V_i @ noise_muL\n",
    "            sigmaV = V_i @ noise_SigmaL @ V_i.T\n",
    "            muH    = H_i @ noise_muH\n",
    "            sigmaH = H_i @ noise_SigmaH @ H_i.T\n",
    "\n",
    "\n",
    "            dist = np.sqrt(compute_wasserstein(muV, sigmaV, muH, sigmaH))\n",
    "            total += dist\n",
    "\n",
    "\n",
    "        iter_avg = total / len(Ill)\n",
    "        abstraction_error[name].append(iter_avg)\n",
    "\n",
    "\n",
    "   # Store results for this contamination level\n",
    "   for method in T_results.keys():\n",
    "       mean_e = np.mean(abstraction_error[method])\n",
    "       std_e = np.std(abstraction_error[method])\n",
    "       omega_plot_results[method]['means'].append(mean_e)\n",
    "       omega_plot_results[method]['stds'].append(std_e)\n",
    "\n",
    "# Compute averages for each method\n",
    "method_averages = []\n",
    "for method in T_results.keys():\n",
    "    # Get all means across misalignment levels\n",
    "    all_means = omega_plot_results[method]['means']\n",
    "    # Compute overall mean and std\n",
    "    overall_mean = np.mean(all_means)\n",
    "    overall_std = np.std(all_means)\n",
    "    method_averages.append((method, overall_mean, overall_std))\n",
    "\n",
    "# Sort methods by mean (worst to best)\n",
    "method_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print sorted averages\n",
    "for method, mean, std in method_averages:\n",
    "    ci = std\n",
    "    print(f\"{method:<15} {mean:>8.4f} ± {ci:<8.4f}\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3477056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
