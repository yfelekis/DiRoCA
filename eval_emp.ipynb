{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local modules\n",
    "import modularised_utils as mut\n",
    "import opt_utils as oput\n",
    "import evaluation_utils as evut\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "import params\n",
    "import random\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c54ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'lucas6x3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd324a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_results_emp = joblib.load(f\"data/{experiment}/diroca_train_results_empirical.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_estimation = True\n",
    "\n",
    "Dll_obs = joblib.load(f\"data/{experiment}/Dll_obs_test.pkl\")\n",
    "Dhl_obs =  joblib.load(f\"data/{experiment}/Dhl_obs_test.pkl\")\n",
    "\n",
    "LLmodels = joblib.load(f\"data/{experiment}/LLmodels.pkl\")\n",
    "HLmodels = joblib.load(f\"data/{experiment}/HLmodels.pkl\")\n",
    "\n",
    "num_llsamples, num_hlsamples  = Dll_obs.shape[0], Dhl_obs.shape[0]\n",
    "\n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "\n",
    "n_varsll, n_varshl = len(Gll.nodes()), len(Ghl.nodes())\n",
    "\n",
    "omega    = mut.load_omega_map(experiment)\n",
    "\n",
    "if coeff_estimation == True:\n",
    "    ll_coeffs = mut.get_coefficients(Dll_obs, Gll)\n",
    "    hl_coeffs = mut.get_coefficients(Dhl_obs, Ghl) \n",
    "else:\n",
    "    ll_coeffs = mut.load_coeffs(experiment, 'LL')\n",
    "    hl_coeffs = mut.load_coeffs(experiment, 'HL')\n",
    "\n",
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll_obs, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl_obs, Ghl, hl_coeffs)\n",
    "\n",
    "data = evut.generate_empirical_data(LLmodels, HLmodels, omega, U_ll_hat, U_hl_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8828c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_observ        = True\n",
    "test_interv        = True \n",
    "metric             = 'fro'\n",
    "num_iter           = 20\n",
    "\n",
    "if test_observ and test_interv:\n",
    "    test_data = data\n",
    "\n",
    "elif test_observ:\n",
    "    test_data = {None: data[None]}\n",
    "\n",
    "elif test_interv:\n",
    "    test_data = {k: v for k, v in data.items() if k is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1a069",
   "metadata": {},
   "source": [
    "## 0-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "if coeff_estimation == True:\n",
    "    results_single = {method: {'errors': [], 'mean': 0, 'ci': 0} for method in T_results_emp.keys()}\n",
    "\n",
    "    for name, method_data in T_results_emp.items():\n",
    "        T = method_data['T_matrix']\n",
    "        errors = []  # Store errors for each intervention\n",
    "        scale_factor = 1/np.sqrt(len(Ill))\n",
    "\n",
    "        for iota in Ill:\n",
    "            L_i = LLmodels[iota].F\n",
    "            H_i = HLmodels[omega[iota]].F\n",
    "\n",
    "            D_l = L_i @ U_ll_hat.T\n",
    "            D_h = H_i @ U_hl_hat.T\n",
    "            \n",
    "            base_norm = D_l#/ np.linalg.norm(D_l, 'fro')\n",
    "            abst_norm = D_h#/ np.linalg.norm(D_h, 'fro')\n",
    "            \n",
    "            tau_base = T @ base_norm\n",
    "            dist = evut.compute_empirical_distance(tau_base, abst_norm, 'fro')\n",
    "            errors.append(dist)  # Store individual errors\n",
    "\n",
    "        # Calculate mean and CI\n",
    "        mean_error = np.mean(errors)\n",
    "        std_error = np.std(errors)\n",
    "        ci = std_error\n",
    "\n",
    "        # Store all statistics\n",
    "        results_single[name] = {\n",
    "            'errors': errors,\n",
    "            'mean': mean_error,\n",
    "            'ci': ci\n",
    "        }\n",
    "\n",
    "    max_mean = max(v['mean'] for v in results_single.values())\n",
    "    scale_factor = 1/max_mean\n",
    "\n",
    "    # Sort by mean error\n",
    "    results_single = dict(sorted(results_single.items(), key=lambda x: x[1]['mean']))\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{'Method':<15} {'Error (mean ± std)':<35}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    for method, stats in results_single.items():\n",
    "        print(f\"{method:<15} {stats['mean']:>8.4f} ± {stats['ci']:<8.4f}\")\n",
    "\n",
    "    # After running the 0-shift test, we can load the coefficients to proceed.\n",
    "    ll_coeffs = mut.load_coeffs(experiment, 'LL')\n",
    "    hl_coeffs = mut.load_coeffs(experiment, 'HL')\n",
    "else:\n",
    "    print('No coeff estimation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eabf55",
   "metadata": {},
   "source": [
    "## ρ-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e009c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_values = np.arange(0.05, 16.05, 1).tolist()  \n",
    "sample_forms = ['sample'] #['boundary', 'sample']\n",
    "\n",
    "hat_dict = {'L': U_ll_hat, 'H': U_hl_hat}\n",
    "\n",
    "worst = 'T_8'\n",
    "U_worst_L = T_results_emp[worst]['optimization_params']['L']['pert_U']\n",
    "U_worst_H = T_results_emp[worst]['optimization_params']['H']['pert_U']\n",
    "\n",
    "\n",
    "target_samplesL = U_ll_hat.shape[0]\n",
    "target_samplesH = U_hl_hat.shape[0]\n",
    "\n",
    "indicesL = np.random.choice(U_worst_L.shape[0], size=target_samplesL, replace=False)\n",
    "indicesH = np.random.choice(U_worst_H.shape[0], size=target_samplesH, replace=False)\n",
    "\n",
    "U_worst_L = U_worst_L[indicesL]\n",
    "U_worst_H = U_worst_H[indicesH]\n",
    "\n",
    "worst_dict = {'L': U_worst_L, 'H': U_worst_H}\n",
    "\n",
    "center = 'worst'\n",
    "if center == 'hat':\n",
    "    center_matrix = hat_dict\n",
    "elif center == 'worst':\n",
    "    center_matrix = worst_dict\n",
    "\n",
    "coverage_type='uniform'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3082660c",
   "metadata": {},
   "source": [
    "### a. Familly of Pertubations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fc24990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate perturbation families\n",
    "pert_family_L = evut.generate_perturbation_family(\n",
    "    np.zeros_like(hat_dict['L']),\n",
    "    k=100,  \n",
    "    r_mu=0.0,\n",
    "    r_sigma=1.0,\n",
    "    coverage=coverage_type\n",
    ")\n",
    "\n",
    "pert_family_H = evut.generate_perturbation_family(\n",
    "    np.zeros_like(hat_dict['H']),\n",
    "    k=100,\n",
    "    r_mu=0.0,\n",
    "    r_sigma=1.0,\n",
    "    coverage=coverage_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39f8058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    sample_form: {\n",
    "        'empirical': {method: [] for method in T_results_emp.keys()}\n",
    "    } for sample_form in sample_forms\n",
    "}\n",
    "\n",
    "for pert_L, pert_H in zip(pert_family_L, pert_family_H):\n",
    "    for sample_form in sample_forms:\n",
    "        for name, method_data in T_results_emp.items():\n",
    "            T = method_data['T_matrix']\n",
    "            \n",
    "            distances = []\n",
    "            \n",
    "            for iota in Ill:\n",
    "                L_i = LLmodels[iota].F\n",
    "                H_i = HLmodels[omega[iota]].F\n",
    "                \n",
    "                pert_noise_L = center_matrix['L'].T + pert_L.T\n",
    "                pert_noise_H = center_matrix['H'].T + pert_H.T\n",
    "                \n",
    "                base_norm = L_i @ pert_noise_L\n",
    "                abst_norm = H_i @ pert_noise_H\n",
    "                \n",
    "                tau_base = T @ base_norm\n",
    "                dist = evut.compute_empirical_distance(tau_base, abst_norm, 'fro')\n",
    "                distances.append(dist)\n",
    "            \n",
    "            results[sample_form]['empirical'][name].extend(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results with ranking\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"{'Rank':<5} {'Method':<15} {'Empirical Distance (mean ± std)':<35}\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for sample_form in sample_forms:\n",
    "    print(f\"\\nSample form: {sample_form}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Calculate means and stds for all methods\n",
    "    method_stats = {}\n",
    "    for method in T_results_emp.keys():\n",
    "        distances = results[sample_form]['empirical'][method]\n",
    "        mean = np.mean(distances)\n",
    "        std = np.std(distances)\n",
    "        method_stats[method] = (mean, std)\n",
    "    \n",
    "    # Sort methods by mean error (worst to best)\n",
    "    sorted_methods = sorted(method_stats.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    \n",
    "    # Print ranked results\n",
    "    for rank, (method, (mean, std)) in enumerate(sorted_methods, 1):\n",
    "        print(f\"{rank:<5} {method:<15} \"\n",
    "              f\"{mean:>8.4f} ± {std/10:<8.4f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Print summary of best and worst methods\n",
    "for sample_form in sample_forms:\n",
    "    print(f\"\\nSummary for {sample_form} sampling:\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Get sorted methods\n",
    "    method_stats = {method: (np.mean(results[sample_form]['empirical'][method]),\n",
    "                           np.std(results[sample_form]['empirical'][method]))\n",
    "                   for method in T_results_emp.keys()}\n",
    "    sorted_methods = sorted(method_stats.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    \n",
    "    # Print worst and best\n",
    "    worst_method, (worst_error, worst_std) = sorted_methods[0]\n",
    "    best_method, (best_error, best_std) = sorted_methods[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b338b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(\n",
    "    T_results_emp,\n",
    "    Ill_relevant,\n",
    "    LLmodels,\n",
    "    HLmodels,\n",
    "    omega,\n",
    "    base_noise_L,\n",
    "    base_noise_H,\n",
    "    shift=False,\n",
    "    r_mu=0.0,\n",
    "    r_sigma=0.0,\n",
    "    num_perturbations=1,\n",
    "    coverage_type='uniform'\n",
    "):\n",
    "\n",
    "    if shift:\n",
    "        pert_family_L = evut.generate_perturbation_family(\n",
    "            np.zeros_like(base_noise_L),\n",
    "            k=num_perturbations,\n",
    "            r_mu=r_mu,\n",
    "            r_sigma=r_sigma,\n",
    "            coverage=coverage_type\n",
    "        )\n",
    "        pert_family_H = evut.generate_perturbation_family(\n",
    "            np.zeros_like(base_noise_H),\n",
    "            k=num_perturbations,\n",
    "            r_mu=r_mu,\n",
    "            r_sigma=r_sigma,\n",
    "            coverage=coverage_type\n",
    "        )\n",
    "    else:\n",
    "        # No perturbations, only one \"fake\" no-perturbation\n",
    "        pert_family_L = [np.zeros_like(base_noise_L)]\n",
    "        pert_family_H = [np.zeros_like(base_noise_H)]\n",
    "        num_perturbations = 1  # force to 1\n",
    "\n",
    "    # Initialize result dictionary\n",
    "    results = {method: [] for method in T_results_emp.keys()}\n",
    "\n",
    "    # Loop over perturbations (only one if shift=False)\n",
    "    for pert_L, pert_H in zip(pert_family_L, pert_family_H):\n",
    "        for method_name, method_data in T_results_emp.items():\n",
    "            T = method_data['T_matrix']\n",
    "            distances = []\n",
    "            for iota in Ill_relevant:\n",
    "                L_i = LLmodels[iota].F\n",
    "                H_i = HLmodels[omega[iota]].F\n",
    "\n",
    "                pert_noise_L = base_noise_L.T + pert_L.T\n",
    "                pert_noise_H = base_noise_H.T + pert_H.T\n",
    "\n",
    "                base_norm = L_i @ pert_noise_L\n",
    "                abst_norm = H_i @ pert_noise_H\n",
    "\n",
    "                tau_base = T @ base_norm\n",
    "                dist = evut.compute_empirical_distance(tau_base, abst_norm, 'fro')\n",
    "                distances.append(dist)\n",
    "\n",
    "            results[method_name].extend(distances)\n",
    "\n",
    "    # Compute mean and CI for each method\n",
    "    stats = {}\n",
    "    for method, vals in results.items():\n",
    "        vals = np.array(vals)\n",
    "        mean = np.mean(vals)\n",
    "        std = np.std(vals)\n",
    "        ci = std\n",
    "        stats[method] = {'mean': mean, 'ci': ci, 'all': vals}\n",
    "\n",
    "    return results, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d6f7c",
   "metadata": {},
   "source": [
    "## F-misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e107e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_structural_matrix(M, contamination_fraction, contamination_type, num_segments=10, seed=None):\n",
    "   \"\"\"\n",
    "   Contaminates a linear transformation matrix M to break its strict linearity.\n",
    "  \n",
    "   Args:\n",
    "       M (np.ndarray): Original linear transformation matrix (n x m).\n",
    "       contamination_fraction (float): Magnitude of contamination (e.g., between 0.05 and 1.0).\n",
    "       contamination_type (str): Type of contamination to apply. Options are:\n",
    "                                 'multiplicative', 'nonlinear', or 'piecewise'.\n",
    "       num_segments (int): Number of segments for piecewise linear contamination (default: 3).\n",
    "       seed (int, optional): Random seed for reproducibility.\n",
    "      \n",
    "   Returns:\n",
    "       np.ndarray: The contaminated matrix.\n",
    "   \"\"\"\n",
    "   rng = np.random.default_rng(seed)\n",
    "   M_cont = M.copy() \n",
    "   n, m = M.shape\n",
    "\n",
    "\n",
    "   if contamination_type == \"multiplicative\":\n",
    "       # Apply element-wise multiplicative noise (preserving zeros below the main diagonal)\n",
    "       # Only perturb the upper-triangular part.\n",
    "       noise = rng.uniform(low=1.0 - contamination_fraction, high=1.0 + contamination_fraction, size=M.shape)\n",
    "       # Create a mask for the upper triangular (including diagonal)\n",
    "       mask = np.triu(np.ones_like(M))\n",
    "       M_cont = M * (1 - mask + mask * noise)\n",
    "  \n",
    "   elif contamination_type == \"nonlinear\":\n",
    "       # Apply a nonlinear function to L: for instance, add a sine-based perturbation.\n",
    "       M_cont = M + contamination_fraction * np.sin(M)\n",
    "  \n",
    "   elif contamination_type == \"piecewise\":\n",
    "       # Contaminate each row with a piecewise linear function.\n",
    "       def piecewise_contaminate_row(row, cont_frac, segments, rng):\n",
    "           n_elem = len(row)\n",
    "           # Choose random breakpoints among indices\n",
    "           if segments < 2:\n",
    "               return row  # nothing to do\n",
    "           breakpoints = np.sort(rng.integers(low=1, high=n_elem, size=segments - 1))\n",
    "           breakpoints = np.concatenate(([0], breakpoints, [n_elem]))\n",
    "           contaminated_row = np.empty_like(row)\n",
    "           # For each segment, assign a random multiplicative factor.\n",
    "           for j in range(len(breakpoints) - 1):\n",
    "               start = breakpoints[j]\n",
    "               end = breakpoints[j+1]\n",
    "               factor = 1.0 + rng.uniform(low=-cont_frac, high=cont_frac)\n",
    "               contaminated_row[start:end] = row[start:end] * factor\n",
    "           return contaminated_row\n",
    "      \n",
    "       # Apply the piecewise contamination row-by-row.\n",
    "       for i in range(n):\n",
    "           M_cont[i, :] = piecewise_contaminate_row(M[i, :], contamination_fraction, num_segments, rng)\n",
    "  \n",
    "   else:\n",
    "       raise ValueError(\"Unknown contamination type. Choose among 'multiplicative', 'nonlinear', or 'piecewise'.\")\n",
    "  \n",
    "   return M_cont\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contamination levels to test\n",
    "contamination_levels = np.linspace(0.0, 1.0, 100)\n",
    "\n",
    "for cont_type in ['piecewise']:\n",
    "    print(f\"\\nContamination type: {cont_type}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Store results for plotting\n",
    "    plot_results = {method: {'means': [], 'stds': []} for method in T_results_emp.keys()}\n",
    "\n",
    "    # Run experiment for each contamination level\n",
    "    for cont_frac in tqdm(contamination_levels):\n",
    "        abstraction_error = {name: [] for name in T_results_emp.keys()}\n",
    "        \n",
    "        for _ in range(1):  # Multiple runs for each contamination level\n",
    "            for name, res in T_results_emp.items():\n",
    "                T = res['T_matrix']\n",
    "                total = 0\n",
    "                \n",
    "                for iota in Ill:\n",
    "                    L_i = LLmodels[iota].F\n",
    "                    L_i = contaminate_structural_matrix(L_i, contamination_fraction=cont_frac, contamination_type=cont_type)\n",
    "                    H_i = HLmodels[omega[iota]].F\n",
    "                    H_i = contaminate_structural_matrix(H_i, contamination_fraction=cont_frac, contamination_type=cont_type)\n",
    "                    \n",
    "                    base_norm = L_i @ (hat_dict['L'].T)\n",
    "                    abst_norm = H_i @ (hat_dict['H'].T)\n",
    "                    \n",
    "                    tau_base = T @ base_norm\n",
    "                    dist = evut.compute_empirical_distance(tau_base, abst_norm, 'fro')\n",
    "                    d = tau_base.shape[0] * tau_base.shape[1]  # number of entries\n",
    "                    dist /= np.sqrt(d)\n",
    "                    # dist *= 100\n",
    "                    total += dist\n",
    "                \n",
    "                # Store average error for this iteration\n",
    "                iter_avg = total / len(Ill)\n",
    "                abstraction_error[name].append(iter_avg)\n",
    "        \n",
    "        # Store results for this contamination level\n",
    "        for method in T_results_emp.keys():\n",
    "            mean_e = np.mean(abstraction_error[method])\n",
    "            std_e = np.std(abstraction_error[method])\n",
    "            plot_results[method]['means'].append(mean_e)\n",
    "            plot_results[method]['stds'].append(std_e)\n",
    "\n",
    "    # Compute and print the overall averages\n",
    "    print(f\"{'Method':<15} {'Mean ± std':<35}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Compute averages for each method\n",
    "    method_averages = []\n",
    "    for method in T_results_emp.keys():\n",
    "        mean = np.mean(plot_results[method]['means'])\n",
    "        std = np.std(plot_results[method]['means'])\n",
    "        method_averages.append((method, mean, std))\n",
    "    \n",
    "    # Sort by mean (worst to best)\n",
    "    method_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Print sorted averages\n",
    "    for method, mean, std in method_averages:\n",
    "        ci = std\n",
    "        print(f\"{method:<15} {mean:>8.4f} ± {ci:<8.4f}\")\n",
    "    \n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac65046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1049bbde",
   "metadata": {},
   "source": [
    "### ω-misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b0979a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_omega_map(original_omega, num_misalignments):\n",
    "    \"\"\"\n",
    "    Randomly corrupt a subset of entries in the ω map to simulate mapping misspecification.\n",
    "    \n",
    "    Args:\n",
    "        original_omega (dict): Original intervention mapping.\n",
    "            For example: {None: None, iota1: H_i1, iota2: H_i1, iota3: H_i2, ...}\n",
    "        num_misalignments (int): Desired number of misaligned mappings.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A new ω mapping with up to num_misalignments entries altered.\n",
    "    \"\"\"\n",
    "    # Exclude keys or values that are None if desired.\n",
    "    omega_keys = [k for k in original_omega.keys() if k is not None]\n",
    "    omega_vals = [original_omega[k] for k in omega_keys if original_omega[k] is not None]\n",
    "    \n",
    "    # Start with a copy of the original mapping.\n",
    "    contaminated_omega = original_omega.copy()\n",
    "    \n",
    "    # Bound the number of misalignments by the number of eligible keys.\n",
    "    num_to_corrupt = min(num_misalignments, len(omega_keys))\n",
    "    \n",
    "    # Randomly select keys to corrupt.\n",
    "    to_corrupt = random.sample(omega_keys, k=num_to_corrupt)\n",
    "    \n",
    "    # Create a random permutation of available targets (ensuring change)\n",
    "    # Use the set of targets from eligible keys.\n",
    "    all_targets = list(set(omega_vals))\n",
    "    \n",
    "    for key in to_corrupt:\n",
    "        original_target = original_omega[key]\n",
    "        # Only corrupt if there's an alternative available.\n",
    "        available_targets = [t for t in all_targets if t != original_target]\n",
    "        if available_targets:\n",
    "            new_target = random.choice(available_targets)\n",
    "            contaminated_omega[key] = new_target\n",
    "            \n",
    "    return contaminated_omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed323862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define misalignment levels to test\n",
    "misalignment_levels = range(0, len(Ill)) \n",
    "omega_plot_results  = {method: {'means': [], 'stds': []} for method in T_results_emp.keys()}\n",
    "\n",
    "for num_mis in tqdm(misalignment_levels):\n",
    "    abstraction_error = {name: [] for name in T_results_emp.keys()}\n",
    "    \n",
    "    for _ in range(1): \n",
    "        # Contaminate the omega map\n",
    "        omega_cont = contaminate_omega_map(omega, num_mis)\n",
    "        \n",
    "        for name, res in T_results_emp.items():\n",
    "            T = res['T_matrix']\n",
    "            \n",
    "            total = 0\n",
    "            for iota in Ill:\n",
    "                L_i = LLmodels[iota].F\n",
    "                H_i = HLmodels[omega_cont[iota]].F\n",
    "                \n",
    "                base_norm = L_i @ (hat_dict['L'].T)\n",
    "                abst_norm = H_i @ (hat_dict['H'].T)\n",
    "                \n",
    "                tau_base = T @ base_norm\n",
    "                dist = evut.compute_empirical_distance(tau_base, abst_norm, 'fro')\n",
    "                total += dist\n",
    "\n",
    "            iter_avg = total / len(Ill)\n",
    "            abstraction_error[name].append(iter_avg)\n",
    "    \n",
    "    for method in T_results_emp.keys():\n",
    "        mean_e = np.mean(abstraction_error[method])\n",
    "        std_e = np.std(abstraction_error[method])\n",
    "        omega_plot_results[method]['means'].append(mean_e)\n",
    "        omega_plot_results[method]['stds'].append(std_e)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"AVERAGE ERROR ACROSS ALL OMEGA MISALIGNMENTS (EMPIRICAL)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Method':<15} {'Mean ± std':<35}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Compute averages for each method\n",
    "method_averages = []\n",
    "for method in T_results_emp.keys():\n",
    "    # Get all means across misalignment levels\n",
    "    all_means = omega_plot_results[method]['means']\n",
    "    # Compute overall mean and std\n",
    "    overall_mean = np.mean(all_means)\n",
    "    overall_std = np.std(all_means)\n",
    "    method_averages.append((method, overall_mean, overall_std))\n",
    "\n",
    "method_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "for method, mean, std in method_averages:\n",
    "    ci = std\n",
    "    print(f\"{method:<15} {mean:>8.4f} ± {ci:<8.4f}\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ba5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
