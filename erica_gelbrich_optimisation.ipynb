{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac2c91",
   "metadata": {},
   "source": [
    "### Barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4631e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-level barycenter Mean: [-0.00678588 -0.01069607 -0.00015191]\n",
      "Low-level barycenter Covariance: [[1.04033442 0.28379336 0.03931234]\n",
      " [0.28379336 2.0108559  0.21143766]\n",
      " [0.03931234 0.21143766 0.99009455]]\n",
      "\n",
      "High-level barycenter Mean: [ 0.0042843  -0.00863504]\n",
      "High-level barycenter Covariance: [[1.35385779 0.58226716]\n",
      " [0.58226716 0.97012678]]\n"
     ]
    }
   ],
   "source": [
    "L_matrices = []  # List of L_i matrices\n",
    "for iota in Ill:\n",
    "    L_matrices.append(LLmodels[iota].compute_mechanism())\n",
    "\n",
    "H_matrices = []  # List of H_i matrices\n",
    "for eta in Ihl:\n",
    "    H_matrices.append(HLmodels[eta].compute_mechanism())\n",
    "\n",
    "mu_bary_L, Sigma_bary_L = oput.compute_gauss_barycenter(L_matrices, mu_U_ll_hat, Sigma_U_ll_hat)\n",
    "mu_bary_H, Sigma_bary_H = oput.compute_gauss_barycenter(H_matrices, mu_U_hl_hat, Sigma_U_hl_hat)\n",
    "\n",
    "print(\"Low-level barycenter Mean:\", mu_bary_L)\n",
    "print(\"Low-level barycenter Covariance:\", Sigma_bary_L)\n",
    "print( )\n",
    "print(\"High-level barycenter Mean:\", mu_bary_H)\n",
    "print(\"High-level barycenter Covariance:\", Sigma_bary_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d98d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "V                 = oput.sample_projection(mu_U_ll_hat.shape[0], mu_U_hl_hat.shape[0], use_stiefel=False)\n",
    "mu_bary_L_proj    = V @ mu_bary_L\n",
    "Sigma_bary_L_proj = V @ Sigma_bary_L @ V.T\n",
    "\n",
    "monge, A = oput.monge_map(mu_bary_L_proj, Sigma_bary_L_proj, mu_bary_H, Sigma_bary_H)\n",
    "T        = V.T @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c6c38736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiguity set construction: Based on epsilon and delta include distribution (as many as the num_envs) that\n",
    "# # pass the \"gelbrich\" test.\n",
    "# ll_moments = mut.sample_moments_U(mu_hat    = mu_U_ll_hat,\n",
    "#                                   Sigma_hat = Sigma_U_ll_hat,\n",
    "#                                   bound     = epsilon,\n",
    "#                                   num_envs  = ll_num_envs)\n",
    "\n",
    "# A_ll       = mut.sample_distros_Gelbrich(ll_moments) #Low-level: A_epsilon\n",
    "\n",
    "\n",
    "# hl_moments = mut.sample_moments_U(mu_hat    = mu_U_hl_hat,\n",
    "#                                   Sigma_hat = Sigma_U_hl_hat,\n",
    "#                                   bound     = delta,\n",
    "#                                   num_envs  = hl_num_envs)\n",
    "\n",
    "# A_hl       = mut.sample_distros_Gelbrich(hl_moments) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd01dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# abstraction_errors             = {}\n",
    "# abstraction_env_errors         = {}\n",
    "# max_env_avg_interv_error_value = -np.inf\n",
    "# max_env_avg_interv_error_key   = None\n",
    "# distance_err                   = 'wass'\n",
    "\n",
    "# for lenv in A_ll:\n",
    "\n",
    "#     Dll_noise      = lenv.sample(num_llsamples)[0]\n",
    "#     ll_environment = mut.get_exogenous_distribution(Dll_noise)\n",
    "\n",
    "#     for henv in A_hl:\n",
    "#         Dhl_noise      = henv.sample(num_hlsamples)[0]\n",
    "#         hl_environment = mut.get_exogenous_distribution(Dhl_noise)\n",
    "\n",
    "#         total_ui_error = 0\n",
    "#         num_distros    = len(Ill)\n",
    "\n",
    "#         n, m  = len(LLmodels[None].endogenous_vars), len(HLmodels[None].endogenous_vars)\n",
    "\n",
    "#         T     = mut.sample_stoch_matrix(n, m)\n",
    "\n",
    "#         for iota in Ill:\n",
    "#             llcm   = LLmodels[iota]\n",
    "#             hlcm   = HLmodels[omega[iota]]\n",
    "#             llmech = llcm.compute_mechanism()\n",
    "#             hlmech = hlcm.compute_mechanism()\n",
    "#             error  = mut.ui_error_dist(distance_err, lenv, henv, llmech, hlmech, T)\n",
    "\n",
    "#             total_ui_error += error\n",
    "\n",
    "#         avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "#         if avg_interv_error > max_env_avg_interv_error_value:\n",
    "#             max_env_avg_interv_error_value = avg_interv_error\n",
    "#             max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "#         abstraction_errors[str(T)] = avg_interv_error\n",
    "#         abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error\n",
    "\n",
    "\n",
    "# max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "# max_error = abstraction_errors[max_tau]\n",
    "\n",
    "# print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "# print('==============================================================================' )\n",
    "# max_lenv = max_env_avg_interv_error_key[0]\n",
    "# max_henv = max_env_avg_interv_error_key[1]\n",
    "\n",
    "# print(f\"max LL mean vector = {max_lenv.means_}\")\n",
    "# print(f\"max LL covariance = {max_lenv.covariances_}\")\n",
    "# print( )\n",
    "\n",
    "# print(f\"max HL mean vector = {max_henv.means_}\")\n",
    "# print(f\"max HL covariance = {max_henv.covariances_}\")\n",
    "# print('==============================================================================' )\n",
    "# print(f\"max environment, average interventional abstraction error = {max_env_avg_interv_error_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0657828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstraction: [[0.21761458 0.78238542]\n",
      " [0.75095088 0.24904912]\n",
      " [0.86664525 0.13335475]], Error: 1.080267499893765\n",
      "==============================================================================\n",
      "max LL mean vector = [[0.03909886 0.02270429 0.149256  ]]\n",
      "max LL covariance = [[[0.82603265 0.         0.        ]\n",
      "  [0.         2.03002425 0.        ]\n",
      "  [0.         0.         0.84745966]]]\n",
      "\n",
      "max HL mean vector = [[ 0.07752375 -0.03284999]]\n",
      "max HL covariance = [[[1.03175598 0.        ]\n",
      "  [0.         0.77764308]]]\n",
      "==============================================================================\n",
      "max environment, average interventional abstraction error = 1.080267499893765\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf53d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a7f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta):\n",
    "    grad_mu_L = torch.zeros_like(mu_L, dtype=torch.float32) \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float() \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float() \n",
    "\n",
    "        grad_mu_L += torch.matmul(V_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float())) \n",
    "    \n",
    "    grad_mu_L = (2 / n) * grad_mu_L - 2 * lambda_L * (mu_L - hat_mu_L)\n",
    "    mu_L = mu_L + (eta * grad_mu_L)\n",
    "    return mu_L\n",
    "\n",
    "def update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta):\n",
    "    grad_mu_H = torch.zeros_like(mu_H, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "        grad_mu_H -= torch.matmul(H_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float()))\n",
    "    \n",
    "    grad_mu_H = (2 / n) * grad_mu_H - 2 * lambda_H * (mu_H - hat_mu_H)\n",
    "    \n",
    "    mu_H = mu_H + (eta * grad_mu_H)\n",
    "    return mu_H\n",
    "\n",
    "\n",
    "def update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta):\n",
    "    grad_Sigma_L = torch.zeros_like(Sigma_L)\n",
    "    \n",
    "    # Term 1: (2/n) * sum_i(V_i^T * V_i)\n",
    "    term1 = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        term1 = term1 + torch.matmul(V_i.T, V_i)\n",
    "\n",
    "    # Term 2: -2 * lambda_L * (Sigma_L^(1/2) - hat_Sigma_L^(1/2)) * Sigma_L^(-1/2)\n",
    "    Sigma_L_sqrt = oput.sqrtm_svd(Sigma_L)  # Compute the square root of Sigma_L\n",
    "    #Sigma_L_sqrt = torch.linalg.matrix_power(Sigma_L, 0.5)\n",
    "\n",
    "    hat_Sigma_L_sqrt = oput.sqrtm_svd(hat_Sigma_L)  # Compute the square root of hat_Sigma_L\n",
    "\n",
    "    term2 = -2 * lambda_L * (Sigma_L_sqrt - hat_Sigma_L_sqrt) @ torch.inverse(Sigma_L_sqrt)\n",
    "\n",
    "    # Combine terms\n",
    "    grad_Sigma_L = (2 / n) * term1 + term2\n",
    "\n",
    "    # Update Sigma_L\n",
    "    Sigma_L_half = Sigma_L + eta * grad_Sigma_L\n",
    "    #Sigma_L_half  = diagonalize(Sigma_L_half)\n",
    "    return Sigma_L_half\n",
    "\n",
    "\n",
    "def update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param):\n",
    "    Sigma_L_final = torch.zeros_like(Sigma_L_half, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "        \n",
    "        Sigma_L_half      = Sigma_L_half.float()\n",
    "        V_Sigma_V         = torch.matmul(V_i, torch.matmul(Sigma_L_half, V_i.T))\n",
    "        sqrtm_V_Sigma_V   = oput.sqrtm_svd(V_Sigma_V)\n",
    "        prox_Sigma_L_half = torch.matmul(oput.prox_operator(sqrtm_V_Sigma_V, lambda_param), oput.prox_operator(sqrtm_V_Sigma_V, lambda_param).T)\n",
    "        ll_term           = torch.matmul(torch.matmul(torch.linalg.pinv(V_i), prox_Sigma_L_half), torch.linalg.pinv(V_i).T)\n",
    "\n",
    "        Sigma_H   = Sigma_H.float()  \n",
    "        H_Sigma_H = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)).float()\n",
    "        hl_term   = torch.norm(oput.sqrtm_svd(H_Sigma_H), p='fro')\n",
    "\n",
    "        Sigma_L_final = Sigma_L_final + (ll_term * hl_term)\n",
    "\n",
    "    Sigma_L_final =  Sigma_L_final * (2 / n)\n",
    "    Sigma_L_final = oput.diagonalize(Sigma_L_final)\n",
    "\n",
    "    return Sigma_L_final\n",
    "\n",
    "\n",
    "def update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta):\n",
    "    grad_Sigma_H = torch.zeros_like(Sigma_H)\n",
    "    term1 = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "        term1 = term1 + torch.matmul(H_i.T, H_i)\n",
    "\n",
    "    Sigma_H_sqrt     = oput.sqrtm_svd(Sigma_H)  \n",
    "    hat_Sigma_H_sqrt = oput.sqrtm_svd(hat_Sigma_H) \n",
    "\n",
    "    term2 = -2 * lambda_H * (Sigma_H_sqrt - hat_Sigma_H_sqrt) @ torch.inverse(Sigma_H_sqrt)\n",
    "\n",
    "    grad_Sigma_H = (2 / n) * term1 + term2\n",
    "\n",
    "    Sigma_H_half = Sigma_H + eta * grad_Sigma_H\n",
    "    return Sigma_H_half\n",
    "\n",
    "\n",
    "def update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param):\n",
    "    Sigma_H_final = torch.zeros_like(Sigma_H_half)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "        H_Sigma_H         = torch.matmul(H_i, torch.matmul(Sigma_H_half, H_i.T))\n",
    "        sqrtm_H_Sigma_H   = oput.sqrtm_svd(H_Sigma_H)\n",
    "        prox_Sigma_H_half = torch.matmul(oput.prox_operator(sqrtm_H_Sigma_H, lambda_param), oput.prox_operator(sqrtm_H_Sigma_H, lambda_param).T)\n",
    "        hl_term           = torch.matmul(torch.matmul(torch.inverse(H_i), prox_Sigma_H_half), torch.inverse(H_i).T)  \n",
    "\n",
    "        \n",
    "        V_Sigma_V = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        ll_term   = torch.norm(oput.sqrtm_svd(V_Sigma_V))\n",
    "\n",
    "        Sigma_H_final = Sigma_H_final + (ll_term * hl_term)\n",
    "    \n",
    "    Sigma_H_final = Sigma_H_final * (2 / n)\n",
    "    Sigma_H_final = oput.diagonalize(Sigma_H_final)\n",
    "    \n",
    "    return Sigma_H_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db35b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max):\n",
    "\n",
    "    for t in range(num_steps_max): \n",
    "\n",
    "        mu_L         = update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "        mu_H         = update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "\n",
    "        Sigma_L_half = update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "        Sigma_L      = update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "                \n",
    "        Sigma_H_half = update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "        Sigma_H      = update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "        \n",
    "        mu_L, Sigma_L, mu_H, Sigma_H = oput.enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "\n",
    "        # Compute the objective function for the current iteration\n",
    "        obj = 0\n",
    "        \n",
    "        for i, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "            V_i = T @ L_i.float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "                        \n",
    "            L_i_mu_L = V_i @ mu_L\n",
    "            H_i_mu_H = H_i @ mu_H\n",
    "            term1 = torch.norm(L_i_mu_L.float() - H_i_mu_H.float())**2\n",
    "            \n",
    "            V_Sigma_V = V_i.float() @ Sigma_L.float() @ V_i.T.float()\n",
    "            H_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "\n",
    "            term2 = torch.trace(V_Sigma_V)\n",
    "            term3 = torch.trace(H_Sigma_H)\n",
    "            \n",
    "            sqrtVSV = oput.sqrtm_svd(V_Sigma_V)\n",
    "            sqrtHSH = oput.sqrtm_svd(H_Sigma_H)\n",
    "\n",
    "            #term4 = -2*torch.trace(oput.sqrtm_svd(sqrtHSH @ V_Sigma_V @ sqrtHSH))\n",
    "            term4 = -2*torch.norm(oput.sqrtm_svd(sqrtVSV) @ oput.sqrtm_svd(sqrtHSH), 'nuc')\n",
    "            \n",
    "            obj = obj + (term1 + term2 + term3 + term4)\n",
    "        \n",
    "        obj = obj/i\n",
    "        \n",
    "        print(f\"Max step {t+1}/{num_steps_max}, Objective: {obj.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "def optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T):\n",
    "\n",
    "    objective_T = 0  # Initialize the objective for this step\n",
    "\n",
    "    for step in range(num_steps_min):\n",
    "        objective_T = 0  # Reset objective at the start of each step\n",
    "        for n, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "            L_i_mu_L = L_i @ mu_L  \n",
    "            H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "            term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "            term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "            term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "            \n",
    "            L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "            H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "\n",
    "            # Using the SVD square root term\n",
    "            term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "            objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "        objective_T = objective_T/n\n",
    "\n",
    "        optimizer_T.zero_grad() # Clear previous gradients\n",
    "        objective_T.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "        optimizer_T.step()      # Update T using the optimizer\n",
    "\n",
    "        print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "\n",
    "    return objective_T, T  # Return both the objective and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a99b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, \n",
    "                     hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                     epsilon, delta, lambda_L, lambda_H, lambda_param, \n",
    "                     eta, max_iter, num_steps_min=2, num_steps_max=5, tol=1e-6):\n",
    "    \n",
    "    T           = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "    optimizer_T = torch.optim.Adam([T], lr=0.01)\n",
    "\n",
    "    previous_objective = float('inf')  # Initialize with a large number\n",
    "\n",
    "    for epoch in range(max_iter):\n",
    "        print('##########################################')\n",
    "        print(f\"Epoch {epoch+1}/{max_iter}\\n\")\n",
    "        print(\"MINIMIZING T\")\n",
    "\n",
    "        # ---- Minimize T ----\n",
    "        objective_T, T               = optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T)\n",
    "\n",
    "        print()\n",
    "        print(\"MAX mu_L, Sigma_L, mu_H, Sigma_H\")\n",
    "        mu_L, Sigma_L, mu_H, Sigma_H = optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max)\n",
    "\n",
    "        # Check for convergence by comparing the difference in objective values\n",
    "        if abs(previous_objective - objective_T.item()) < tol:\n",
    "            print(f\"Convergence reached at epoch {epoch+1} with objective {objective_T.item()}\")\n",
    "            break\n",
    "\n",
    "        # Update previous objective for the next check\n",
    "        previous_objective = objective_T.item()\n",
    "        print('##########################################')\n",
    "\n",
    "    print(\"Final T:\", T)\n",
    "    print(\"Final mu_L:\", mu_L)\n",
    "    print(\"Final Sigma_L:\", Sigma_L)\n",
    "    print(\"Final mu_H:\", mu_H)\n",
    "    print(\"Final Sigma_H:\", Sigma_H)\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3913bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "hat_Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "hat_mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "hat_Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = hat_mu_L.shape[0]\n",
    "h = hat_mu_H.shape[0]\n",
    "\n",
    "\n",
    "# Gelbrich initialization\n",
    "ll_moments      = mut.sample_moments_U(mu_hat = mu_U_ll_hat, Sigma_hat = Sigma_U_ll_hat, bound = epsilon, num_envs = 1)\n",
    "mu_L0, Sigma_L0 = ll_moments[0]\n",
    "#mu_L0, Sigma_L0 = torch.from_numpy(mu_L0), torch.from_numpy(Sigma_L0)\n",
    "\n",
    "hl_moments      = mut.sample_moments_U(mu_hat = mu_U_hl_hat, Sigma_hat = Sigma_U_hl_hat, bound = delta, num_envs = 1)\n",
    "mu_H0, Sigma_H0 = hl_moments[0]\n",
    "#mu_H0, Sigma_H0 = torch.from_numpy(mu_H0), torch.from_numpy(Sigma_H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_L    = torch.from_numpy(mu_L0).float()\n",
    "Sigma_L = torch.from_numpy(Sigma_L0).float()\n",
    "mu_H    = torch.from_numpy(mu_H0).float()\n",
    "Sigma_H = torch.from_numpy(Sigma_H0).float()\n",
    "\n",
    "mu_L, Sigma_L, mu_H, Sigma_H, T = optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, \n",
    "                                                    LLmodels, HLmodels, \n",
    "                                                    hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                                                    epsilon, delta, lambda_L=0.7, lambda_H=0.6, lambda_param=0.9, \n",
    "                                                    eta=0.01, max_iter=10, tol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d40d887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, \n",
    "#                      hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "#                      epsilon, delta, lambda_L, lambda_H, lambda_param, \n",
    "#                      eta, max_iter, num_steps_min = 2, num_steps_max = 5, tol=1e-6):\n",
    "    \n",
    "#     # Initialize T as a tensor with requires_grad=True for automatic differentiation\n",
    "#     T           = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "#     optimizer_T = torch.optim.Adam([T], lr=0.01)\n",
    "\n",
    "#     # objective_values = []\n",
    "#     previous_objective = float('inf')  # Initialize with a large number\n",
    "\n",
    "#     for epoch in range(max_iter):\n",
    "#         print('##########################################')\n",
    "#         print(f\"Epoch {epoch+1}/{max_iter}\\n\")\n",
    "#         print(\"MINIMIZING T\")\n",
    "\n",
    "#         for step in range(num_steps_min):\n",
    "#             objective_T = 0\n",
    "#             for n, iota in enumerate(Ill):\n",
    "#                 L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()\n",
    "#                 H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "                \n",
    "#                 L_i_mu_L = L_i @ mu_L  \n",
    "#                 H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "#                 term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "#                 term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "#                 term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "                \n",
    "#                 L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "#                 H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "                \n",
    "#                 # Using the SVD square root term\n",
    "#                 term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "#                 objective_T = objective_T + (term1 + term2 + term3 + term4)\n",
    "\n",
    "#             objective_T = objective_T/n\n",
    "            \n",
    "#             optimizer_T.zero_grad() # Clear previous gradients\n",
    "#             objective_T.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "#             optimizer_T.step()      # Update T using the optimizer\n",
    "\n",
    "#             # # Store the objective value for plotting\n",
    "#             # objective_values.append(objective_T.item())\n",
    "\n",
    "#             print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "#         print()\n",
    "#         print(\"MAX mu_L, Sigma_L, mu_H, Sigma_H\")\n",
    "#         mu_L, Sigma_L, mu_H, Sigma_H = optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, \n",
    "#                                                     LLmodels, HLmodels, \n",
    "#                                                     hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "#                                                     lambda_L, lambda_H, lambda_param, eta, num_steps_max)\n",
    "\n",
    "#         # Check for convergence by comparing the difference in objective values\n",
    "#         if abs(previous_objective - objective_T.item()) < tol:\n",
    "#             print(f\"Convergence reached at epoch {epoch+1} with objective {objective_T.item()}\")\n",
    "#             break\n",
    "\n",
    "#         # Update previous objective for the next check\n",
    "#         previous_objective = objective_T.item()\n",
    "#         print('##########################################')\n",
    "#     print(\"Final T:\", T)\n",
    "#     print(\"Final mu_L:\", mu_L)\n",
    "#     print(\"Final Sigma_L:\", Sigma_L)\n",
    "#     print(\"Final mu_H:\", mu_H)\n",
    "#     print(\"Final Sigma_H:\", Sigma_H)\n",
    "\n",
    "#     return mu_L, Sigma_L, mu_H, Sigma_H, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a151d15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b387cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "Epoch 1/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 3.5857627391815186\n",
      "Min step 2/2, Objective: 3.5004982948303223\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 2.7105984687805176\n",
      "Max step 2/5, Objective: 2.595881938934326\n",
      "Max step 3/5, Objective: 2.674814462661743\n",
      "Max step 4/5, Objective: 2.8121447563171387\n",
      "Max step 5/5, Objective: 3.017307996749878\n",
      "##########################################\n",
      "##########################################\n",
      "Epoch 2/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 1.7584670782089233\n",
      "Min step 2/2, Objective: 1.7110252380371094\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 3.1379024982452393\n",
      "Max step 2/5, Objective: 3.5451431274414062\n",
      "Max step 3/5, Objective: 4.129366397857666\n",
      "Max step 4/5, Objective: 4.966151237487793\n",
      "Max step 5/5, Objective: 6.072812080383301\n",
      "##########################################\n",
      "##########################################\n",
      "Epoch 3/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 4.884915351867676\n",
      "Min step 2/2, Objective: 4.812129974365234\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 6.290666580200195\n",
      "Max step 2/5, Objective: 6.613593101501465\n",
      "Max step 3/5, Objective: 6.884836673736572\n",
      "Max step 4/5, Objective: 7.010390281677246\n",
      "Max step 5/5, Objective: 7.139386177062988\n",
      "##########################################\n",
      "##########################################\n",
      "Epoch 4/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 6.017140865325928\n",
      "Min step 2/2, Objective: 5.9239373207092285\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 7.005700588226318\n",
      "Max step 2/5, Objective: 7.125044822692871\n",
      "Max step 3/5, Objective: 7.247609615325928\n",
      "Max step 4/5, Objective: 7.373139381408691\n",
      "Max step 5/5, Objective: 7.501593589782715\n",
      "##########################################\n",
      "##########################################\n",
      "Epoch 5/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 6.4449615478515625\n",
      "Min step 2/2, Objective: 6.33629846572876\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 7.335476875305176\n",
      "Max step 2/5, Objective: 7.348396301269531\n",
      "Max step 3/5, Objective: 7.359551906585693\n",
      "Max step 4/5, Objective: 7.3707756996154785\n",
      "Max step 5/5, Objective: 7.381993770599365\n",
      "##########################################\n",
      "##########################################\n",
      "Epoch 6/10\n",
      "\n",
      "MINIMIZING T\n",
      "Min step 1/2, Objective: 6.389901638031006\n",
      "Min step 2/2, Objective: 6.277713775634766\n",
      "\n",
      "MAX mu_L, Sigma_L, mu_H, Sigma_H\n",
      "Max step 1/5, Objective: 7.1054887771606445\n",
      "Max step 2/5, Objective: 7.112439155578613\n",
      "Max step 3/5, Objective: 7.11962890625\n",
      "Max step 4/5, Objective: 7.126864433288574\n",
      "Max step 5/5, Objective: 7.134090423583984\n",
      "Convergence reached at epoch 6 with objective 6.277713775634766\n",
      "Final T: tensor([[ 2.2472, -0.7430,  0.1499],\n",
      "        [ 0.8166, -0.6623, -0.3641]], requires_grad=True)\n",
      "Final mu_L: tensor([-0.5053,  0.4893,  0.2826], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.4787, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4741, 0.0000],\n",
      "        [0.0000, 0.0000, 1.1747]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.3095, 0.2914], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[0.7044, 0.0000],\n",
      "        [0.0000, 0.6701]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "923db2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2472, -0.7430,  0.1499],\n",
       "        [ 0.8166, -0.6623, -0.3641]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "384c8244",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check if the parameters satisfy the constraints and get violation amounts\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m satisfied, violation_1, violation_2 \u001b[38;5;241m=\u001b[39m oput\u001b[38;5;241m.\u001b[39mcheck_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstraints satisfied:\u001b[39m\u001b[38;5;124m\"\u001b[39m, satisfied)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViolation of constraint 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, violation_1)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Check if the parameters satisfy the constraints and get violation amounts\n",
    "satisfied, violation_1, violation_2 = oput.check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "\n",
    "print(\"Constraints satisfied:\", satisfied)\n",
    "print(\"Violation of constraint 1:\", violation_1)\n",
    "print(\"Violation of constraint 2:\", violation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7926494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4628, grad_fn=<SubBackward0>),\n",
       " tensor(-0.1442, grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oput.check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
