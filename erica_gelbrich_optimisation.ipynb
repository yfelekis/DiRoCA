{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1c2d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.28018097, 0.06072652],\n",
       "       [0.        , 1.        , 0.21674035],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLmodels[None].compute_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac2c91",
   "metadata": {},
   "source": [
    "### Barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4631e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-level barycenter Mean: [-0.00678588 -0.01069607 -0.00015191]\n",
      "Low-level barycenter Covariance: [[1.04033442 0.28379336 0.03931234]\n",
      " [0.28379336 2.0108559  0.21143766]\n",
      " [0.03931234 0.21143766 0.99009455]]\n",
      "\n",
      "High-level barycenter Mean: [ 0.0042843  -0.00863504]\n",
      "High-level barycenter Covariance: [[1.35385779 0.58226716]\n",
      " [0.58226716 0.97012678]]\n"
     ]
    }
   ],
   "source": [
    "L_matrices = []  # List of L_i matrices\n",
    "for iota in Ill:\n",
    "    L_matrices.append(LLmodels[iota].compute_mechanism())\n",
    "\n",
    "H_matrices = []  # List of H_i matrices\n",
    "for eta in Ihl:\n",
    "    H_matrices.append(HLmodels[eta].compute_mechanism())\n",
    "\n",
    "mu_bary_L, Sigma_bary_L = oput.compute_gauss_barycenter(L_matrices, mu_U_ll_hat, Sigma_U_ll_hat)\n",
    "mu_bary_H, Sigma_bary_H = oput.compute_gauss_barycenter(H_matrices, mu_U_hl_hat, Sigma_U_hl_hat)\n",
    "\n",
    "print(\"Low-level barycenter Mean:\", mu_bary_L)\n",
    "print(\"Low-level barycenter Covariance:\", Sigma_bary_L)\n",
    "print( )\n",
    "print(\"High-level barycenter Mean:\", mu_bary_H)\n",
    "print(\"High-level barycenter Covariance:\", Sigma_bary_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d98d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "V                 = oput.sample_projection(mu_U_ll_hat.shape[0], mu_U_hl_hat.shape[0], use_stiefel=False)\n",
    "mu_bary_L_proj    = V @ mu_bary_L\n",
    "Sigma_bary_L_proj = V @ Sigma_bary_L @ V.T\n",
    "\n",
    "monge, A = oput.monge_map(mu_bary_L_proj, Sigma_bary_L_proj, mu_bary_H, Sigma_bary_H)\n",
    "T        = V.T @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6c38736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguity set construction: Based on epsilon and delta include distribution (as many as the num_envs) that\n",
    "# pass the \"gelbrich\" test.\n",
    "ll_moments = mut.sample_moments_U(mu_hat    = mu_U_ll_hat,\n",
    "                                  Sigma_hat = Sigma_U_ll_hat,\n",
    "                                  bound     = epsilon,\n",
    "                                  num_envs  = ll_num_envs)\n",
    "\n",
    "A_ll       = mut.sample_distros_Gelbrich(ll_moments) #Low-level: A_epsilon\n",
    "\n",
    "\n",
    "hl_moments = mut.sample_moments_U(mu_hat    = mu_U_hl_hat,\n",
    "                                  Sigma_hat = Sigma_U_hl_hat,\n",
    "                                  bound     = delta,\n",
    "                                  num_envs  = hl_num_envs)\n",
    "\n",
    "A_hl       = mut.sample_distros_Gelbrich(hl_moments) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd01dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "abstraction_errors             = {}\n",
    "abstraction_env_errors         = {}\n",
    "max_env_avg_interv_error_value = -np.inf\n",
    "max_env_avg_interv_error_key   = None\n",
    "distance_err                   = 'wass'\n",
    "\n",
    "for lenv in A_ll:\n",
    "\n",
    "    Dll_noise      = lenv.sample(num_llsamples)[0]\n",
    "    ll_environment = mut.get_exogenous_distribution(Dll_noise)\n",
    "\n",
    "    for henv in A_hl:\n",
    "        Dhl_noise      = henv.sample(num_hlsamples)[0]\n",
    "        hl_environment = mut.get_exogenous_distribution(Dhl_noise)\n",
    "\n",
    "        total_ui_error = 0\n",
    "        num_distros    = len(Ill)\n",
    "\n",
    "        n, m  = len(LLmodels[None].endogenous_vars), len(HLmodels[None].endogenous_vars)\n",
    "\n",
    "        T     = mut.sample_stoch_matrix(n, m)\n",
    "\n",
    "        for iota in Ill:\n",
    "            llcm   = LLmodels[iota]\n",
    "            hlcm   = HLmodels[omega[iota]]\n",
    "            llmech = llcm.compute_mechanism()\n",
    "            hlmech = hlcm.compute_mechanism()\n",
    "            error  = mut.ui_error_dist(distance_err, lenv, henv, llmech, hlmech, T)\n",
    "\n",
    "            total_ui_error += error\n",
    "\n",
    "        avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "        if avg_interv_error > max_env_avg_interv_error_value:\n",
    "            max_env_avg_interv_error_value = avg_interv_error\n",
    "            max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "        abstraction_errors[str(T)] = avg_interv_error\n",
    "        abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0657828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstraction: [[0.21761458 0.78238542]\n",
      " [0.75095088 0.24904912]\n",
      " [0.86664525 0.13335475]], Error: 1.080267499893765\n",
      "==============================================================================\n",
      "max LL mean vector = [[0.03909886 0.02270429 0.149256  ]]\n",
      "max LL covariance = [[[0.82603265 0.         0.        ]\n",
      "  [0.         2.03002425 0.        ]\n",
      "  [0.         0.         0.84745966]]]\n",
      "\n",
      "max HL mean vector = [[ 0.07752375 -0.03284999]]\n",
      "max HL covariance = [[[1.03175598 0.        ]\n",
      "  [0.         0.77764308]]]\n",
      "==============================================================================\n",
      "max environment, average interventional abstraction error = 1.080267499893765\n"
     ]
    }
   ],
   "source": [
    "max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "max_error = abstraction_errors[max_tau]\n",
    "\n",
    "print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "print('==============================================================================' )\n",
    "max_lenv = max_env_avg_interv_error_key[0]\n",
    "max_henv = max_env_avg_interv_error_key[1]\n",
    "\n",
    "print(f\"max LL mean vector = {max_lenv.means_}\")\n",
    "print(f\"max LL covariance = {max_lenv.covariances_}\")\n",
    "print( )\n",
    "\n",
    "print(f\"max HL mean vector = {max_henv.means_}\")\n",
    "print(f\"max HL covariance = {max_henv.covariances_}\")\n",
    "print('==============================================================================' )\n",
    "print(f\"max environment, average interventional abstraction error = {max_env_avg_interv_error_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ec79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf53d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8d921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments     = ['synth1_gnd', 'little_lucas']\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    # Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "    epsilon         = params.radius[experiment][0]\n",
    "    ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "    delta           = params.radius[experiment][1]\n",
    "    hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "    # Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "    num_llsamples   = params.n_samples[experiment][0]\n",
    "    num_hlsamples   = params.n_samples[experiment][1]\n",
    "\n",
    "    Dll = mut.load_samples(experiment)[None][0] \n",
    "    Gll = mut.load_ll_model(experiment)[0]\n",
    "    Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "    Dhl = mut.load_samples(experiment)[None][1] \n",
    "    Ghl = mut.load_hl_model(experiment)[0]\n",
    "    Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "    omega = mut.load_omega_map(experiment)\n",
    "\n",
    "    ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "    hl_coeffs = mut.get_coefficients(Dhl, Ghl) \n",
    "    num_experiments = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f5502a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<operations.Intervention object at 0x19850ce90>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute the objective function as the expectation over samples\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, iota \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(Ill):\n\u001b[0;32m---> 32\u001b[0m     L_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(LLmodels[iota]\u001b[38;5;241m.\u001b[39mcompute_mechanism())\n\u001b[1;32m     33\u001b[0m     H_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(HLmodels[omega[iota]]\u001b[38;5;241m.\u001b[39mcompute_mechanism())\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# 1st term: || T (L_i * mu_L) - (H_i * mu_H) ||_2^2\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: <operations.Intervention object at 0x19850ce90>"
     ]
    }
   ],
   "source": [
    "# Define initial values (replace with actual values from your environment)\n",
    "mu_L    = torch.from_numpy(mu_U_ll_hat)\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat)\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "num_intervs = len(Ill)\n",
    "\n",
    "# Define T as a tensor with requires_grad=True for automatic differentiation\n",
    "T = torch.randn(h, l, requires_grad=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam([T], lr=0.01)\n",
    "#optimizer = torch.optim.SGD([T], lr=0.01)\n",
    "\n",
    "# Number of iterations (epochs)\n",
    "num_epochs = 100  \n",
    "\n",
    "# List to store the objective values at each epoch\n",
    "objective_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialize the objective for this iteration\n",
    "    objective = 0\n",
    "    \n",
    "    # Compute the objective function as the expectation over samples\n",
    "    for i, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "        \n",
    "        # 1st term: || T (L_i * mu_L) - (H_i * mu_H) ||_2^2\n",
    "        L_i_mu_L = L_i @ mu_L  # Result: (m,)\n",
    "        H_i_mu_H = H_i @ mu_H  # Result: (n,)\n",
    "        \n",
    "        term1 = torch.norm(T.float() @ L_i_mu_L.float() - H_i_mu_H.float())**2  # Now this is dimensionally consistent\n",
    "        \n",
    "        # 2nd term: Tr(T L_i Sigma_L L_i^T T^T)\n",
    "        term2 = torch.trace(T.float() @ L_i.float() @ Sigma_L.float() @ L_i.T.float() @ T.T.float())\n",
    "        \n",
    "        # 3rd term: Tr(H_i Sigma_H H_i^T)\n",
    "        term3 = torch.trace(H_i.float() @ Sigma_H.float() @ H_i.T.float())\n",
    "        \n",
    "        # Ensure positive-definiteness for Cholesky decomposition\n",
    "        L_i_Sigma_L = T.float() @ L_i.float() @ Sigma_L.float() @ L_i.T.float() @ T.T.float()\n",
    "        H_i_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "        \n",
    "        # 4th term: -2 * || (T L_i Sigma_L L_i^T T^T)^(1/2) * (H_i Sigma_H H_i^T)^(1/2) ||_*\n",
    "        term4 = -2 * torch.norm(torch.linalg.cholesky(L_i_Sigma_L) @ torch.linalg.cholesky(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "        # Sum up terms\n",
    "        objective += term1 + term2 + term3 + term4\n",
    "\n",
    "    # Average the objective over all interventions\n",
    "    objective /= num_intervs\n",
    "\n",
    "    # Append the objective value at this epoch\n",
    "    objective_values.append(objective.item())\n",
    "\n",
    "    # Compute gradients (subgradients for nuclear norm)\n",
    "    optimizer.zero_grad()  # Zero out previous gradients\n",
    "    objective.backward()  # Backpropagate to compute gradients\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Objective: {objective.item()}\")\n",
    "\n",
    "    # Perform one step of gradient descent\n",
    "    optimizer.step()  # Update T using the optimizer\n",
    "\n",
    "# After the loop, plot the objective values per epoch\n",
    "plt.plot(range(num_epochs), objective_values, label='Objective per Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Objective Value')\n",
    "plt.title('Optimization Progress')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final T:\", T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "940896c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d27b938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the update functions\n",
    "# def update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat):\n",
    "#     N = len(LLmodels)\n",
    "#     E_LL = sum(LL_i.T @ LL_i for LL_i in LLmodels) / N\n",
    "#     E_LH_mu_H = sum(LL_i.T @ HL_i @ mu_H for LL_i, HL_i in zip(LLmodels, HLmodels)) / N\n",
    "#     reg_term = (lambda_eps / 2) * np.eye(mu_U_ll_hat.shape[0])\n",
    "#     mu_L_expr = np.linalg.inv(E_LL + reg_term) @ (E_LH_mu_H + (lambda_eps / 2) * mu_U_ll_hat)\n",
    "#     return mu_L_expr\n",
    "\n",
    "# def update_mu_H(HLmodels, LLmodels, mu_L, lambda_delta, mu_U_hl_hat):\n",
    "#     N = len(HLmodels)\n",
    "#     E_HH = sum(HL_iota.T @ HL_iota for HL_iota in HLmodels) / N\n",
    "#     E_HL_mu_L = sum(HL_iota.T @ LL_iota @ mu_L for LL_iota, HL_iota in zip(LLmodels, HLmodels)) / N\n",
    "#     reg_term = (lambda_delta / 2) * np.eye(mu_U_hl_hat.shape[0])\n",
    "#     mu_H_expr = np.linalg.inv(E_HH + reg_term) @ (E_HL_mu_L + (lambda_delta / 2) * mu_U_hl_hat)\n",
    "#     return mu_H_expr\n",
    "\n",
    "# def update_Sigma_L(LLmodels, lambda_eps, Sigma_U_ll_hat, lambda_Q, Sigma_L_k):\n",
    "#     # Ensure that lambda_eps is a scalar or a single value\n",
    "#     if np.isscalar(lambda_eps) or lambda_eps.size == 1:\n",
    "#         lambda_eps_value = lambda_eps if np.isscalar(lambda_eps) else lambda_eps.item()\n",
    "#         if lambda_eps_value > 0:\n",
    "#             # Check shapes of LLmodels and lambda_Q\n",
    "#             LL_sum = np.sum([LL_i @ LL_i.T for LL_i in LLmodels], axis=0)  # Should result in (4, 4) if each LL_i is (4, 3)\n",
    "#             lambda_Q_sum = np.sum(lambda_Q, axis=0)  # Ensure this matches the expected shape\n",
    "            \n",
    "#             # Make sure lambda_Q_sum is shaped correctly for the operation\n",
    "#             if lambda_Q_sum.shape != LL_sum.shape:\n",
    "#                 raise ValueError(f\"Shape mismatch: LL_sum shape {LL_sum.shape} and lambda_Q_sum shape {lambda_Q_sum.shape}\")\n",
    "\n",
    "#             Sigma_L_updated = np.linalg.inv(LL_sum + lambda_eps_value * np.eye(Sigma_L_k.shape[0])) @ (\n",
    "#                 lambda_eps_value * Sigma_U_ll_hat + lambda_Q_sum\n",
    "#             )\n",
    "#         else:\n",
    "#             Sigma_L_updated = np.zeros_like(Sigma_L_k)  # Handle the case where the condition is not met\n",
    "#     else:\n",
    "#         raise ValueError(\"lambda_eps should be a scalar or a single value\")\n",
    "\n",
    "#     return Sigma_L_updated\n",
    "\n",
    "# def update_Sigma_H(HLmodels, lambda_delta, Sigma_U_hl_hat, lambda_W, Sigma_H_k):\n",
    "#     if np.isscalar(lambda_delta) or lambda_delta.size == 1:\n",
    "#         lambda_delta_value = lambda_delta if np.isscalar(lambda_delta) else lambda_delta.item()\n",
    "#         if lambda_delta_value > 0:\n",
    "#             Sigma_H_updated = np.linalg.inv(np.sum(HLmodels, axis=0) + lambda_delta_value * np.eye(Sigma_H_k.shape[0])) @ (lambda_delta_value * Sigma_U_hl_hat + np.sum(lambda_W, axis=0))\n",
    "#         else:\n",
    "#             Sigma_H_updated = np.zeros_like(Sigma_H_k)\n",
    "#     else:\n",
    "#         raise ValueError(\"lambda_delta should be a scalar or a single value\")\n",
    "#     return Sigma_H_updated\n",
    "\n",
    "# def update_Q_i(Q_i_k, lambda_Q_i, rho_Q, alpha):\n",
    "#     norm_Q = np.linalg.norm(Q_i_k, 'fro')\n",
    "#     if norm_Q > alpha:\n",
    "#         Q_i_updated = (1 - (alpha / norm_Q)) * (Q_i_k - (1 / rho_Q) * lambda_Q_i)\n",
    "#     else:\n",
    "#         Q_i_updated = np.zeros_like(Q_i_k)\n",
    "#     return Q_i_updated\n",
    "\n",
    "# def update_W_i(W_i_k, lambda_W_i, rho_W, alpha):\n",
    "#     norm_W = np.linalg.norm(W_i_k, 'fro')\n",
    "#     if norm_W > alpha:\n",
    "#         W_i_updated = (1 - (alpha / norm_W)) * (W_i_k - (1 / rho_W) * lambda_W_i)\n",
    "#     else:\n",
    "#         W_i_updated = np.zeros_like(W_i_k)\n",
    "#     return W_i_updated\n",
    "\n",
    "# def update_lambda_Q(lambda_Q_i_k, rho_Q, Q_i_k_plus_1, LL_i, Sigma_L_k_plus_1):\n",
    "#     term = np.sqrt(LL_i @ Sigma_L_k_plus_1 @ LL_i.T)\n",
    "#     lambda_Q_i_updated = lambda_Q_i_k + rho_Q * (Q_i_k_plus_1 - term)\n",
    "#     return lambda_Q_i_updated\n",
    "\n",
    "# def update_lambda_W(lambda_W_i_k, rho_W, W_i_k_plus_1, HL_i, Sigma_H_k_plus_1):\n",
    "#     term = np.sqrt(HL_i @ Sigma_H_k_plus_1 @ HL_i.T)\n",
    "#     lambda_W_i_updated = lambda_W_i_k + rho_W * (W_i_k_plus_1 - term)\n",
    "#     return lambda_W_i_updated\n",
    "\n",
    "# def update_lambda_epsilon(lambda_eps_k, mu_L_k_plus_1, mu_U_ll_hat, Sigma_L_k_plus_1, Sigma_U_ll_hat, epsilon):\n",
    "#     lambda_eps_updated = lambda_eps_k + (epsilon**2 - np.linalg.norm(mu_L_k_plus_1 - mu_U_ll_hat)**2 - np.linalg.norm(np.sqrt(Sigma_L_k_plus_1) - np.sqrt(Sigma_U_ll_hat))**2)\n",
    "#     return lambda_eps_updated\n",
    "\n",
    "# def update_lambda_delta(lambda_del_k, mu_H_k_plus_1, mu_U_hl_hat, Sigma_H_k_plus_1, Sigma_U_hl_hat, delta):\n",
    "#     lambda_del_updated = lambda_del_k + (delta**2 - np.linalg.norm(mu_H_k_plus_1 - mu_U_hl_hat)**2 - np.linalg.norm(np.sqrt(Sigma_H_k_plus_1) - np.sqrt(Sigma_U_hl_hat))**2)\n",
    "#     return lambda_del_updated\n",
    "\n",
    "# def compute_monge_map(mu_a, Sigma_a, mu_b, Sigma_b):\n",
    "#     # Check dimensions\n",
    "#     if mu_a.shape[0] != 3 or mu_b.shape[0] != 2:\n",
    "#         raise ValueError(\"Mean vectors must be of shape (3,) for Gaussian A and (2,) for Gaussian B.\")\n",
    "    \n",
    "#     if Sigma_a.shape != (3, 3) or Sigma_b.shape != (2, 2):\n",
    "#         raise ValueError(\"Covariance matrices must be of shape (3, 3) for Gaussian A and (2, 2) for Gaussian B.\")\n",
    "\n",
    "#     # Compute the square root of the covariance matrices\n",
    "#     Sigma_a_sqrt = np.linalg.cholesky(Sigma_a)\n",
    "\n",
    "#     # Use a 2D projection from 3D to 2D, we need a method to match the dimensions\n",
    "#     # Here we assume a simple linear map for projection\n",
    "#     A = Sigma_a_sqrt[:2, :2]  # Take the first two rows/columns for the projection\n",
    "\n",
    "#     # Compute the transformation matrix\n",
    "#     A_inv = np.linalg.inv(A)\n",
    "#     B_inv = np.linalg.inv(Sigma_b)\n",
    "    \n",
    "#     # Calculate the optimal transformation using the covariance matrices\n",
    "#     transformation_matrix = A_inv @ Sigma_b @ A_inv\n",
    "\n",
    "#     # Compute the inverse square root of the transformation matrix\n",
    "#     transformation_matrix_sqrt_inv = np.linalg.inv(np.linalg.cholesky(transformation_matrix))\n",
    "\n",
    "#     def T(x):\n",
    "#         # Ensure the input x is in the expected shape (3,)\n",
    "#         if x.shape[0] != 3:\n",
    "#             raise ValueError(\"Input x must be a 3D vector (shape: (3,)).\")\n",
    "#         # Map the 3D vector to 2D\n",
    "#         return transformation_matrix_sqrt_inv @ (x - mu_a)[:2] + mu_b\n",
    "\n",
    "#     return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddc20a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimization Parameters Initialization\n",
    "# num_iterations = 100000  # Number of iterations for optimization\n",
    "# rho_Q = 1.0  # Penalty parameter for Q updates\n",
    "# rho_W = 1.0  # Penalty parameter for W updates\n",
    "# alpha = 0.5  # Proximal parameter\n",
    "# epsilon = 0.1  # Epsilon constraint value\n",
    "# delta = 0.1  # Delta constraint value\n",
    "\n",
    "# # Example initialization (replace these with actual data)\n",
    "# LLmodels = [np.random.rand(4, 3) for _ in range(5)]  # Example L_i matrices\n",
    "# HLmodels = [np.random.rand(4, 2) for _ in range(5)]  # Example H_i matrices\n",
    "# mu_U_ll_hat = np.random.rand(3)  # Target mean for mu_L\n",
    "# mu_U_hl_hat = np.random.rand(2)  # Target mean for mu_H\n",
    "# Sigma_U_ll_hat = np.eye(3)  # Target covariance for Sigma_L\n",
    "# Sigma_U_hl_hat = np.eye(2)  # Target covariance for Sigma_H\n",
    "\n",
    "# # Initialize variables\n",
    "# mu_L = np.random.rand(3)  # Initial estimate for mu_L\n",
    "# mu_H = np.random.rand(2)  # Initial estimate for mu_H\n",
    "# Sigma_L = np.eye(3)  # Initial estimate for Sigma_L\n",
    "# Sigma_H = np.eye(2)  # Initial estimate for Sigma_H\n",
    "# Q_vars = [np.random.rand(4, 4) for _ in range(5)]  # Initial Q matrices\n",
    "# W_vars = [np.random.rand(4, 4) for _ in range(5)]  # Initial W matrices\n",
    "# lambda_Q = [np.zeros((4, 4)) for _ in range(5)]  # Initial lambda_Q\n",
    "# lambda_W = [np.zeros((4, 4)) for _ in range(5)]  # Initial lambda_W\n",
    "# lambda_eps = 0.0  # Initial lambda_eps\n",
    "# lambda_del = 0.0  # Initial lambda_delta\n",
    "\n",
    "# # Optimization Loop\n",
    "# for k in range(num_iterations):\n",
    "#     # Update mu_L and mu_H\n",
    "#     mu_L = update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat)\n",
    "#     mu_H = update_mu_H(HLmodels, LLmodels, mu_L, lambda_delta, mu_U_hl_hat)\n",
    "\n",
    "#     # Update Sigma_L and Sigma_H\n",
    "#     Sigma_L = Sigma_U_ll_hat #update_Sigma_L(LLmodels, rho_Q, Sigma_U_ll_hat, lambda_Q, Sigma_L)\n",
    "#     Sigma_H = Sigma_U_hl_hat #update_Sigma_H(HLmodels, rho_W, Sigma_U_hl_hat, lambda_W, Sigma_H)\n",
    "\n",
    "#     # Update Q and W\n",
    "#     for iota in range(len(Q_vars)):\n",
    "#         Q_vars[iota] = update_Q_i(Q_vars[iota], lambda_Q[iota], rho_Q, alpha)\n",
    "#         W_vars[iota] = update_W_i(W_vars[iota], lambda_W[iota], rho_W, alpha)\n",
    "\n",
    "#     # Update lambda multipliers\n",
    "#     for iota in range(len(Q_vars)):\n",
    "#         lambda_Q[iota] = update_lambda_Q(lambda_Q[iota], rho_Q, Q_vars[iota], LLmodels[iota], Sigma_L)\n",
    "#         lambda_W[iota] = update_lambda_W(lambda_W[iota], rho_W, W_vars[iota], HLmodels[iota], Sigma_H)\n",
    "\n",
    "#     # Update lambda_eps and lambda_delta\n",
    "#     lambda_eps = update_lambda_epsilon(lambda_eps, mu_L, mu_U_ll_hat, Sigma_L, Sigma_U_ll_hat, epsilon)\n",
    "#     lambda_del = update_lambda_delta(lambda_del, mu_H, mu_U_hl_hat, Sigma_H, Sigma_U_hl_hat, delta)\n",
    "\n",
    "# # Print the final results\n",
    "# print(\"Final mu_L:\", mu_L)\n",
    "# print(\"Final Sigma_L:\\n\", Sigma_L)\n",
    "# print(\"Final mu_H:\", mu_H)\n",
    "# print(\"Final Sigma_H:\\n\", Sigma_H)\n",
    "# print( )\n",
    "# tau = compute_monge_map(mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "# x_sample = np.array([1.5, 2.5, 3.5])\n",
    "# mapped_point = tau(x_sample)\n",
    "\n",
    "# print(\"Mapped point:\", mapped_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b42f0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Sigma_H(Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param):\n",
    "    Sigma_H_final = torch.zeros_like(Sigma_H_half, dtype=torch.complex64)  # Ensure dtype is complex64\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  # Convert to float32\n",
    "        V_i = T @ L_i  # Matrix multiplication, ensure V_i is float32\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  # Convert to float32\n",
    "\n",
    "        # Ensure Sigma_H_half is float32 for matrix multiplication\n",
    "        Sigma_H_half = Sigma_H_half.float()\n",
    "\n",
    "        # Compute term_sqrt with cholesky\n",
    "        term_sqrt = torch.cholesky(torch.matmul(H_i, torch.matmul(Sigma_H_half, H_i.T)))\n",
    "\n",
    "        # Proximal operator\n",
    "        prox_Sigma_H_half = torch.matmul(prox_operator(term_sqrt, lambda_param), prox_operator(term_sqrt, lambda_param).T)\n",
    "\n",
    "        # Matrix multiplication and inversion\n",
    "        hl_term = torch.matmul(torch.matmul(torch.inverse(H_i), prox_Sigma_H_half), torch.inverse(H_i).T)\n",
    "\n",
    "        # Convert Sigma_L to complex64 for compatibility with complex hl_term\n",
    "        Sigma_L_complex = Sigma_L.to(torch.complex64)  # Convert Sigma_L to complex64\n",
    "\n",
    "        # Convert V_i and V_i.T to complex64 to match Sigma_L_complex\n",
    "        V_i_complex = V_i.to(torch.complex64)  # Convert V_i to complex64\n",
    "        V_i_T_complex = V_i.T.to(torch.complex64)  # Convert V_i.T to complex64\n",
    "\n",
    "        # Compute ll_term with the complex tensors\n",
    "        ll_term = torch.norm(torch.cholesky(torch.matmul(V_i_complex, torch.matmul(Sigma_L_complex, V_i_T_complex))), p='fro')\n",
    "\n",
    "        # Ensure hl_term is complex to match ll_term\n",
    "        hl_term = hl_term.to(torch.complex64)  # Convert hl_term to complex64\n",
    "\n",
    "        # Update Sigma_H_final by scaling ll_term with hl_term (instead of matmul)\n",
    "        Sigma_H_final += ll_term * hl_term  # Scale ll_term by the scalar hl_term\n",
    "\n",
    "    # Final update\n",
    "    Sigma_H_final *= - (2 / n)\n",
    "    Sigma_H_final = diagonalize(Sigma_H_final)\n",
    "\n",
    "    return Sigma_H_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7afa9090",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H \u001b[38;5;241m=\u001b[39m optimize(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels,\n\u001b[1;32m      2\u001b[0m                                         HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n\u001b[1;32m      3\u001b[0m                                         lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[150], line 14\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, max_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Update Sigma_H (t+1/2, t)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m Sigma_H_half \u001b[38;5;241m=\u001b[39m update_Sigma_H_half(Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n\u001b[0;32m---> 14\u001b[0m Sigma_H      \u001b[38;5;241m=\u001b[39m update_Sigma_H(Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - mu_L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmu_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mu_H: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmu_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sigma_L: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSigma_L\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Sigma_H: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSigma_H\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[159], line 28\u001b[0m, in \u001b[0;36mupdate_Sigma_H\u001b[0;34m(Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\u001b[0m\n\u001b[1;32m     25\u001b[0m V_i_T_complex \u001b[38;5;241m=\u001b[39m V_i\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcomplex64)  \u001b[38;5;66;03m# Convert V_i.T to complex64\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute ll_term with the complex tensors\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m ll_term \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(torch\u001b[38;5;241m.\u001b[39mcholesky(torch\u001b[38;5;241m.\u001b[39mmatmul(V_i_complex, torch\u001b[38;5;241m.\u001b[39mmatmul(Sigma_L_complex, V_i_T_complex))), p\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Ensure hl_term is complex to match ll_term\u001b[39;00m\n\u001b[1;32m     31\u001b[0m hl_term \u001b[38;5;241m=\u001b[39m hl_term\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mcomplex64)  \u001b[38;5;66;03m# Convert hl_term to complex64\u001b[39;00m\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "mu_L, Sigma_L, mu_H, Sigma_H = optimize(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels,\n",
    "                                        HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n",
    "                                        lambda_L=0.1, lambda_H=0.1, lambda_param=0.1, eta=0.01, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "501f6535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0a7f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_L(mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta):\n",
    "    grad_mu_L = torch.zeros_like(mu_L, dtype=torch.float32) \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float() \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float() \n",
    "\n",
    "        grad_mu_L += torch.matmul(V_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float())) \n",
    "    \n",
    "    grad_mu_L = (2 / n) * grad_mu_L - 2 * lambda_L * (mu_L - hat_mu_L)\n",
    "    mu_L += eta * grad_mu_L\n",
    "    return mu_L\n",
    "\n",
    "def update_mu_H(mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta):\n",
    "    grad_mu_H = torch.zeros_like(mu_H, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "        grad_mu_H -= torch.matmul(H_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float()))\n",
    "    \n",
    "    grad_mu_H = (2 / n) * grad_mu_H - 2 * lambda_H * (mu_H - hat_mu_H)\n",
    "    \n",
    "    mu_H += eta * grad_mu_H\n",
    "    return mu_H\n",
    "\n",
    "def update_Sigma_L_half(Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta):\n",
    "    grad_Sigma_L = torch.zeros_like(Sigma_L)\n",
    "    \n",
    "    # Term 1: (2/n) * sum_i(V_i^T * V_i)\n",
    "    term1 = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        term1 += torch.matmul(V_i.T, V_i)\n",
    "\n",
    "    # Term 2: -2 * lambda_L * (Sigma_L^(1/2) - hat_Sigma_L^(1/2)) * Sigma_L^(-1/2)\n",
    "    Sigma_L_sqrt = torch.cholesky(Sigma_L)  # Compute the square root of Sigma_L\n",
    "    #Sigma_L_sqrt = torch.linalg.matrix_power(Sigma_L, 0.5)\n",
    "\n",
    "    hat_Sigma_L_sqrt = torch.cholesky(hat_Sigma_L)  # Compute the square root of hat_Sigma_L\n",
    "\n",
    "    term2 = -2 * lambda_L * (Sigma_L_sqrt - hat_Sigma_L_sqrt) @ torch.inverse(Sigma_L_sqrt)\n",
    "\n",
    "    # Combine terms\n",
    "    grad_Sigma_L = (2 / n) * term1 + term2\n",
    "\n",
    "    # Update Sigma_L\n",
    "    Sigma_L_half = Sigma_L + eta * grad_Sigma_L\n",
    "    return Sigma_L_half\n",
    "\n",
    "# Proximal operator of a matrix frobenious norm\n",
    "def prox_operator(A, lambda_param):\n",
    "    frobenius_norm = torch.norm(A, p='fro')\n",
    "    scaling_factor = torch.max(1 - lambda_param / frobenius_norm, torch.zeros_like(frobenius_norm))\n",
    "    return scaling_factor * A\n",
    "\n",
    "def diagonalize(A):\n",
    "    # Get eigenvalues and eigenvectors using the new method\n",
    "    eigvals, eigvecs = torch.linalg.eig(A)  # This will return complex values\n",
    "    \n",
    "    # If you need only real eigenvalues, we take the real part\n",
    "    eigvals_real = eigvals.real  # We only need the real part if A is symmetric\n",
    "    \n",
    "    # Convert the diagonal matrix to complex to match the dtype of eigvecs\n",
    "    D = torch.diag(eigvals_real).to(torch.complex64)  # Ensure D is of complex type\n",
    "    \n",
    "    # Return the diagonalized matrix\n",
    "    return torch.matmul(torch.matmul(eigvecs, D), eigvecs.T.conj())\n",
    "\n",
    "\n",
    "def update_Sigma_L(Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param):\n",
    "    Sigma_L_final = torch.zeros_like(Sigma_L_half, dtype=torch.float32)  # Ensure dtype is float32\n",
    "    for n, iota in enumerate(Ill):\n",
    "        # Convert L_i and H_i to float32\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  # Convert to float32\n",
    "        V_i = T @ L_i  # Matrix multiplication, ensure V_i is float32\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  # Convert to float32\n",
    "\n",
    "        # Make sure Sigma_L_half is float32 for matrix multiplication\n",
    "        Sigma_L_half = Sigma_L_half.float()\n",
    "\n",
    "        # Perform matrix multiplication with consistent dtype (float32)\n",
    "        term_sqrt = torch.cholesky(torch.matmul(V_i, torch.matmul(Sigma_L_half, V_i.T)))  # Ensure all are float32\n",
    "\n",
    "        # Proximal operator\n",
    "        prox_Sigma_L_half = torch.matmul(prox_operator(term_sqrt, lambda_param), prox_operator(term_sqrt, lambda_param).T)\n",
    "\n",
    "        # Pseudo-inverse of V_i and other matrix multiplications\n",
    "        ll_term = torch.matmul(torch.matmul(torch.linalg.pinv(V_i), prox_Sigma_L_half), torch.linalg.pinv(V_i).T)\n",
    "\n",
    "        # Ensure Sigma_H is float32 before matrix multiplication and Cholesky\n",
    "        Sigma_H = Sigma_H.float()  # Explicitly cast Sigma_H to float32\n",
    "\n",
    "        # Compute H_i * Sigma_H * H_i^T and take its Cholesky\n",
    "        hl_term = torch.norm(torch.cholesky(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T).float())), p='fro')\n",
    "\n",
    "        # Update Sigma_L_final\n",
    "        Sigma_L_final += ll_term * hl_term\n",
    "\n",
    "    # Final update\n",
    "    Sigma_L_final *= - (2 / n)\n",
    "    Sigma_L_final = diagonalize(Sigma_L_final)\n",
    "\n",
    "    return Sigma_L_final\n",
    "\n",
    "\n",
    "def update_Sigma_H_half(Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta):\n",
    "    grad_Sigma_H = torch.zeros_like(Sigma_H)\n",
    "    \n",
    "    # Term 1: (2/n) * sum_i(H_i^T * H_i)\n",
    "    term1 = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "\n",
    "        term1 += torch.matmul(H_i.T, H_i)\n",
    "\n",
    "    # Term 2: -2 * lambda_H * (Sigma_H^(1/2) - hat_Sigma_H^(1/2)) * Sigma_H^(-1/2)\n",
    "    Sigma_H_sqrt     = torch.cholesky(Sigma_H)  # Compute the square root of Sigma_H\n",
    "    hat_Sigma_H_sqrt = torch.cholesky(hat_Sigma_H)  # Compute the square root of hat_Sigma_H\n",
    "\n",
    "    term2 = -2 * lambda_H * (Sigma_H_sqrt - hat_Sigma_H_sqrt) @ torch.inverse(Sigma_H_sqrt)\n",
    "\n",
    "    # Combine terms\n",
    "    grad_Sigma_H = (2 / n) * term1 + term2\n",
    "\n",
    "    # Update Sigma_H\n",
    "    Sigma_H_half = Sigma_H + eta * grad_Sigma_H\n",
    "    return Sigma_H_half\n",
    "\n",
    "\n",
    "def update_Sigma_H(Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param):\n",
    "    Sigma_H_final = torch.zeros_like(Sigma_H_half)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "\n",
    "        term_sqrt         = torch.cholesky(torch.matmul(H_i, torch.matmul(Sigma_H_half, H_i.T))) \n",
    "        prox_Sigma_H_half = torch.matmul(prox_operator(term_sqrt, lambda_param), prox_operator(term_sqrt, lambda_param).T) \n",
    "        hl_term           = torch.matmul(torch.matmul(torch.inverse(H_i), prox_Sigma_H_half), torch.inverse(H_i).T)  \n",
    "\n",
    "        #Compute H_i * Sigma_H * H_i^T and take its Cholesky\n",
    "        ll_term = torch.norm(torch.cholesky(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))), p='fro')\n",
    "\n",
    "        #Update Sigma_L_final\n",
    "        Sigma_H_final += torch.matmul(ll_term, hl_term)\n",
    "    \n",
    "    Sigma_H_final *= - (2 / n)\n",
    "    Sigma_H_final  = diagonalize(Sigma_H_final)\n",
    "    \n",
    "    return Sigma_H_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3112bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6798ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, max_iter):\n",
    "    for t in range(max_iter):\n",
    "        # Update mu_L and mu_H\n",
    "        mu_L = update_mu_L(mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "        mu_H = update_mu_H(mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "        \n",
    "        # Update Sigma_L (t+1/2, t)\n",
    "        Sigma_L_half = update_Sigma_L_half(Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "        Sigma_L      = update_Sigma_L(Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "\n",
    "                \n",
    "        # Update Sigma_H (t+1/2, t)\n",
    "        Sigma_H_half = update_Sigma_H_half(Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "        Sigma_H      = update_Sigma_H(Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "        \n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t} - mu_L: {mu_L}, mu_H: {mu_H}, Sigma_L: {Sigma_L}, Sigma_H: {Sigma_H}\")\n",
    "    \n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "227ac88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "n = 10  # Number of data points\n",
    "l = 5  # Dimension of mu_L and Sigma_L\n",
    "h = 5  # Dimension of mu_H and Sigma_H\n",
    "\n",
    "mu_L    = torch.from_numpy(mu_U_ll_hat)\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat)\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "# Given estimates (mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "hat_mu_L = torch.randn(l)  # Estimate of mu_L\n",
    "hat_Sigma_L = torch.eye(l)  # Estimate of Sigma_L (identity for simplicity)\n",
    "\n",
    "hat_mu_H = torch.randn(h)  # Estimate of mu_H\n",
    "hat_Sigma_H = torch.eye(h)  # Estimate of Sigma_H (identity for simplicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2b52f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.from_numpy(np.random.randn(2, 3)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7016dba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H \u001b[38;5;241m=\u001b[39m optimize(mu_L\u001b[38;5;241m.\u001b[39mfloat(), Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), mu_H\u001b[38;5;241m.\u001b[39mfloat(), Sigma_H\u001b[38;5;241m.\u001b[39mfloat(), LLmodels,\n\u001b[1;32m      2\u001b[0m                                         HLmodels, hat_mu_L\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_L\u001b[38;5;241m.\u001b[39mfloat(), hat_mu_H\u001b[38;5;241m.\u001b[39mfloat(), hat_Sigma_H\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[1;32m      3\u001b[0m                                         lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lambda_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[119], line 4\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, max_iter)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, max_iter):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Update mu_L and mu_H\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m         mu_L \u001b[38;5;241m=\u001b[39m update_mu_L(mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n\u001b[1;32m      5\u001b[0m         mu_H \u001b[38;5;241m=\u001b[39m update_mu_H(mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Update Sigma_L (t+1/2, t)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[118], line 7\u001b[0m, in \u001b[0;36mupdate_mu_L\u001b[0;34m(mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\u001b[0m\n\u001b[1;32m      5\u001b[0m     V_i \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m@\u001b[39m L_i\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      6\u001b[0m     H_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(HLmodels[omega[iota]]\u001b[38;5;241m.\u001b[39mcompute_mechanism())\n\u001b[0;32m----> 7\u001b[0m     grad_mu_L \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(V_i\u001b[38;5;241m.\u001b[39mT, (torch\u001b[38;5;241m.\u001b[39mmatmul(V_i, mu_L) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(H_i, mu_H))) \n\u001b[1;32m      9\u001b[0m grad_mu_L \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m n) \u001b[38;5;241m*\u001b[39m grad_mu_L \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m lambda_L \u001b[38;5;241m*\u001b[39m (mu_L \u001b[38;5;241m-\u001b[39m hat_mu_L)\n\u001b[1;32m     10\u001b[0m mu_L \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m eta \u001b[38;5;241m*\u001b[39m grad_mu_L\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "mu_L, Sigma_L, mu_H, Sigma_H = optimize(mu_L.float(), Sigma_L.float(), mu_H.float(), Sigma_H.float(), LLmodels,\n",
    "                                        HLmodels, hat_mu_L.float(), hat_Sigma_L.float(), hat_mu_H.float(), hat_Sigma_H.float(),\n",
    "                                        lambda_L=0.1, lambda_H=0.1, lambda_param=0.1, eta=0.01, max_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c75211ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.2802, 0.0607],\n",
       "        [0.0000, 1.0000, 0.2167],\n",
       "        [0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_i.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc330ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cbc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68df92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dafddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac46e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201dc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a33125d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c84d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the optimize function with autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "def optimize(LLmodels, HLmodels, n, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    # Enable autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "    mu_L.requires_grad_(True)\n",
    "    mu_H.requires_grad_(True)\n",
    "\n",
    "    Sigma_L_half = Sigma_L.clone().requires_grad_(True)  # Enable autograd for Sigma_L_half\n",
    "    Sigma_H_half = Sigma_H.clone().requires_grad_(True)  # Enable autograd for Sigma_H_half\n",
    "\n",
    "    # Sigma_L and Sigma_H do not need autograd for the final update\n",
    "    Sigma_L.requires_grad_(False)\n",
    "    Sigma_H.requires_grad_(False)\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # Forward pass: compute the objective function\n",
    "        F_value = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta)\n",
    "        objective = F_value   # Total objective function\n",
    "\n",
    "        # Backward pass: compute gradients for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "        objective.backward()\n",
    "\n",
    "        # Update mu_L and mu_H using gradient ascent with autograd\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Gradient ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Gradient ascent for mu_H\n",
    "\n",
    "            # Zero the gradients after the update for mu_L and mu_H\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "\n",
    "        # Update Sigma_L_half and Sigma_H_half using autograd\n",
    "        with torch.no_grad():\n",
    "            # Update Sigma_L_half and Sigma_H_half using the gradient of F\n",
    "            Sigma_L_half += eta * Sigma_L_half.grad  # Update Sigma_L_half using gradients of F\n",
    "            Sigma_H_half += eta * Sigma_H_half.grad  # Update Sigma_H_half using gradients of F\n",
    "\n",
    "            # Zero the gradients after the update for Sigma_L_half and Sigma_H_half\n",
    "            Sigma_L_half.grad.zero_()\n",
    "            Sigma_H_half.grad.zero_()\n",
    "\n",
    "        # Sigma_L and Sigma_H Updates (no autograd for final update, using proximal operator)\n",
    "        with torch.no_grad():\n",
    "            # Final update for Sigma_L and Sigma_H\n",
    "            Sigma_L_final = update_Sigma_L_final(Sigma_L_half, V, H, Sigma_H, lambda_L, lambda_param, n)\n",
    "            Sigma_H_final = update_Sigma_H_final(Sigma_H_half, V, Sigma_L, H, lambda_H, lambda_param, n)\n",
    "            \n",
    "            # Apply the proximal operator and projections\n",
    "            Sigma_L = update_Sigma_L_with_projection(Sigma_L_final)\n",
    "            Sigma_H = update_Sigma_H_with_projection(Sigma_H_final)\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a548785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Objective function F(mu_L, Sigma_L, mu_H, Sigma_H) calculation\n",
    "def F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta):\n",
    "    F_value = 0.0\n",
    "    for i in range(n):\n",
    "        # Compute terms inside the summation\n",
    "        term1 = torch.norm(torch.matmul(LLmodels[i], mu_L) - torch.matmul(HLmodels[i], mu_H)) ** 2\n",
    "        term2 = torch.trace(torch.matmul(LLmodels[i], torch.matmul(Sigma_L, LLmodels[i].T)))\n",
    "        term3 = torch.trace(torch.matmul(HLmodels[i], torch.matmul(Sigma_H, HLmodels[i].T)))\n",
    "        \n",
    "        # Add terms to the objective\n",
    "        F_value += term1 + term2 + term3\n",
    "\n",
    "    # Add the regularization terms\n",
    "    reg_L = lambda_L * (epsilon**2 - torch.norm(mu_L - hat_mu_L)**2 - torch.norm(torch.cholesky(Sigma_L) - torch.cholesky(hat_Sigma_L))**2)\n",
    "    reg_H = lambda_H * (delta**2 - torch.norm(mu_H - hat_mu_H)**2 - torch.norm(torch.cholesky(Sigma_H) - torch.cholesky(hat_Sigma_H))**2)\n",
    "    \n",
    "    F_value += reg_L + reg_H\n",
    "    return F_value / n\n",
    "\n",
    "# Objective function G(Sigma_L, Sigma_H) calculation\n",
    "def G_func(Sigma_L, Sigma_H, LLmodels, HLmodels, n):\n",
    "    G_value = 0.0\n",
    "    for i in range(n):\n",
    "        # Compute the Frobenius norm of the square root of the matrices\n",
    "        term1 = torch.norm(torch.cholesky(torch.matmul(LLmodels[i], torch.matmul(Sigma_L, LLmodels[i].T))), p='fro')\n",
    "        term2 = torch.norm(torch.cholesky(torch.matmul(HLmodels[i], torch.matmul(Sigma_H, HLmodels[i].T))), p='fro')\n",
    "        \n",
    "        # Add terms to the objective\n",
    "        G_value += term1 * term2\n",
    "\n",
    "    return -2 * G_value / n\n",
    "\n",
    "# Define the optimize function with autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "def optimize(LLmodels, HLmodels, n, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    # Enable autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "    mu_L.requires_grad_(True)\n",
    "    mu_H.requires_grad_(True)\n",
    "\n",
    "    Sigma_L_half = Sigma_L.clone().requires_grad_(True)  # Enable autograd for Sigma_L_half\n",
    "    Sigma_H_half = Sigma_H.clone().requires_grad_(True)  # Enable autograd for Sigma_H_half\n",
    "\n",
    "    # Sigma_L and Sigma_H do not need autograd for the final update\n",
    "    Sigma_L.requires_grad_(False)\n",
    "    Sigma_H.requires_grad_(False)\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # Compute the total objective (F + G)\n",
    "        F_value = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta)\n",
    "        G_value = G_func(Sigma_L, Sigma_H, LLmodels, HLmodels, n)\n",
    "        objective = F_value + G_value  # Total objective function\n",
    "\n",
    "        # Backward pass: compute gradients for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "        objective.backward()\n",
    "\n",
    "        # Update mu_L and mu_H using gradient ascent with autograd\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Gradient ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Gradient ascent for mu_H\n",
    "\n",
    "            # Zero the gradients after the update for mu_L and mu_H\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "\n",
    "        # Update Sigma_L_half and Sigma_H_half using autograd\n",
    "        with torch.no_grad():\n",
    "            # Update Sigma_L_half and Sigma_H_half using the gradient of F\n",
    "            Sigma_L_half += eta * Sigma_L_half.grad  # Update Sigma_L_half using gradients of F\n",
    "            Sigma_H_half += eta * Sigma_H_half.grad  # Update Sigma_H_half using gradients of F\n",
    "\n",
    "            # Zero the gradients after the update for Sigma_L_half and Sigma_H_half\n",
    "            Sigma_L_half.grad.zero_()\n",
    "            Sigma_H_half.grad.zero_()\n",
    "\n",
    "        # Sigma_L and Sigma_H Updates (no autograd for final update, using proximal operator)\n",
    "        with torch.no_grad():\n",
    "            # Final update for Sigma_L and Sigma_H\n",
    "            Sigma_L_final = update_Sigma_L_final(Sigma_L_half, V, H, Sigma_H, lambda_L, lambda_param, n)\n",
    "            Sigma_H_final = update_Sigma_H_final(Sigma_H_half, V, Sigma_L, H, lambda_H, lambda_param, n)\n",
    "            \n",
    "            # Apply the proximal operator and projections\n",
    "            Sigma_L = update_Sigma_L_with_projection(Sigma_L_final)\n",
    "            Sigma_H = update_Sigma_H_with_projection(Sigma_H_final)\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimize function with autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "def optimize(LLmodels, HLmodels, n, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    # Enable autograd for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "    mu_L.requires_grad_(True)\n",
    "    mu_H.requires_grad_(True)\n",
    "\n",
    "    Sigma_L_half = Sigma_L.clone().requires_grad_(True)  # Enable autograd for Sigma_L_half\n",
    "    Sigma_H_half = Sigma_H.clone().requires_grad_(True)  # Enable autograd for Sigma_H_half\n",
    "\n",
    "    # Sigma_L and Sigma_H do not need autograd for the final update\n",
    "    Sigma_L.requires_grad_(False)\n",
    "    Sigma_H.requires_grad_(False)\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # Compute the total objective (F + G)\n",
    "        F_value = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta)\n",
    "        G_value = G_func(Sigma_L, Sigma_H, LLmodels, HLmodels, n)\n",
    "        objective = F_value + G_value  # Total objective function\n",
    "\n",
    "        # Backward pass: compute gradients for mu_L, mu_H, Sigma_L_half, Sigma_H_half\n",
    "        objective.backward()\n",
    "\n",
    "        # Update mu_L and mu_H using gradient ascent with autograd\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Gradient ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Gradient ascent for mu_H\n",
    "\n",
    "            # Zero the gradients after the update for mu_L and mu_H\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "\n",
    "        # Update Sigma_L_half and Sigma_H_half using autograd\n",
    "        with torch.no_grad():\n",
    "            # Update Sigma_L_half and Sigma_H_half using the gradient of F\n",
    "            Sigma_L_half += eta * Sigma_L_half.grad  # Update Sigma_L_half using gradients of F\n",
    "            Sigma_H_half += eta * Sigma_H_half.grad  # Update Sigma_H_half using gradients of F\n",
    "\n",
    "            # Zero the gradients after the update for Sigma_L_half and Sigma_H_half\n",
    "            Sigma_L_half.grad.zero_()\n",
    "            Sigma_H_half.grad.zero_()\n",
    "\n",
    "        # Sigma_L and Sigma_H Updates (no autograd for final update, using proximal operator)\n",
    "        with torch.no_grad():\n",
    "            # Final update for Sigma_L and Sigma_H\n",
    "            Sigma_L_final = update_Sigma_L_final(Sigma_L_half, V, H, Sigma_H, lambda_L, lambda_param, n)\n",
    "            Sigma_H_final = update_Sigma_H_final(Sigma_H_half, V, Sigma_L, H, lambda_H, lambda_param, n)\n",
    "            \n",
    "            # Apply the proximal operator and projections\n",
    "            Sigma_L = update_Sigma_L_with_projection(Sigma_L_final)\n",
    "            Sigma_H = update_Sigma_H_with_projection(Sigma_H_final)\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary functions using PyTorch for automatic differentiation\n",
    "def F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta):\n",
    "    term1 = 0\n",
    "    term2 = 0\n",
    "    term3 = 0\n",
    "\n",
    "    # Loop to compute the sum of terms\n",
    "    for i, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "\n",
    "        term1 += torch.norm(torch.matmul(V_i, mu_L) - torch.matmul(H_i, mu_H))**2 + torch.trace(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))) + torch.trace(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)))\n",
    "\n",
    "    term2 = lambda_L * (epsilon**2 - torch.norm(mu_L - hat_mu_L)**2 - torch.norm(torch.cholesky(Sigma_L) - torch.cholesky(hat_Sigma_L))**2)\n",
    "    term3 = lambda_H * (delta**2 - torch.norm(mu_H - hat_mu_H)**2 - torch.norm(torch.cholesky(Sigma_H) - torch.cholesky(hat_Sigma_H))**2)\n",
    "\n",
    "    return term1 / n + term2 + term3\n",
    "\n",
    "\n",
    "def G_func(Sigma_L, Sigma_H, LLmodels, HLmodels, n):\n",
    "    term = 0\n",
    "    for i, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "        term += torch.norm(torch.cholesky(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T)))) * torch.norm(torch.cholesky(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))))\n",
    "    return -2 * term / n\n",
    "\n",
    "\n",
    "# Proximal operator for Sigma_L (using soft-thresholding)\n",
    "def prox_Sigma_L(Sigma_L, lambda_L, LLmodels, HLmodels, Sigma_H, n):\n",
    "    # Using the Frobenius norm as a soft-thresholding operator for Sigma_L\n",
    "    prox = torch.zeros_like(Sigma_L)\n",
    "    for i, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "\n",
    "        V_Sigma_V = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        H_Sigma_H = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        term = torch.norm(torch.cholesky(V_Sigma_V)) * torch.norm(torch.cholesky(H_Sigma_H))\n",
    "        prox[i] = (1 - lambda_L / term) * Sigma_L[i]\n",
    "    return prox\n",
    "\n",
    "\n",
    "# Proximal operator for Sigma_H (using soft-thresholding)\n",
    "def prox_Sigma_H(Sigma_H, lambda_H, LLmodels, HLmodels, Sigma_L, n):\n",
    "    prox = torch.zeros_like(Sigma_H)\n",
    "    for i, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "        \n",
    "        V_Sigma_V = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        H_Sigma_H = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "        term = torch.norm(torch.cholesky(V_Sigma_V)) * torch.norm(torch.cholesky(H_Sigma_H))\n",
    "        prox[i] = (1 - lambda_H / term) * Sigma_H[i]\n",
    "    return prox\n",
    "\n",
    "\n",
    "# Optimization loop using autograd and PyProximal (maximize using gradient ascent)\n",
    "def optimize11(LLmodels, HLmodels, n, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "    mu_L.requires_grad_(True)  # Enable autograd for mu_L\n",
    "    Sigma_L.requires_grad_(True)  # Enable autograd for Sigma_L\n",
    "    mu_H.requires_grad_(True)  # Enable autograd for mu_H\n",
    "    Sigma_H.requires_grad_(True)  # Enable autograd for Sigma_H\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # Forward pass: compute the objective function\n",
    "        F_value = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, n, epsilon, delta)\n",
    "        G_value = G_func(Sigma_L, Sigma_H, LLmodels, HLmodels, n)\n",
    "        objective = F_value + G_value  # Total objective function\n",
    "\n",
    "        # Backward pass: compute gradients (automatically using PyTorch's autograd)\n",
    "        objective.backward()\n",
    "\n",
    "        # Update the parameters using gradient ascent with autograd\n",
    "        with torch.no_grad():\n",
    "            mu_L += eta * mu_L.grad  # Ascent for mu_L\n",
    "            mu_H += eta * mu_H.grad  # Ascent for mu_H\n",
    "            Sigma_L += eta * Sigma_L.grad  # Ascent for Sigma_L\n",
    "            Sigma_H += eta * Sigma_H.grad  # Ascent for Sigma_H\n",
    "\n",
    "            # Apply the proximal operator to Sigma_L and Sigma_H (e.g., for regularization)\n",
    "            Sigma_L = prox_Sigma_L(Sigma_L, lambda_L, LLmodels, HLmodels, Sigma_H, n)\n",
    "            Sigma_H = prox_Sigma_H(Sigma_H, lambda_H, LLmodels, HLmodels, Sigma_L, n)\n",
    "\n",
    "            # Zero the gradients after the update\n",
    "            mu_L.grad.zero_()\n",
    "            mu_H.grad.zero_()\n",
    "            Sigma_L.grad.zero_()\n",
    "            Sigma_H.grad.zero_()\n",
    "\n",
    "        # Print progress\n",
    "        if t % 10 == 0:\n",
    "            print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "# Run optimization\n",
    "mu_L, Sigma_L, mu_H, Sigma_H = optimize(LLmodels, HLmodels, n, mu_L.float(), Sigma_L.float(), mu_H.float(), Sigma_H.float(), hat_mu_L.float(), hat_Sigma_L.float(), hat_mu_H.float(), hat_Sigma_H.float(), epsilon, delta, lambda_L, lambda_H, eta, max_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b26889",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "1. Apply LLmodels and  HLmodels to the current closed-form and autograd implementations.\n",
    "\n",
    "2. Look for libraries for proximal operators to incorporate in autograd.\n",
    "\n",
    "3. Check whether I can run autograd direct even for the non-smooth term.\n",
    "\n",
    "4. Merge T descent with theta_l, theta_h ascent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
