{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local modules\n",
    "import modularised_utils as mut\n",
    "import opt_utils as oput\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'little_lucas'\n",
    "abduction  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon, delta           = params.radius[experiment]\n",
    "ll_num_envs, hl_num_envs = params.n_envs[experiment]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples, num_hlsamples  = params.n_samples[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll      = mut.load_samples(experiment)[None][0] \n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "\n",
    "Dhl      = mut.load_samples(experiment)[None][1] \n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "\n",
    "omega    = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if abduction == True:\n",
    "    U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "    U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)\n",
    "else:\n",
    "    U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.load_exogenous(experiment, 'LL')\n",
    "    U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.load_exogenous(experiment, 'HL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1cdb99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.35197865e-02, -8.29807720e-05,\n",
       "        -2.54723483e-05,  8.26212323e-02],\n",
       "       [ 0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.67646137e-02, -6.62578297e-05,\n",
       "        -2.03389589e-05, -5.78675622e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.75571311e-02,\n",
       "         8.45912640e-03,  2.40675555e-03],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.63587032e-01,  1.92522493e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  2.47557579e-03,\n",
       "         7.59919765e-04,  2.16209219e-04],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "         3.06966875e-01,  8.73369418e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  3.41602063e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLmodels[None]._compute_reduced_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6c38736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiguity set construction: Based on epsilon and delta include distribution (as many as the num_envs) that\n",
    "# # pass the \"gelbrich\" test.\n",
    "# ll_moments = mut.sample_moments_U(mu_hat    = mu_U_ll_hat,\n",
    "#                                   Sigma_hat = Sigma_U_ll_hat,\n",
    "#                                   bound     = epsilon,\n",
    "#                                   num_envs  = ll_num_envs)\n",
    "\n",
    "# A_ll       = mut.sample_distros_Gelbrich(ll_moments) #Low-level: A_epsilon\n",
    "\n",
    "\n",
    "# hl_moments = mut.sample_moments_U(mu_hat    = mu_U_hl_hat,\n",
    "#                                   Sigma_hat = Sigma_U_hl_hat,\n",
    "#                                   bound     = delta,\n",
    "#                                   num_envs  = hl_num_envs)\n",
    "\n",
    "# A_hl       = mut.sample_distros_Gelbrich(hl_moments) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcd01dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstraction_errors             = {}\n",
    "# abstraction_env_errors         = {}\n",
    "# max_env_avg_interv_error_value = -np.inf\n",
    "# max_env_avg_interv_error_key   = None\n",
    "# distance_err                   = 'wass'\n",
    "\n",
    "# for lenv in A_ll:\n",
    "\n",
    "#     Dll_noise      = lenv.sample(num_llsamples)[0]\n",
    "#     ll_environment = mut.get_exogenous_distribution(Dll_noise)\n",
    "\n",
    "#     for henv in A_hl:\n",
    "#         Dhl_noise      = henv.sample(num_hlsamples)[0]\n",
    "#         hl_environment = mut.get_exogenous_distribution(Dhl_noise)\n",
    "\n",
    "#         total_ui_error = 0\n",
    "#         num_distros    = len(Ill)\n",
    "\n",
    "#         n, m  = len(LLmodels[None].endogenous_vars), len(HLmodels[None].endogenous_vars)\n",
    "\n",
    "#         T     = mut.sample_stoch_matrix(n, m)\n",
    "\n",
    "#         for iota in Ill:\n",
    "#             llcm   = LLmodels[iota]\n",
    "#             hlcm   = HLmodels[omega[iota]]\n",
    "#             llmech = llcm.compute_mechanism()\n",
    "#             hlmech = hlcm.compute_mechanism()\n",
    "#             error  = mut.ui_error_dist(distance_err, lenv, henv, llmech, hlmech, T)\n",
    "\n",
    "#             total_ui_error += error\n",
    "\n",
    "#         avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "#         if avg_interv_error > max_env_avg_interv_error_value:\n",
    "#             max_env_avg_interv_error_value = avg_interv_error\n",
    "#             max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "#         abstraction_errors[str(T)] = avg_interv_error\n",
    "#         abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error\n",
    "\n",
    "\n",
    "# max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "# max_error = abstraction_errors[max_tau]\n",
    "\n",
    "# print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "# print('==============================================================================' )\n",
    "# max_lenv = max_env_avg_interv_error_key[0]\n",
    "# max_henv = max_env_avg_interv_error_key[1]\n",
    "\n",
    "# print(f\"max LL mean vector = {max_lenv.means_}\")\n",
    "# print(f\"max LL covariance = {max_lenv.covariances_}\")\n",
    "# print( )\n",
    "\n",
    "# print(f\"max HL mean vector = {max_henv.means_}\")\n",
    "# print(f\"max HL covariance = {max_henv.covariances_}\")\n",
    "# print('==============================================================================' )\n",
    "# print(f\"max environment, average interventional abstraction error = {max_env_avg_interv_error_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a7f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta):\n",
    "    grad_mu_L = torch.zeros_like(mu_L, dtype=torch.float32) \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form()).float() \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float() \n",
    "\n",
    "        grad_mu_L += torch.matmul(V_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float())) \n",
    "    \n",
    "    grad_mu_L = (2 / n) * grad_mu_L - 2 * lambda_L * (mu_L - hat_mu_L)\n",
    "    mu_L = mu_L + (eta * grad_mu_L)\n",
    "    return mu_L\n",
    "\n",
    "def update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta):\n",
    "    grad_mu_H = torch.zeros_like(mu_H, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()  \n",
    "\n",
    "        grad_mu_H -= torch.matmul(H_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float()))\n",
    "    \n",
    "    grad_mu_H = (2 / n) * grad_mu_H - 2 * lambda_H * (mu_H - hat_mu_H)\n",
    "    \n",
    "    mu_H = mu_H + (eta * grad_mu_H)\n",
    "    return mu_H\n",
    "\n",
    "\n",
    "def update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta):\n",
    "\n",
    "    grad_Sigma_L = torch.zeros_like(Sigma_L)\n",
    "    term1 = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form())\n",
    "        V_i = T @ L_i.float()\n",
    "        term1 = term1 + torch.matmul(V_i.T, V_i)\n",
    "\n",
    "    Sigma_L_sqrt     = oput.sqrtm_svd(Sigma_L)  \n",
    "    hat_Sigma_L_sqrt = oput.sqrtm_svd(hat_Sigma_L) \n",
    "\n",
    "    term2 = -2 * lambda_L * (Sigma_L_sqrt - hat_Sigma_L_sqrt) @ torch.inverse(Sigma_L_sqrt)\n",
    "\n",
    "    grad_Sigma_L = (2 / n) * term1 + term2\n",
    "\n",
    "    Sigma_L_half = Sigma_L + eta * grad_Sigma_L\n",
    "    #Sigma_L_half  = oput.diagonalize(Sigma_L_half)\n",
    "    return Sigma_L_half\n",
    "\n",
    "def update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param):\n",
    "    if check_for_invalid_values(Sigma_L_half):\n",
    "        print(\"Sigma_L_half contains NaN or Inf values!\")\n",
    "        print(Sigma_L_half)\n",
    "    if check_for_invalid_values(Sigma_H):\n",
    "        print(\"Sigma_H contains NaN or Inf values!\")\n",
    "    Sigma_L_final = torch.zeros_like(Sigma_L_half, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()  \n",
    "        \n",
    "        Sigma_L_half      = Sigma_L_half.float()\n",
    "        V_Sigma_V         = torch.matmul(V_i, torch.matmul(Sigma_L_half, V_i.T))\n",
    "\n",
    "        sqrtm_V_Sigma_V   = oput.sqrtm_svd(oput.regmat(V_Sigma_V)) \n",
    "        #sqrtm_V_Sigma_V   = oput.sqrtm_svd(V_Sigma_V)\n",
    "\n",
    "        prox_Sigma_L_half = torch.matmul(oput.prox_operator(sqrtm_V_Sigma_V, lambda_param), oput.prox_operator(sqrtm_V_Sigma_V, lambda_param).T)\n",
    "        \n",
    "        #V_i       = V_i\n",
    "        V_i       = oput.regmat(V_i) \n",
    "\n",
    "        #ll_term_a = torch.matmul(torch.linalg.pinv(V_i), prox_Sigma_L_half)\n",
    "        ll_term_a = torch.matmul(oput.regmat(torch.linalg.pinv(V_i)), oput.regmat(prox_Sigma_L_half)) \n",
    "\n",
    "        ll_term_b = torch.linalg.pinv(V_i).T\n",
    "        ll_term   = torch.matmul(ll_term_a, ll_term_b)\n",
    "        #ll_term  = torch.matmul(torch.matmul(torch.linalg.pinv(V_i), oput.regmat(prox_Sigma_L_half)), torch.linalg.pinv(V_i).T)\n",
    "\n",
    "        Sigma_H   = Sigma_H.float()  \n",
    "        H_Sigma_H = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)).float()\n",
    "\n",
    "        #hl_term   = torch.norm(oput.sqrtm_svd(H_Sigma_H), p='fro')\n",
    "        hl_term   = torch.norm(oput.sqrtm_svd(oput.regmat(H_Sigma_H)), p='fro') \n",
    "\n",
    "        Sigma_L_final = Sigma_L_final + (ll_term * hl_term)\n",
    "\n",
    "    Sigma_L_final = Sigma_L_final * (2 / n)\n",
    "    Sigma_L_final = oput.diagonalize(Sigma_L_final)\n",
    "\n",
    "    return Sigma_L_final\n",
    "\n",
    "\n",
    "def update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta):\n",
    "    grad_Sigma_H = torch.zeros_like(Sigma_H)\n",
    "    term1        = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        H_i   = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()\n",
    "        term1 = term1 + torch.matmul(H_i.T, H_i)\n",
    "\n",
    "    Sigma_H_sqrt     = oput.sqrtm_svd(Sigma_H)  \n",
    "    hat_Sigma_H_sqrt = oput.sqrtm_svd(hat_Sigma_H) \n",
    "\n",
    "    term2 = -2 * lambda_H * (Sigma_H_sqrt - hat_Sigma_H_sqrt) @ torch.inverse(Sigma_H_sqrt)\n",
    "\n",
    "    grad_Sigma_H = (2 / n) * term1 + term2\n",
    "\n",
    "    Sigma_H_half = Sigma_H + eta * grad_Sigma_H\n",
    "    Sigma_H_half  = oput.diagonalize(Sigma_H_half)\n",
    "    return Sigma_H_half\n",
    "\n",
    "def check_for_invalid_values(matrix):\n",
    "    if torch.isnan(matrix).any() or torch.isinf(matrix).any():\n",
    "        #print(\"Matrix contains NaN or Inf values!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def handle_nans(matrix, replacement_value=0.0):\n",
    "    # Replace NaNs with a given value (default is 0)\n",
    "    if torch.isnan(matrix).any():\n",
    "        print(\"Warning: NaN values found! Replacing with zero.\")\n",
    "        matrix = torch.nan_to_num(matrix, nan=replacement_value)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param):\n",
    "    if check_for_invalid_values(Sigma_L):\n",
    "        print(\"Sigma_L contains NaN or Inf values!\")\n",
    "    if check_for_invalid_values(Sigma_H_half):\n",
    "        print(\"Sigma_H_half contains NaN or Inf values!\")\n",
    "\n",
    "    Sigma_H_final = torch.zeros_like(Sigma_H_half)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()\n",
    "\n",
    "        H_Sigma_H         = torch.matmul(H_i, torch.matmul(Sigma_H_half, H_i.T))\n",
    "        #sqrtm_H_Sigma_H   = oput.sqrtm_svd(H_Sigma_H)\n",
    "        sqrtm_H_Sigma_H   = oput.sqrtm_svd(oput.regmat(H_Sigma_H)) \n",
    "        prox_Sigma_H_half = torch.matmul(oput.prox_operator(sqrtm_H_Sigma_H, lambda_param), oput.prox_operator(sqrtm_H_Sigma_H, lambda_param).T)\n",
    "        #hl_term           = torch.matmul(torch.matmul(torch.inverse(H_i), prox_Sigma_H_half), torch.inverse(H_i).T)  \n",
    "        hl_term           = torch.matmul(torch.matmul(torch.inverse(H_i), oput.regmat(prox_Sigma_H_half)), torch.inverse(H_i).T)  \n",
    "        \n",
    "        V_Sigma_V = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        #ll_term   = torch.norm(oput.sqrtm_svd(V_Sigma_V))\n",
    "        ll_term   = torch.norm(oput.sqrtm_svd(oput.regmat(V_Sigma_V)))\n",
    "\n",
    "        Sigma_H_final = Sigma_H_final + (ll_term * hl_term)\n",
    "    \n",
    "    Sigma_H_final = Sigma_H_final * (2 / n)\n",
    "    #Sigma_H_final = oput.diagonalize(Sigma_H_final)\n",
    "    \n",
    "    return Sigma_H_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "996b4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "#     # Constraint 1: epsilon^2 - ||mu_L - hat_mu_L||_2^2 - ||Sigma_L^{1/2} - hat_Sigma_L^{1/2}||_2^2 >= 0\n",
    "#     constraint_L = epsilon**2 - (torch.norm(mu_L - hat_mu_L)**2) - (torch.norm(oput.sqrtm_svd(Sigma_L) - oput.sqrtm_svd(hat_Sigma_L))**2)\n",
    "    \n",
    "#     # Constraint 2: delta^2 - ||mu_H - hat_mu_H||_2^2 - ||Sigma_H^{1/2} - hat_Sigma_H^{1/2}||_2^2 >= 0\n",
    "#     constraint_H = delta**2 - (torch.norm(mu_H - hat_mu_H)**2) - (torch.norm(oput.sqrtm_svd(Sigma_H) - oput.sqrtm_svd(hat_Sigma_H))**2)\n",
    "    \n",
    "#     # Return whether constraints are satisfied (i.e., >= 0) and the constraint violations\n",
    "#     return constraint_L, constraint_H\n",
    "\n",
    "\n",
    "# def enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "#     constraint_L, constraint_H = check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "    \n",
    "#     # Clip values if constraints are violated\n",
    "#     if constraint_L < 0:\n",
    "#         print(f\"Constraint for mu_L and Sigma_L violated. Fixing...\")\n",
    "#         mu_L = hat_mu_L + torch.clamp(mu_L - hat_mu_L, min=-epsilon, max=epsilon)\n",
    "#         Sigma_L = hat_Sigma_L + torch.clamp(Sigma_L - hat_Sigma_L, min=-epsilon, max=epsilon)\n",
    "    \n",
    "#     if constraint_H < 0:\n",
    "#         print(f\"Constraint for mu_H and Sigma_H violated. Fixing...\")\n",
    "#         mu_H = hat_mu_H + torch.clamp(mu_H - hat_mu_H, min=-delta, max=delta)\n",
    "#         Sigma_H = hat_Sigma_H + torch.clamp(Sigma_H - hat_Sigma_H, min=-delta, max=delta)\n",
    "    \n",
    "#     return mu_L, Sigma_L, mu_H, Sigma_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db35b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max, epsilon, delta, seed):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    for t in range(num_steps_max): \n",
    "        mu_L         = update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "        mu_H         = update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "        Sigma_L_half = update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "        Sigma_L      = update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "        Sigma_H_half = update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "        Sigma_H      = update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "        \n",
    "        # Project onto Gelbrich balls\n",
    "        mu_L, Sigma_L = oput.project_onto_gelbrich_ball(mu_L, Sigma_L, hat_mu_L, hat_Sigma_L, epsilon)\n",
    "        mu_H, Sigma_H = oput.project_onto_gelbrich_ball(mu_H, Sigma_H, hat_mu_H, hat_Sigma_H, delta)\n",
    "        \n",
    "        # Verify constraints\n",
    "        satisfied_L, dist_L, epsi = oput.verify_gelbrich_constraint(mu_L, Sigma_L, hat_mu_L, hat_Sigma_L, epsilon)\n",
    "        satisfied_H, dist_H, delt = oput.verify_gelbrich_constraint(mu_H, Sigma_H, hat_mu_H, hat_Sigma_H, delta)\n",
    "        \n",
    "        if not satisfied_L:\n",
    "            print(f\"Warning: Constraints not satisfied for mu_L and Sigma_L! Distance: {dist_L} and epsilon = {epsi}\")\n",
    "\n",
    "        if not satisfied_H:\n",
    "            print(f\"Warning: Constraints not satisfied for mu_H and Sigma_H! Distance: {dist_H} and delta = {delt}\")\n",
    "        \n",
    "        obj = 0\n",
    "        \n",
    "        for i, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form())\n",
    "            V_i = T @ L_i.float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()\n",
    "                        \n",
    "            L_i_mu_L = V_i @ mu_L\n",
    "            H_i_mu_H = H_i @ mu_H\n",
    "            term1 = torch.norm(L_i_mu_L.float() - H_i_mu_H.float())**2\n",
    "            \n",
    "            V_Sigma_V = oput.regmat(V_i.float() @ Sigma_L.float() @ V_i.T.float())\n",
    "            H_Sigma_H = oput.regmat(H_i.float() @ Sigma_H.float() @ H_i.T.float())\n",
    "            #V_Sigma_V = V_i.float() @ Sigma_L.float() @ V_i.T.float()\n",
    "            #H_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "\n",
    "            term2 = torch.trace(V_Sigma_V)\n",
    "            term3 = torch.trace(H_Sigma_H)\n",
    "            \n",
    "            sqrtVSV = oput.sqrtm_svd(V_Sigma_V)\n",
    "            sqrtHSH = oput.sqrtm_svd(H_Sigma_H)\n",
    "\n",
    "            term4 = -2 * torch.trace(oput.sqrtm_svd(oput.regmat(sqrtVSV @ sqrtHSH @ sqrtVSV)))\n",
    "            #term4 = -2 * torch.trace(oput.sqrtm_svd(sqrtVSV @ sqrtHSH @ sqrtVSV))\n",
    "            #term4 = -2 * torch.norm(sqrtVSV @ sqrtHSH, 'nuc')\n",
    "            \n",
    "            obj = obj + (term1 + term2 + term3 + term4)\n",
    "        \n",
    "        obj = obj/i\n",
    "        \n",
    "        #print(f\"Max step {t+1}/{num_steps_max}, Objective: {obj.item()}\")\n",
    "\n",
    "    return obj, mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "\n",
    "def optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    T = torch.nn.init.xavier_normal_(T, gain=0.01)\n",
    "    #objective_T = 0\n",
    "    objective_T = torch.tensor(0.0)  # Initialize as tensor instead of int \n",
    "    for step in range(num_steps_min):\n",
    "        #objective_T = 0  # Reset objective at the start of each step\n",
    "        objective_T = torch.tensor(0.0)\n",
    "        for n, iota in enumerate(Ill):\n",
    "            # Check for inf/nan values\n",
    "            if torch.isinf(objective_T).any() or torch.isnan(objective_T).any():\n",
    "                print(f\"Invalid values detected in objective at step {step}\")\n",
    "                break\n",
    "\n",
    "            L_i = torch.from_numpy(LLmodels[iota]._compute_reduced_form()).float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]]._compute_reduced_form()).float()\n",
    "            \n",
    "            L_i_mu_L = L_i @ mu_L  \n",
    "            H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "            term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "            term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "            term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "            \n",
    "            L_i_Sigma_L = oput.regmat(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "            H_i_Sigma_H = oput.regmat(H_i @ Sigma_H @ H_i.T)\n",
    "\n",
    "            # L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "            # H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "\n",
    "            #term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "            term4 = -2 * torch.trace(oput.sqrtm_svd(oput.sqrtm_svd(L_i_Sigma_L) @ H_i_Sigma_H @ oput.sqrtm_svd(L_i_Sigma_L)))\n",
    "\n",
    "            objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "        objective_T = objective_T/n\n",
    "        reg_lambda = 1e-4\n",
    "        reg_term = reg_lambda * torch.norm(T, p=2)\n",
    "        objective_T = objective_T + reg_term\n",
    "\n",
    "\n",
    "        optimizer_T.zero_grad() \n",
    "        objective_T.backward(retain_graph=True)  \n",
    "        optimizer_T.step()      \n",
    "\n",
    "        #print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "\n",
    "    return objective_T, T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49a99b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, \n",
    "                     hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                     epsilon, delta, lambda_L, lambda_H, lambda_param, \n",
    "                     eta_min, eta_max, max_iter, num_steps_min, num_steps_max, tol, seed):\n",
    "    \n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    #T           = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "    T = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "    #optimizer_T = torch.optim.Adam([T], lr=0.001)\n",
    "    #optimizer_T = torch.optim.Adam([T], lr=eta_min, eps=1e-8, amsgrad=True)\n",
    "    \n",
    "    optimizer_T        = torch.optim.SGD([T], lr=1e-4, momentum=0.9)\n",
    "    previous_objective = float('inf')  \n",
    "    for epoch in tqdm(range(max_iter)): \n",
    "        # ---- Minimize T ----\n",
    "        objective_T, T = optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed)\n",
    "        \n",
    "        # ---- Maximize mu_L, Sigma_L, mu_H, Sigma_H ----\n",
    "        objective_theta, mu_L, Sigma_L, mu_H, Sigma_H = optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n",
    "                                                                         lambda_L, lambda_H, lambda_param, eta_max, num_steps_max, epsilon, delta, seed)\n",
    "        if oput.contains_negative(Sigma_L) == True:\n",
    "            print('Sigma_L contains negative values')\n",
    "            #print(Sigma_L)\n",
    "            print( )\n",
    "        if oput.contains_negative(Sigma_H) == True:\n",
    "            print('Sigma_H contains negative values')\n",
    "            #print(Sigma_H)\n",
    "            print( )\n",
    "\n",
    "        criterion = abs(previous_objective - objective_T.item())\n",
    "        \n",
    "        if criterion < tol:\n",
    "            print(f\"Convergence reached at epoch {epoch+1} with objective {objective_T.item()}\")\n",
    "            break\n",
    "\n",
    "        # Update previous objective for the next check\n",
    "        previous_objective = objective_T.item()\n",
    "\n",
    "    print(\"Final T:\", T)\n",
    "    print(\"Final mu_L:\", mu_L)\n",
    "    print(\"Final Sigma_L:\", Sigma_L)\n",
    "    print(\"Final mu_H:\", mu_H)\n",
    "    print(\"Final Sigma_H:\", Sigma_H)\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3913bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "hat_Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "hat_mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "hat_Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = hat_mu_L.shape[0]\n",
    "h = hat_mu_H.shape[0]\n",
    "\n",
    "# Gelbrich initialization\n",
    "ll_moments      = mut.sample_moments_U(mu_hat = mu_U_ll_hat, Sigma_hat = Sigma_U_ll_hat, bound = epsilon, num_envs = 1)\n",
    "mu_L0, Sigma_L0 = ll_moments[0]\n",
    "mu_L0, Sigma_L0 = torch.from_numpy(mu_L0).float(), torch.from_numpy(Sigma_L0).float()\n",
    "\n",
    "hl_moments      = mut.sample_moments_U(mu_hat = mu_U_hl_hat, Sigma_hat = Sigma_U_hl_hat, bound = delta, num_envs = 1)\n",
    "mu_H0, Sigma_H0 = hl_moments[0]\n",
    "mu_H0, Sigma_H0 = torch.from_numpy(mu_H0).float(), torch.from_numpy(Sigma_H0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504a3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:08,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma_L contains NaN or Inf values!\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "Invalid values detected in objective at step 1\n",
      "O\n",
      "Invalid values detected in objective at step 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.linalg.eig: input tensor should not contain infs or NaNs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H, T \u001b[38;5;241m=\u001b[39m optimize_min_max(\n\u001b[1;32m      2\u001b[0m                                                    mu_L0, Sigma_L0, mu_H0, Sigma_H0, LLmodels, HLmodels, \n\u001b[1;32m      3\u001b[0m                                                    hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n\u001b[1;32m      4\u001b[0m                                                    epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lambda_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, \n\u001b[1;32m      5\u001b[0m                                                    eta_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,  num_steps_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_steps_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      6\u001b[0m                                                   )\n",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m, in \u001b[0;36moptimize_min_max\u001b[0;34m(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, lambda_param, eta_min, eta_max, max_iter, num_steps_min, num_steps_max, tol, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m objective_T, T \u001b[38;5;241m=\u001b[39m optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ---- Maximize mu_L, Sigma_L, mu_H, Sigma_H ----\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m objective_theta, mu_L, Sigma_L, mu_H, Sigma_H \u001b[38;5;241m=\u001b[39m optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n\u001b[1;32m     22\u001b[0m                                                                  lambda_L, lambda_H, lambda_param, eta_max, num_steps_max, epsilon, delta, seed)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m oput\u001b[38;5;241m.\u001b[39mcontains_negative(Sigma_L) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSigma_L contains negative values\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36moptimize_max\u001b[0;34m(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max, epsilon, delta, seed)\u001b[0m\n\u001b[1;32m      7\u001b[0m mu_L         \u001b[38;5;241m=\u001b[39m update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n\u001b[1;32m      8\u001b[0m mu_H         \u001b[38;5;241m=\u001b[39m update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n\u001b[0;32m----> 9\u001b[0m Sigma_L_half \u001b[38;5;241m=\u001b[39m update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n\u001b[1;32m     10\u001b[0m Sigma_L      \u001b[38;5;241m=\u001b[39m update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n\u001b[1;32m     11\u001b[0m Sigma_H_half \u001b[38;5;241m=\u001b[39m update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
      "Cell \u001b[0;32mIn[29], line 46\u001b[0m, in \u001b[0;36mupdate_Sigma_L_half\u001b[0;34m(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\u001b[0m\n\u001b[1;32m     43\u001b[0m grad_Sigma_L \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m n) \u001b[38;5;241m*\u001b[39m term1 \u001b[38;5;241m+\u001b[39m term2\n\u001b[1;32m     45\u001b[0m Sigma_L_half \u001b[38;5;241m=\u001b[39m Sigma_L \u001b[38;5;241m+\u001b[39m eta \u001b[38;5;241m*\u001b[39m grad_Sigma_L\n\u001b[0;32m---> 46\u001b[0m Sigma_L_half  \u001b[38;5;241m=\u001b[39m oput\u001b[38;5;241m.\u001b[39mdiagonalize(Sigma_L_half)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Sigma_L_half\n",
      "File \u001b[0;32m~/Desktop/ERiCA/opt_utils.py:49\u001b[0m, in \u001b[0;36mdiagonalize\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdiagonalize\u001b[39m(A):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Get eigenvalues and eigenvectors\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     eigvals, eigvecs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39meig(A)  \n\u001b[1;32m     50\u001b[0m     eigvals_real     \u001b[38;5;241m=\u001b[39m eigvals\u001b[38;5;241m.\u001b[39mreal  \n\u001b[1;32m     51\u001b[0m     eigvals_real     \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(eigvals_real)  \u001b[38;5;66;03m# Take the square root of the eigenvalues\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.linalg.eig: input tensor should not contain infs or NaNs."
     ]
    }
   ],
   "source": [
    "mu_L, Sigma_L, mu_H, Sigma_H, T = optimize_min_max(\n",
    "                                                   mu_L0, Sigma_L0, mu_H0, Sigma_H0, LLmodels, HLmodels, \n",
    "                                                   hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                                                   epsilon=0.5, delta=0.5, lambda_L=0.5, lambda_H=0.5, lambda_param=0.5, \n",
    "                                                   eta_max=0.001, eta_min=0.0001, max_iter=50,  num_steps_min=3, num_steps_max=1, tol=1e-6, seed=42\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e0f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb75a353",
   "metadata": {},
   "source": [
    "### Run a grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a3a1005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running combination 1/5: {'lambda_param': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.6026020050048828, Gradient Norm = 3.541008710861206\n",
      "Step 1/1: Objective = 2.157719850540161, Gradient Norm = 3.592430353164673\n",
      "Step 1/1: Objective = 2.0324082374572754, Gradient Norm = 2.9353740215301514\n",
      "Step 1/1: Objective = 2.0185794830322266, Gradient Norm = 2.871495246887207\n",
      "Step 1/1: Objective = 2.0127692222595215, Gradient Norm = 2.8650739192962646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 2.007770538330078, Gradient Norm = 2.8635976314544678\n",
      "Step 1/1: Objective = 2.00293231010437, Gradient Norm = 2.862715482711792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:00<00:00, 15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.9981982707977295, Gradient Norm = 2.8620455265045166\n",
      "Step 1/1: Objective = 1.9935672283172607, Gradient Norm = 2.8615567684173584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.9890460968017578, Gradient Norm = 2.861253261566162\n",
      "Final T: tensor([[ 0.3467,  0.1189,  0.2439],\n",
      "        [ 0.2403, -1.1140, -0.1766]], requires_grad=True)\n",
      "Final mu_L: tensor([ 0.1181, -0.0881,  0.1137], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5633, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.1358, 0.1298], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[1.5000, 0.0000],\n",
      "        [0.0000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Results saved to: synth1_erica_results/result_1.pkl\n",
      "Running combination 2/5: {'lambda_param': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.6026020050048828, Gradient Norm = 3.541008710861206\n",
      "Step 1/1: Objective = 1.7905457019805908, Gradient Norm = 3.2711431980133057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00, 26.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 2.181090831756592, Gradient Norm = 3.886312246322632\n",
      "Step 1/1: Objective = 1.9982795715332031, Gradient Norm = 2.9500863552093506\n",
      "Step 1/1: Objective = 1.9930298328399658, Gradient Norm = 2.9484946727752686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:00<00:00, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.9878889322280884, Gradient Norm = 2.9471583366394043\n",
      "Step 1/1: Objective = 1.9828557968139648, Gradient Norm = 2.946053981781006\n",
      "Step 1/1: Objective = 1.9779307842254639, Gradient Norm = 2.9451639652252197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.9731204509735107, Gradient Norm = 2.944492816925049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.9684324264526367, Gradient Norm = 2.9440340995788574\n",
      "Final T: tensor([[ 0.3466,  0.1188,  0.2443],\n",
      "        [ 0.2402, -1.1142, -0.1766]], requires_grad=True)\n",
      "Final mu_L: tensor([ 0.1181, -0.0881,  0.1137], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.1358, 0.1298], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[1.5000, 0.0000],\n",
      "        [0.0000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Results saved to: synth1_erica_results/result_2.pkl\n",
      "Running combination 3/5: {'lambda_param': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.6026020050048828, Gradient Norm = 3.541008710861206\n",
      "Step 1/1: Objective = 1.5258506536483765, Gradient Norm = 3.138038396835327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 29.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.5836281776428223, Gradient Norm = 3.502628803253174\n",
      "Step 1/1: Objective = 1.6426475048065186, Gradient Norm = 3.956981897354126\n",
      "Step 1/1: Objective = 1.697021722793579, Gradient Norm = 4.438928604125977\n",
      "Step 1/1: Objective = 1.744328498840332, Gradient Norm = 4.839473247528076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00, 19.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.78388249874115, Gradient Norm = 5.183915138244629\n",
      "Step 1/1: Objective = 1.8157119750976562, Gradient Norm = 5.470893859863281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.8403030633926392, Gradient Norm = 5.699570655822754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.858405351638794, Gradient Norm = 5.874941825866699\n",
      "Final T: tensor([[ 0.3467,  0.1187,  0.2387],\n",
      "        [ 0.2405, -1.1128, -0.1763]], requires_grad=True)\n",
      "Final mu_L: tensor([ 0.1181, -0.0881,  0.1137], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5000]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.1359, 0.1298], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[1.4179, 0.0000],\n",
      "        [0.0000, 1.4033]], grad_fn=<DiagEmbedBackward0>)\n",
      "Results saved to: synth1_erica_results/result_3.pkl\n",
      "Running combination 4/5: {'lambda_param': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.6026020050048828, Gradient Norm = 3.541008710861206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 29.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.3395202159881592, Gradient Norm = 3.0672192573547363\n",
      "Step 1/1: Objective = 1.2413148880004883, Gradient Norm = 3.0120716094970703\n",
      "Step 1/1: Objective = 1.2113540172576904, Gradient Norm = 2.903855323791504\n",
      "Step 1/1: Objective = 1.202183723449707, Gradient Norm = 2.840022563934326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00, 19.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.197507619857788, Gradient Norm = 2.8274905681610107\n",
      "Step 1/1: Objective = 1.1934016942977905, Gradient Norm = 2.820382833480835\n",
      "Step 1/1: Objective = 1.1894582509994507, Gradient Norm = 2.813884973526001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:00<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.1856379508972168, Gradient Norm = 2.807492971420288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.1819442510604858, Gradient Norm = 2.801156520843506\n",
      "Final T: tensor([[ 0.3458,  0.1190,  0.2441],\n",
      "        [ 0.2401, -1.1129, -0.1766]], requires_grad=True)\n",
      "Final mu_L: tensor([ 0.1181, -0.0881,  0.1137], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.8770]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.1358, 0.1298], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[0.5000, 0.0000],\n",
      "        [0.0000, 0.5000]], grad_fn=<AddBackward0>)\n",
      "Results saved to: synth1_erica_results/result_4.pkl\n",
      "Running combination 5/5: {'lambda_param': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.6026020050048828, Gradient Norm = 3.541008710861206\n",
      "Step 1/1: Objective = 1.2434988021850586, Gradient Norm = 2.9818167686462402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.1818166971206665, Gradient Norm = 2.768096923828125\n",
      "Step 1/1: Objective = 1.173356294631958, Gradient Norm = 2.735426187515259\n",
      "Step 1/1: Objective = 1.1689836978912354, Gradient Norm = 2.7261195182800293\n",
      "Step 1/1: Objective = 1.1651160717010498, Gradient Norm = 2.7191154956817627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.1613925695419312, Gradient Norm = 2.71236252784729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:00<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.157782793045044, Gradient Norm = 2.705651044845581\n",
      "Step 1/1: Objective = 1.1542861461639404, Gradient Norm = 2.6989614963531494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1: Objective = 1.1509135961532593, Gradient Norm = 2.6922953128814697\n",
      "Final T: tensor([[ 0.3459,  0.1190,  0.2443],\n",
      "        [ 0.2401, -1.1130, -0.1767]], requires_grad=True)\n",
      "Final mu_L: tensor([ 0.1181, -0.0881,  0.1137], grad_fn=<AddBackward0>)\n",
      "Final Sigma_L: tensor([[0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.5000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5837]], grad_fn=<AddBackward0>)\n",
      "Final mu_H: tensor([0.1358, 0.1298], grad_fn=<AddBackward0>)\n",
      "Final Sigma_H: tensor([[0.5000, 0.0000],\n",
      "        [0.0000, 0.5000]], grad_fn=<AddBackward0>)\n",
      "Results saved to: synth1_erica_results/result_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_dir = f\"{experiment}_erica_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid = {\n",
    "    'lambda_param': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "    # Add other hyperparameters here\n",
    "}\n",
    "\n",
    "# Fixed parameters\n",
    "fixed_params = {\n",
    "    'epsilon': 0.5,\n",
    "    'delta': 0.5,\n",
    "    'eta_max': 0.01,\n",
    "    'eta_min': 0.001,\n",
    "    'lambda_L': 0.8,\n",
    "    'lambda_H': 0.8,\n",
    "    'max_iter': 10,\n",
    "    'num_steps_min': 1,\n",
    "    'num_steps_max': 1,\n",
    "    'tol_max': 1e-4,\n",
    "    'tol': 1e-4,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Iterate over each combination\n",
    "for idx, params in enumerate(param_combinations, 1):\n",
    "    try:\n",
    "        print(f\"Running combination {idx}/{len(param_combinations)}: {params}\")\n",
    "\n",
    "        # Run your optimization function here\n",
    "        final_mu_L, final_Sigma_L, final_mu_H, final_Sigma_H, final_T = optimize_min_max(\n",
    "                                                                                        mu_L0, Sigma_L0, mu_H0, Sigma_H0,\n",
    "                                                                                        LLmodels, HLmodels,\n",
    "                                                                                        hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n",
    "                                                                                        **params,\n",
    "                                                                                        **fixed_params\n",
    "                                                                                        )\n",
    "\n",
    "        # Prepare results\n",
    "        results = {\n",
    "            'parameters': params,\n",
    "            'mu_L': final_mu_L.detach().numpy(),\n",
    "            'Sigma_L': final_Sigma_L.detach().numpy(),\n",
    "            'mu_H': final_mu_H.detach().numpy(),\n",
    "            'Sigma_H': final_Sigma_H.detach().numpy(),\n",
    "            'T': final_T.detach().numpy(),\n",
    "            'execution_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        result_filename = f\"{results_dir}/result_{idx}.pkl\"\n",
    "        with open(result_filename, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "        print(f\"Results saved to: {result_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with parameters {params}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1442a9d4",
   "metadata": {},
   "source": [
    "### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d53133ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters used:\n",
      "{'lambda_param': 1.0}\n",
      "T matrix:\n",
      "[[ 0.34585983  0.11897802  0.24429919]\n",
      " [ 0.24009041 -1.1129556  -0.17671084]]\n",
      "Execution time: 2024-12-03 10:55:55\n"
     ]
    }
   ],
   "source": [
    "with open('synth1_erica_results/result_5.pkl', 'rb') as f:  # Replace with your actual filename\n",
    "    loaded_results = pickle.load(f)\n",
    "\n",
    "# Access specific results\n",
    "print(f\"Parameters used:\\n{loaded_results['parameters']}\")\n",
    "print(f\"T matrix:\\n{loaded_results['T']}\")\n",
    "print(f\"Execution time: {loaded_results['execution_time']}\")\n",
    "\n",
    "# Access other components as needed:\n",
    "# loaded_results['mu_L']\n",
    "# loaded_results['Sigma_L'] \n",
    "# loaded_results['mu_H']\n",
    "# loaded_results['Sigma_H']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1b240",
   "metadata": {},
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d87692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'lambda_param': 1.0}\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"synth1_erica_results\"\n",
    "result_files = [f for f in os.listdir(results_dir) if f.startswith('result_') and f.endswith('.pkl')]\n",
    "\n",
    "# Load all results into a list\n",
    "all_results = []\n",
    "for filename in result_files:\n",
    "    with open(os.path.join(results_dir, filename), 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "        all_results.append(result)\n",
    "\n",
    "# Now you can compare results, find the best parameters, etc.\n",
    "# For example, if you stored objective values:\n",
    "best_result = min(all_results, key=lambda x: x.get('objective', float('inf')))\n",
    "print(f\"Best parameters: {best_result['parameters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22515356",
   "metadata": {},
   "source": [
    "### Find experiments with a specific parameter value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68727235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_experiments_by_param(results_dir, param_name, param_value):\n",
    "    # List all result files in the directory\n",
    "    result_files = [f for f in os.listdir(results_dir) if f.startswith('result_') and f.endswith('.pkl')]\n",
    "    \n",
    "    matching_results = []\n",
    "    \n",
    "    for filename in result_files:\n",
    "        with open(os.path.join(results_dir, filename), 'rb') as f:\n",
    "            result = pickle.load(f)\n",
    "            # Check if the parameter matches the desired value\n",
    "            if result['parameters'].get(param_name) == param_value:\n",
    "                matching_results.append({\n",
    "                    'filename': filename,\n",
    "                    'data': result\n",
    "                })\n",
    "    \n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869cfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "results_dir = \"synth1_erica_results\"\n",
    "lambda_param_value = 0.8  # The value you're looking for\n",
    "\n",
    "matching_experiments = find_experiments_by_param(results_dir, 'lambda_param', lambda_param_value)\n",
    "\n",
    "if matching_experiments:\n",
    "    print(f\"Found {len(matching_experiments)} experiments with lambda_param = {lambda_param_value}\")\n",
    "    for exp in matching_experiments:\n",
    "        print(f\"\\nFilename: {exp['filename']}\")\n",
    "        print(f\"Parameters: {exp['data']['parameters']}\")\n",
    "        print(f\"Execution time: {exp['data']['execution_time']}\")\n",
    "        # Print other relevant information as needed\n",
    "        # print(f\"T matrix:\\n{exp['data']['T']}\")\n",
    "        # print(f\"Objective value: {exp['data'].get('objective')}\")\n",
    "else:\n",
    "    print(f\"No experiments found with lambda_param = {lambda_param_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba3ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "398a4222",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dhl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Dhl)\n\u001b[1;32m      3\u001b[0m TDll \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xl \u001b[38;5;129;01min\u001b[39;00m Dll:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "Dhl = torch.from_numpy(Dhl)\n",
    "\n",
    "TDll = []\n",
    "for xl in Dll:\n",
    "    xl = torch.tensor(xl, dtype=torch.float32)\n",
    "\n",
    "    TDll.append(T @ xl)\n",
    "TDll = torch.stack(TDll)\n",
    "\n",
    "# Compute average distance\n",
    "# Method 1: Using MSE (Mean Squared Error)\n",
    "mse_distance = torch.mean(torch.sum((TDll - Dhl)**2, dim=1))\n",
    "\n",
    "# Method 2: Using L2/Euclidean distance\n",
    "l2_distance = torch.mean(torch.sqrt(torch.sum((TDll - Dhl)**2, dim=1)))\n",
    "\n",
    "# Method 3: Using L1/Manhattan distance\n",
    "l1_distance = torch.mean(torch.sum(torch.abs(TDll - Dhl), dim=1))\n",
    "\n",
    "print(f\"MSE Distance: {mse_distance.item()}\")\n",
    "print(f\"L2 Distance: {l2_distance.item()}\")\n",
    "print(f\"L1 Distance: {l1_distance.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
