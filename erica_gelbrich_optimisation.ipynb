{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac2c91",
   "metadata": {},
   "source": [
    "### Barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4631e7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-level barycenter Mean: [-0.00678588 -0.01069607 -0.00015191]\n",
      "Low-level barycenter Covariance: [[1.04033442 0.28379336 0.03931234]\n",
      " [0.28379336 2.0108559  0.21143766]\n",
      " [0.03931234 0.21143766 0.99009455]]\n",
      "\n",
      "High-level barycenter Mean: [ 0.0042843  -0.00863504]\n",
      "High-level barycenter Covariance: [[1.35385779 0.58226716]\n",
      " [0.58226716 0.97012678]]\n"
     ]
    }
   ],
   "source": [
    "L_matrices = []  # List of L_i matrices\n",
    "for iota in Ill:\n",
    "    L_matrices.append(LLmodels[iota].compute_mechanism())\n",
    "\n",
    "H_matrices = []  # List of H_i matrices\n",
    "for eta in Ihl:\n",
    "    H_matrices.append(HLmodels[eta].compute_mechanism())\n",
    "\n",
    "mu_bary_L, Sigma_bary_L = oput.compute_gauss_barycenter(L_matrices, mu_U_ll_hat, Sigma_U_ll_hat)\n",
    "mu_bary_H, Sigma_bary_H = oput.compute_gauss_barycenter(H_matrices, mu_U_hl_hat, Sigma_U_hl_hat)\n",
    "\n",
    "print(\"Low-level barycenter Mean:\", mu_bary_L)\n",
    "print(\"Low-level barycenter Covariance:\", Sigma_bary_L)\n",
    "print( )\n",
    "print(\"High-level barycenter Mean:\", mu_bary_H)\n",
    "print(\"High-level barycenter Covariance:\", Sigma_bary_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d98d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "V                 = oput.sample_projection(mu_U_ll_hat.shape[0], mu_U_hl_hat.shape[0], use_stiefel=False)\n",
    "mu_bary_L_proj    = V @ mu_bary_L\n",
    "Sigma_bary_L_proj = V @ Sigma_bary_L @ V.T\n",
    "\n",
    "monge, A = oput.monge_map(mu_bary_L_proj, Sigma_bary_L_proj, mu_bary_H, Sigma_bary_H)\n",
    "T        = V.T @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17cbdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32158873 0.29159566]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0.1, -0.2])  # Example point from the first Gaussian (l = 3)\n",
    "\n",
    "print(T.T @ x)\n",
    "# # Apply the Monge map to the point x\n",
    "# T_x = T_func(x_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7752b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c2a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e88da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2f8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6c38736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguity set construction: Based on epsilon and delta include distribution (as many as the num_envs) that\n",
    "# pass the \"gelbrich\" test.\n",
    "ll_moments = mut.sample_moments_U(mu_hat    = mu_U_ll_hat,\n",
    "                                  Sigma_hat = Sigma_U_ll_hat,\n",
    "                                  bound     = epsilon,\n",
    "                                  num_envs  = ll_num_envs)\n",
    "\n",
    "A_ll       = mut.sample_distros_Gelbrich(ll_moments) #Low-level: A_epsilon\n",
    "\n",
    "\n",
    "hl_moments = mut.sample_moments_U(mu_hat    = mu_U_hl_hat,\n",
    "                                  Sigma_hat = Sigma_U_hl_hat,\n",
    "                                  bound     = delta,\n",
    "                                  num_envs  = hl_num_envs)\n",
    "\n",
    "A_hl       = mut.sample_distros_Gelbrich(hl_moments) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd01dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "abstraction_errors             = {}\n",
    "abstraction_env_errors         = {}\n",
    "max_env_avg_interv_error_value = -np.inf\n",
    "max_env_avg_interv_error_key   = None\n",
    "distance_err                   = 'wass'\n",
    "\n",
    "for lenv in A_ll:\n",
    "\n",
    "    Dll_noise      = lenv.sample(num_llsamples)[0]\n",
    "    ll_environment = mut.get_exogenous_distribution(Dll_noise)\n",
    "\n",
    "    for henv in A_hl:\n",
    "        Dhl_noise      = henv.sample(num_hlsamples)[0]\n",
    "        hl_environment = mut.get_exogenous_distribution(Dhl_noise)\n",
    "\n",
    "        total_ui_error = 0\n",
    "        num_distros    = len(Ill)\n",
    "\n",
    "        n, m  = len(LLmodels[None].endogenous_vars), len(HLmodels[None].endogenous_vars)\n",
    "\n",
    "        T     = mut.sample_stoch_matrix(n, m)\n",
    "\n",
    "        for iota in Ill:\n",
    "            llcm   = LLmodels[iota]\n",
    "            hlcm   = HLmodels[omega[iota]]\n",
    "            llmech = llcm.compute_mechanism()\n",
    "            hlmech = hlcm.compute_mechanism()\n",
    "            error  = mut.ui_error_dist(distance_err, lenv, henv, llmech, hlmech, T)\n",
    "\n",
    "            total_ui_error += error\n",
    "\n",
    "        avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "        if avg_interv_error > max_env_avg_interv_error_value:\n",
    "            max_env_avg_interv_error_value = avg_interv_error\n",
    "            max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "        abstraction_errors[str(T)] = avg_interv_error\n",
    "        abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0657828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstraction: [[0.21761458 0.78238542]\n",
      " [0.75095088 0.24904912]\n",
      " [0.86664525 0.13335475]], Error: 1.080267499893765\n",
      "==============================================================================\n",
      "max LL mean vector = [[0.03909886 0.02270429 0.149256  ]]\n",
      "max LL covariance = [[[0.82603265 0.         0.        ]\n",
      "  [0.         2.03002425 0.        ]\n",
      "  [0.         0.         0.84745966]]]\n",
      "\n",
      "max HL mean vector = [[ 0.07752375 -0.03284999]]\n",
      "max HL covariance = [[[1.03175598 0.        ]\n",
      "  [0.         0.77764308]]]\n",
      "==============================================================================\n",
      "max environment, average interventional abstraction error = 1.080267499893765\n"
     ]
    }
   ],
   "source": [
    "max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "max_error = abstraction_errors[max_tau]\n",
    "\n",
    "print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "print('==============================================================================' )\n",
    "max_lenv = max_env_avg_interv_error_key[0]\n",
    "max_henv = max_env_avg_interv_error_key[1]\n",
    "\n",
    "print(f\"max LL mean vector = {max_lenv.means_}\")\n",
    "print(f\"max LL covariance = {max_lenv.covariances_}\")\n",
    "print( )\n",
    "\n",
    "print(f\"max HL mean vector = {max_henv.means_}\")\n",
    "print(f\"max HL covariance = {max_henv.covariances_}\")\n",
    "print('==============================================================================' )\n",
    "print(f\"max environment, average interventional abstraction error = {max_env_avg_interv_error_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc902b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.28018097, 0.06072652],\n",
       "       [0.        , 1.        , 0.21674035],\n",
       "       [0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLmodels[None].compute_mechanism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ec79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf53d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d921f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments     = ['synth1_gnd', 'little_lucas']\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    # Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "    epsilon         = params.radius[experiment][0]\n",
    "    ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "    delta           = params.radius[experiment][1]\n",
    "    hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "    # Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "    num_llsamples   = params.n_samples[experiment][0]\n",
    "    num_hlsamples   = params.n_samples[experiment][1]\n",
    "\n",
    "    Dll = mut.load_samples(experiment)[None][0] \n",
    "    Gll = mut.load_ll_model(experiment)[0]\n",
    "    Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "    Dhl = mut.load_samples(experiment)[None][1] \n",
    "    Ghl = mut.load_hl_model(experiment)[0]\n",
    "    Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "    omega = mut.load_omega_map(experiment)\n",
    "\n",
    "    ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "    hl_coeffs = mut.get_coefficients(Dhl, Ghl) \n",
    "    num_experiments = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    errors = []\n",
    "    for n in num_experiments:\n",
    "        run_opt(n)\n",
    "        plot_abstraction_error(m)\n",
    "        errors.append(abst_error(n))\n",
    "    avg_abst_error = np.mean(errors)\n",
    "    std_abst_error = np.std(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d56f7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of T: tensor([[-6.6168,  1.0648,  8.8707],\n",
      "        [ 2.5843, -4.8567, -1.3703]])\n"
     ]
    }
   ],
   "source": [
    "mu_L    = torch.from_numpy(mu_U_ll_hat)\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat)\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat)\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat)\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "num_intervs = len(Ill)\n",
    "# Define T as a tensor with requires_grad=True for automatic differentiation\n",
    "T = torch.randn(h, l, requires_grad=True)\n",
    "\n",
    "# Compute the objective function as the expectation over samples\n",
    "objective = 0\n",
    "for i in range(num_intervs):\n",
    "    L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "    H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism())\n",
    "    \n",
    "    # 1st term: || T (L_i * mu_L) - (H_i * mu_H) ||_2^2\n",
    "    # Fix matrix-vector multiplication and norm calculation\n",
    "    L_i_mu_L = L_i @ mu_L  # Result: (m,)\n",
    "    H_i_mu_H = H_i @ mu_H  # Result: (n,)\n",
    "    \n",
    "    term1 = torch.norm(T.float() @ L_i_mu_L.float() - H_i_mu_H.float())**2  # Now this is dimensionally consistent\n",
    "   \n",
    "    # 2nd term: Tr(T L_i Sigma_L L_i^T T^T)\n",
    "    term2 = torch.trace(T.float() @ L_i.float() @ Sigma_L.float() @ L_i.T.float() @ T.T.float())\n",
    "    \n",
    "    # 3rd term: Tr(H_i Sigma_H H_i^T)\n",
    "    term3 = torch.trace(H_i.float() @ Sigma_H.float() @ H_i.T.float())\n",
    "    \n",
    "    # Ensure positive-definiteness for Cholesky decomposition\n",
    "    L_i_Sigma_L = T.float() @ L_i.float() @ Sigma_L.float() @ L_i.T.float() @ T.T.float()\n",
    "    H_i_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "    #L_i_Sigma_L = L_i_Sigma_L + torch.eye(L_i_Sigma_L.shape[0]) * 1e-6\n",
    "    #H_i_Sigma_H = H_i_Sigma_H + torch.eye(H_i_Sigma_H.shape[0]) * 1e-6\n",
    "\n",
    "    # 4th term: -2 * || (T L_i Sigma_L L_i^T T^T)^(1/2) * (H_i Sigma_H H_i^T)^(1/2) ||_*\n",
    "    # Whether you compute the matrix square root via torch.sqrt() or Cholesky decomposition, \n",
    "    # the singular values of the resulting matrix product will be the same.\n",
    "    term4 = -2 * torch.norm(torch.linalg.cholesky(L_i_Sigma_L) @ torch.linalg.cholesky(H_i_Sigma_H), 'nuc')\n",
    "    #term4 = -2 * torch.norm(torch.sqrt(L_i_Sigma_L) @ torch.sqrt(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "    \n",
    "    # Sum up terms\n",
    "    objective += term1 + term2 + term3 + term4\n",
    "\n",
    "# Average the objective over all interventions\n",
    "objective /= num_intervs\n",
    "\n",
    "# Compute gradients (subgradients for nuclear norm)\n",
    "objective.backward()\n",
    "\n",
    "# Get the gradient of T\n",
    "grad_T = T.grad\n",
    "\n",
    "# Print the gradient\n",
    "print(\"Gradient of T:\", grad_T)\n",
    "\n",
    "# Update T using an optimizer (e.g., stochastic gradient descent)\n",
    "optimizer = torch.optim.Adam([T], lr=0.01)\n",
    "optimizer = torch.optim.SGD([T], lr=0.01)\n",
    "\n",
    "# Perform one step of gradient descent\n",
    "optimizer.step()\n",
    "\n",
    "# Optionally, zero out gradients for the next step\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c76bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5502a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940896c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b2798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD STUFFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cb67167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_lagrangian(L, H, Q_vars, W_vars, mu_L, mu_H, Sigma_L, Sigma_H,\n",
    "                         lambda_eps, lambda_del, lambda_Q, lambda_W, rho_Q, rho_W):\n",
    "    loss = 0\n",
    "\n",
    "    # Loss components from the original objective\n",
    "    for i in range(N):\n",
    "        loss += cp.norm(L[i] @ mu_L - H[i] @ mu_H, 'fro')**2\n",
    "        loss += cp.trace(L[i] @ Sigma_L @ L[i].T)\n",
    "        loss += cp.trace(H[i] @ Sigma_H @ H[i].T)\n",
    "        loss -= 2 * cp.norm(Q_vars[i], 'fro') * cp.norm(W_vars[i], 'fro')\n",
    "\n",
    "    # To compute the expected value, we will average the loss\n",
    "    loss /= N  # Average over the number of samples\n",
    "\n",
    "    # Penalties for epsilon constraint\n",
    "    loss += lambda_eps * (epsilon**2 - cp.norm(mu_L - mu_L_hat)**2 - cp.norm(cp.sqrt(Sigma_L) - cp.sqrt(Sigma_L_hat))**2)\n",
    "\n",
    "    # Penalties for delta constraint\n",
    "    loss += lambda_del * (delta**2 - cp.norm(mu_H - mu_H_hat)**2 - cp.norm(cp.sqrt(Sigma_H) - cp.sqrt(Sigma_H_hat))**2)\n",
    "\n",
    "    # Lagrange multiplier terms for Q\n",
    "    for i in range(N):\n",
    "        loss += cp.sum(lambda_Q[i] * (Q_vars[i] - cp.sqrt(L[i] @ Sigma_L @ L[i].T)))\n",
    "\n",
    "    # Lagrange multiplier terms for W\n",
    "    for i in range(N):\n",
    "        loss += cp.sum(lambda_W[i] * (W_vars[i] - cp.sqrt(H[i] @ Sigma_H @ H[i].T)))\n",
    "\n",
    "    # Penalty terms for Q\n",
    "    loss += (rho_Q / 2) * sum(cp.norm(Q_vars[i] - cp.sqrt(L[i] @ Sigma_L @ L[i].T), 'fro')**2 for i in range(N))\n",
    "\n",
    "    # Penalty terms for W\n",
    "    loss += (rho_W / 2) * sum(cp.norm(W_vars[i] - cp.sqrt(H[i] @ Sigma_H @ H[i].T), 'fro')**2 for i in range(N))\n",
    "\n",
    "    return cp.Maximize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8d47564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized mu_L_hat: [0.90415869 0.34825547 0.51398949]\n",
      "Initialized Sigma_L_hat:\n",
      " [[1.15835009 1.14381192 1.08191875]\n",
      " [1.14381192 1.66689119 1.30431101]\n",
      " [1.08191875 1.30431101 1.16743324]]\n",
      "Initialized mu_H_hat: [0.45913576 0.98003258 0.49261809]\n",
      "Initialized Sigma_H_hat:\n",
      " [[0.56694418 0.13732236 0.29177477]\n",
      " [0.13732236 0.03876096 0.1114772 ]\n",
      " [0.29177477 0.1114772  0.45306785]]\n",
      "Initialized Lagrange multipliers for Q:\n",
      " [array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]])]\n",
      "Initialized Lagrange multipliers for W:\n",
      " [array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize parameters\n",
    "N = 5  # Number of samples\n",
    "dim_mu_L = 3  # Dimension of mu_L\n",
    "dim_mu_H = 3  # Dimension of mu_H\n",
    "dim_Sigma = 3  # Dimension for covariance matrices\n",
    "\n",
    "# Step 2: Generate example L and H matrices\n",
    "L = [np.random.rand(4, dim_mu_L) for _ in range(N)]  # Example L_i matrices\n",
    "H = [np.random.rand(4, dim_mu_H) for _ in range(N)]  # Example H_i matrices\n",
    "\n",
    "# Step 3: Define initial estimates for mu_L, Sigma_L, mu_H, and Sigma_H\n",
    "mu_L_hat = np.random.rand(dim_mu_L)  # Initial estimate for mu_L\n",
    "Sigma_L_hat = np.random.rand(dim_Sigma, dim_Sigma)\n",
    "Sigma_L_hat = Sigma_L_hat @ Sigma_L_hat.T  # Make Sigma_L_hat symmetric and positive semi-definite\n",
    "\n",
    "mu_H_hat = np.random.rand(dim_mu_H)  # Initial estimate for mu_H\n",
    "Sigma_H_hat = np.random.rand(dim_Sigma, dim_Sigma)\n",
    "Sigma_H_hat = Sigma_H_hat @ Sigma_H_hat.T  # Make Sigma_H_hat symmetric and positive semi-definite\n",
    "\n",
    "# Step 4: Define constraint parameters\n",
    "epsilon = 0.1  # Parameter for epsilon constraint\n",
    "delta = 0.1    # Parameter for delta constraint\n",
    "alpha = 0.1    # Parameter for proximal operator\n",
    "\n",
    "# Step 5: Initialize Lagrange multipliers\n",
    "lambda_eps = 0.0  # Lagrange multiplier for epsilon constraint\n",
    "lambda_del = 0.0  # Lagrange multiplier for delta constraint\n",
    "lambda_Q = [np.zeros((4, 4)) for _ in range(N)]  # Lagrange multipliers for Q_i\n",
    "lambda_W = [np.zeros((4, 4)) for _ in range(N)]  # Lagrange multipliers for W_i\n",
    "\n",
    "# Step 6: Define penalty parameters\n",
    "rho_Q = 1.0  # Penalty parameter for Q constraints\n",
    "rho_W = 1.0  # Penalty parameter for W constraints\n",
    "\n",
    "# Print initialized values\n",
    "print(\"Initialized mu_L_hat:\", mu_L_hat)\n",
    "print(\"Initialized Sigma_L_hat:\\n\", Sigma_L_hat)\n",
    "print(\"Initialized mu_H_hat:\", mu_H_hat)\n",
    "print(\"Initialized Sigma_H_hat:\\n\", Sigma_H_hat)\n",
    "print(\"Initialized Lagrange multipliers for Q:\\n\", lambda_Q)\n",
    "print(\"Initialized Lagrange multipliers for W:\\n\", lambda_W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c6ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Lagrangian result: maximize (power(Pnorm(reshape([-0.68077198 -0.41429305  0.04588054 -0.90939219], (4,), F), 2), 2.0) + trace([[1.75 2.73 0.95 0.90]\n",
      " [2.73 4.26 1.51 1.40]\n",
      " [0.95 1.51 0.56 0.48]\n",
      " [0.90 1.40 0.48 0.47]]) + trace([[1.33 1.10 0.28 0.98]\n",
      " [1.10 0.97 0.27 0.78]\n",
      " [0.28 0.27 0.08 0.19]\n",
      " [0.98 0.78 0.19 0.74]]) + -2.0 @ Pnorm(reshape(var197, (16,), F), 2) @ Pnorm(reshape(var202, (16,), F), 2) + power(Pnorm(reshape([-0.40805684 -0.38390375  0.18957148 -0.31252703], (4,), F), 2), 2.0) + trace([[3.00 3.78 2.80 3.44]\n",
      " [3.78 4.80 3.54 4.39]\n",
      " [2.80 3.54 2.62 3.19]\n",
      " [3.44 4.39 3.19 4.25]]) + trace([[1.11 1.04 0.37 1.40]\n",
      " [1.04 1.04 0.39 1.34]\n",
      " [0.37 0.39 0.15 0.49]\n",
      " [1.40 1.34 0.49 1.78]]) + -2.0 @ Pnorm(reshape(var198, (16,), F), 2) @ Pnorm(reshape(var203, (16,), F), 2) + power(Pnorm(reshape([-0.76014819 -0.18013291  0.57865513  0.27663614], (4,), F), 2), 2.0) + trace([[3.01 2.80 3.90 3.30]\n",
      " [2.80 2.97 3.96 3.32]\n",
      " [3.90 3.96 5.35 4.49]\n",
      " [3.30 3.32 4.49 3.78]]) + trace([[1.54 1.20 1.09 1.05]\n",
      " [1.20 0.97 0.81 0.81]\n",
      " [1.09 0.81 0.82 0.76]\n",
      " [1.05 0.81 0.76 0.72]]) + -2.0 @ Pnorm(reshape(var199, (16,), F), 2) @ Pnorm(reshape(var204, (16,), F), 2) + power(Pnorm(reshape([ 0.1747486  -0.15611224 -0.57534899 -0.80226251], (4,), F), 2), 2.0) + trace([[3.12 4.27 2.70 2.02]\n",
      " [4.27 5.88 3.70 2.78]\n",
      " [2.70 3.70 2.34 1.75]\n",
      " [2.02 2.78 1.75 1.32]]) + trace([[0.34 0.56 0.47 0.45]\n",
      " [0.56 1.02 0.85 0.80]\n",
      " [0.47 0.85 0.71 0.67]\n",
      " [0.45 0.80 0.67 0.63]]) + -2.0 @ Pnorm(reshape(var200, (16,), F), 2) @ Pnorm(reshape(var205, (16,), F), 2) + power(Pnorm(reshape([0.59388453 0.33064015 0.66564223 0.01627008], (4,), F), 2), 2.0) + trace([[3.05 4.24 3.53 3.72]\n",
      " [4.24 5.97 5.00 5.28]\n",
      " [3.53 5.00 4.25 4.51]\n",
      " [3.72 5.28 4.51 4.79]]) + trace([[0.33 0.34 0.18 0.39]\n",
      " [0.34 0.49 0.20 0.51]\n",
      " [0.18 0.20 0.10 0.22]\n",
      " [0.39 0.51 0.22 0.54]]) + -2.0 @ Pnorm(reshape(var201, (16,), F), 2) @ Pnorm(reshape(var206, (16,), F), 2)) / 5.0 + 0.0 @ (0.010000000000000002 + -power(Pnorm([0. 0. 0.], 2), 2.0) + -power(sigma_max(power([[1.16 1.14 1.08]\n",
      " [1.14 1.67 1.30]\n",
      " [1.08 1.30 1.17]], 0.5) + -power([[1.16 1.14 1.08]\n",
      " [1.14 1.67 1.30]\n",
      " [1.08 1.30 1.17]], 0.5)), 2.0)) + 0.0 @ (0.010000000000000002 + -power(Pnorm([0. 0. 0.], 2), 2.0) + -power(sigma_max(power([[0.57 0.14 0.29]\n",
      " [0.14 0.04 0.11]\n",
      " [0.29 0.11 0.45]], 0.5) + -power([[0.57 0.14 0.29]\n",
      " [0.14 0.04 0.11]\n",
      " [0.29 0.11 0.45]], 0.5)), 2.0)) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var197 + -power([[1.75 2.73 0.95 0.90]\n",
      " [2.73 4.26 1.51 1.40]\n",
      " [0.95 1.51 0.56 0.48]\n",
      " [0.90 1.40 0.48 0.47]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var198 + -power([[3.00 3.78 2.80 3.44]\n",
      " [3.78 4.80 3.54 4.39]\n",
      " [2.80 3.54 2.62 3.19]\n",
      " [3.44 4.39 3.19 4.25]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var199 + -power([[3.01 2.80 3.90 3.30]\n",
      " [2.80 2.97 3.96 3.32]\n",
      " [3.90 3.96 5.35 4.49]\n",
      " [3.30 3.32 4.49 3.78]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var200 + -power([[3.12 4.27 2.70 2.02]\n",
      " [4.27 5.88 3.70 2.78]\n",
      " [2.70 3.70 2.34 1.75]\n",
      " [2.02 2.78 1.75 1.32]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var201 + -power([[3.05 4.24 3.53 3.72]\n",
      " [4.24 5.97 5.00 5.28]\n",
      " [3.53 5.00 4.25 4.51]\n",
      " [3.72 5.28 4.51 4.79]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var202 + -power([[1.33 1.10 0.28 0.98]\n",
      " [1.10 0.97 0.27 0.78]\n",
      " [0.28 0.27 0.08 0.19]\n",
      " [0.98 0.78 0.19 0.74]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var203 + -power([[1.11 1.04 0.37 1.40]\n",
      " [1.04 1.04 0.39 1.34]\n",
      " [0.37 0.39 0.15 0.49]\n",
      " [1.40 1.34 0.49 1.78]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var204 + -power([[1.54 1.20 1.09 1.05]\n",
      " [1.20 0.97 0.81 0.81]\n",
      " [1.09 0.81 0.82 0.76]\n",
      " [1.05 0.81 0.76 0.72]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var205 + -power([[0.34 0.56 0.47 0.45]\n",
      " [0.56 1.02 0.85 0.80]\n",
      " [0.47 0.85 0.71 0.67]\n",
      " [0.45 0.80 0.67 0.63]], 0.5)), None, False) + Sum([[0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]\n",
      " [0.00 0.00 0.00 0.00]] @ (var206 + -power([[0.33 0.34 0.18 0.39]\n",
      " [0.34 0.49 0.20 0.51]\n",
      " [0.18 0.20 0.10 0.22]\n",
      " [0.39 0.51 0.22 0.54]], 0.5)), None, False) + 0.5 @ (power(Pnorm(reshape(var197 + -power([[1.75 2.73 0.95 0.90]\n",
      " [2.73 4.26 1.51 1.40]\n",
      " [0.95 1.51 0.56 0.48]\n",
      " [0.90 1.40 0.48 0.47]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var198 + -power([[3.00 3.78 2.80 3.44]\n",
      " [3.78 4.80 3.54 4.39]\n",
      " [2.80 3.54 2.62 3.19]\n",
      " [3.44 4.39 3.19 4.25]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var199 + -power([[3.01 2.80 3.90 3.30]\n",
      " [2.80 2.97 3.96 3.32]\n",
      " [3.90 3.96 5.35 4.49]\n",
      " [3.30 3.32 4.49 3.78]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var200 + -power([[3.12 4.27 2.70 2.02]\n",
      " [4.27 5.88 3.70 2.78]\n",
      " [2.70 3.70 2.34 1.75]\n",
      " [2.02 2.78 1.75 1.32]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var201 + -power([[3.05 4.24 3.53 3.72]\n",
      " [4.24 5.97 5.00 5.28]\n",
      " [3.53 5.00 4.25 4.51]\n",
      " [3.72 5.28 4.51 4.79]], 0.5), (16,), F), 2), 2.0)) + 0.5 @ (power(Pnorm(reshape(var202 + -power([[1.33 1.10 0.28 0.98]\n",
      " [1.10 0.97 0.27 0.78]\n",
      " [0.28 0.27 0.08 0.19]\n",
      " [0.98 0.78 0.19 0.74]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var203 + -power([[1.11 1.04 0.37 1.40]\n",
      " [1.04 1.04 0.39 1.34]\n",
      " [0.37 0.39 0.15 0.49]\n",
      " [1.40 1.34 0.49 1.78]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var204 + -power([[1.54 1.20 1.09 1.05]\n",
      " [1.20 0.97 0.81 0.81]\n",
      " [1.09 0.81 0.82 0.76]\n",
      " [1.05 0.81 0.76 0.72]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var205 + -power([[0.34 0.56 0.47 0.45]\n",
      " [0.56 1.02 0.85 0.80]\n",
      " [0.47 0.85 0.71 0.67]\n",
      " [0.45 0.80 0.67 0.63]], 0.5), (16,), F), 2), 2.0) + power(Pnorm(reshape(var206 + -power([[0.33 0.34 0.18 0.39]\n",
      " [0.34 0.49 0.20 0.51]\n",
      " [0.18 0.20 0.10 0.22]\n",
      " [0.39 0.51 0.22 0.54]], 0.5), (16,), F), 2), 2.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 11 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 12 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 13 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 14 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 15 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 16 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 17 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 18 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 19 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/giofelekis/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/expressions/expression.py:650: UserWarning: \n",
      "This use of ``*`` has resulted in matrix multiplication.\n",
      "Using ``*`` for matrix multiplication has been deprecated since CVXPY 1.1.\n",
      "    Use ``*`` for matrix-scalar and vector-scalar multiplication.\n",
      "    Use ``@`` for matrix-matrix and matrix-vector multiplication.\n",
      "    Use ``multiply`` for elementwise multiplication.\n",
      "This code path has been hit 20 times so far.\n",
      "\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Compute the augmented Lagrangian\n",
    "# This step assumes you have defined your augmented_lagrangian function already.\n",
    "\n",
    "result = augmented_lagrangian(L, H, Q_vars, W_vars, mu_L_hat, mu_H_hat, Sigma_L_hat, Sigma_H_hat,\n",
    "                              lambda_eps, lambda_del, lambda_Q, lambda_W, rho_Q, rho_W)\n",
    "\n",
    "# Print the result\n",
    "print(\"Augmented Lagrangian result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e7bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vars = [cp.Variable((4, 4)) for _ in range(N)]  # Adjust size as necessary\n",
    "W_vars = [cp.Variable((4, 4)) for _ in range(N)]  # Adjust size as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "055e9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmented_lagrangian computes the augmented lagrangian for the optimization problem\n",
    "def augmented_lagrangian(LLmodels, HLmodels, Q_vars, W_vars, mu_L, mu_H, Sigma_L, Sigma_H,\n",
    "                     mu_U_ll_hat, mu_U_hl_hat, Sigma_U_ll_hat, Sigma_U_hl_hat,\n",
    "                     lambda_eps, lambda_del, lambda_Q, lambda_W, \n",
    "                     rho_Q, rho_W):\n",
    "    #LLmodels are the ll models for every iota\n",
    "    #HLmodels are the hl models for every omega_iota\n",
    "    #mu_U_ll_hat is the mean vector of the low-level model\n",
    "    #mu_U_hl_hat is the mean vector of the high-level model\n",
    "    #Sigma_U_ll_hat is the covariance matrix of the low-level model\n",
    "    #Sigma_U_hl_hat is the covariance matrix of the high-level model\n",
    "\n",
    "    #mu_L is the mean vector of the low-level model\n",
    "    #mu_H is the mean vector of the high-level model\n",
    "    #Sigma_L is the covariance matrix of the low-level model\n",
    "    #Sigma_H is the covariance matrix of the high-level model\n",
    "\n",
    "    #lambda_eps is the Lagrange multiplier for the epsilon constraint (ll)\n",
    "    #lambda_del is the Lagrange multiplier for the delta constraint (hl)\n",
    "\n",
    "    #lambda_Q is the Lagrange multiplier for the Q_i constraints for every i\n",
    "    #lambda_W is the Lagrange multiplier for the W_i constraints for every i\n",
    "\n",
    "    #rho_Q is the penalty parameter for the Q_i constraints: same for all i\n",
    "    #rho_W is the penalty parameter for the W_i constraints: same for all i\n",
    "\n",
    "    #Q_vars are the Q_i variables for every i\n",
    "    #W_vars are the W_i variables for every i\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Loss components from the original objective\n",
    "    for iota in Ill:\n",
    "\n",
    "        llcm = LLmodels[iota]\n",
    "        hlcm = HLmodels[omega[iota]]\n",
    "        llmech = llcm.compute_mechanism() \n",
    "        hlmech = hlcm.compute_mechanism()\n",
    "\n",
    "        loss += cp.norm(llmech @ mu_L - hlmech @ mu_H, 'fro')**2\n",
    "        loss += cp.trace(llmech @ Sigma_L @ llmech.T)\n",
    "        loss += cp.trace(hlmech @ Sigma_H @ hlmech.T)\n",
    "        loss -= 2 * cp.norm(Q_vars[iota], 'fro') * cp.norm(W_vars[iota], 'fro')\n",
    "\n",
    "    # To compute the expected value, we will average the loss\n",
    "    loss /= len(Ill)  # Average over the number of samples\n",
    "\n",
    "    #Penalties for epsilon constraint\n",
    "    loss += lambda_eps * (epsilon**2 - cp.norm(mu_L - mu_U_ll_hat)**2 - cp.norm(cp.sqrt(Sigma_L) - cp.sqrt(Sigma_U_ll_hat))**2)\n",
    "\n",
    "    # Penalties for delta constraint\n",
    "    loss += lambda_del * (delta**2 - cp.norm(mu_H - mu_U_hl_hat)**2 - cp.norm(cp.sqrt(Sigma_H) - cp.sqrt(Sigma_U_hl_hat))**2)\n",
    "\n",
    "    # Lagrange multiplier terms for Q and W\n",
    "    for iota in Ill:\n",
    "        llcm = LLmodels[iota]\n",
    "        hlcm = HLmodels[omega[iota]]\n",
    "        llmech = llcm.compute_mechanism() \n",
    "        hlmech = hlcm.compute_mechanism()\n",
    "        loss += cp.sum(lambda_Q[iota] * (Q_vars[iota] - cp.sqrt(llmech @ Sigma_L @ llmech.T)))\n",
    "        loss += cp.sum(lambda_W[iota] * (W_vars[iota] - cp.sqrt(hlmech @ Sigma_H @ hlmech.T)))\n",
    "\n",
    "    # Penalty terms for Q and W\n",
    "    for iota in Ill:\n",
    "        llcm = LLmodels[iota]\n",
    "        hlcm = HLmodels[omega[iota]]\n",
    "        llmech = llcm.compute_mechanism() \n",
    "        hlmech = hlcm.compute_mechanism()\n",
    "        loss += (rho_Q / 2) * cp.norm(Q_vars[iota] - cp.sqrt(llmech @ Sigma_L @ llmech.T), 'fro')**2\n",
    "        loss += (rho_W / 2) * cp.norm(W_vars[iota] - cp.sqrt(hlmech @ Sigma_H @ hlmech.T), 'fro')**2 \n",
    "\n",
    "    return loss\n",
    "    # return cp.Maximize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7668bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "check the updates and add proper variable names\n",
    "use cvxpy to solve the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d27b938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the update functions\n",
    "def update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat):\n",
    "    N = len(LLmodels)\n",
    "    E_LL = sum(LL_i.T @ LL_i for LL_i in LLmodels) / N\n",
    "    E_LH_mu_H = sum(LL_i.T @ HL_i @ mu_H for LL_i, HL_i in zip(LLmodels, HLmodels)) / N\n",
    "    reg_term = (lambda_eps / 2) * np.eye(mu_U_ll_hat.shape[0])\n",
    "    mu_L_expr = np.linalg.inv(E_LL + reg_term) @ (E_LH_mu_H + (lambda_eps / 2) * mu_U_ll_hat)\n",
    "    return mu_L_expr\n",
    "\n",
    "def update_mu_H(HLmodels, LLmodels, mu_L, lambda_delta, mu_U_hl_hat):\n",
    "    N = len(HLmodels)\n",
    "    E_HH = sum(HL_iota.T @ HL_iota for HL_iota in HLmodels) / N\n",
    "    E_HL_mu_L = sum(HL_iota.T @ LL_iota @ mu_L for LL_iota, HL_iota in zip(LLmodels, HLmodels)) / N\n",
    "    reg_term = (lambda_delta / 2) * np.eye(mu_U_hl_hat.shape[0])\n",
    "    mu_H_expr = np.linalg.inv(E_HH + reg_term) @ (E_HL_mu_L + (lambda_delta / 2) * mu_U_hl_hat)\n",
    "    return mu_H_expr\n",
    "\n",
    "def update_Sigma_L(LLmodels, lambda_eps, Sigma_U_ll_hat, lambda_Q, Sigma_L_k):\n",
    "    # Ensure that lambda_eps is a scalar or a single value\n",
    "    if np.isscalar(lambda_eps) or lambda_eps.size == 1:\n",
    "        lambda_eps_value = lambda_eps if np.isscalar(lambda_eps) else lambda_eps.item()\n",
    "        if lambda_eps_value > 0:\n",
    "            # Check shapes of LLmodels and lambda_Q\n",
    "            LL_sum = np.sum([LL_i @ LL_i.T for LL_i in LLmodels], axis=0)  # Should result in (4, 4) if each LL_i is (4, 3)\n",
    "            lambda_Q_sum = np.sum(lambda_Q, axis=0)  # Ensure this matches the expected shape\n",
    "            \n",
    "            # Make sure lambda_Q_sum is shaped correctly for the operation\n",
    "            if lambda_Q_sum.shape != LL_sum.shape:\n",
    "                raise ValueError(f\"Shape mismatch: LL_sum shape {LL_sum.shape} and lambda_Q_sum shape {lambda_Q_sum.shape}\")\n",
    "\n",
    "            Sigma_L_updated = np.linalg.inv(LL_sum + lambda_eps_value * np.eye(Sigma_L_k.shape[0])) @ (\n",
    "                lambda_eps_value * Sigma_U_ll_hat + lambda_Q_sum\n",
    "            )\n",
    "        else:\n",
    "            Sigma_L_updated = np.zeros_like(Sigma_L_k)  # Handle the case where the condition is not met\n",
    "    else:\n",
    "        raise ValueError(\"lambda_eps should be a scalar or a single value\")\n",
    "\n",
    "    return Sigma_L_updated\n",
    "\n",
    "def update_Sigma_H(HLmodels, lambda_delta, Sigma_U_hl_hat, lambda_W, Sigma_H_k):\n",
    "    if np.isscalar(lambda_delta) or lambda_delta.size == 1:\n",
    "        lambda_delta_value = lambda_delta if np.isscalar(lambda_delta) else lambda_delta.item()\n",
    "        if lambda_delta_value > 0:\n",
    "            Sigma_H_updated = np.linalg.inv(np.sum(HLmodels, axis=0) + lambda_delta_value * np.eye(Sigma_H_k.shape[0])) @ (lambda_delta_value * Sigma_U_hl_hat + np.sum(lambda_W, axis=0))\n",
    "        else:\n",
    "            Sigma_H_updated = np.zeros_like(Sigma_H_k)\n",
    "    else:\n",
    "        raise ValueError(\"lambda_delta should be a scalar or a single value\")\n",
    "    return Sigma_H_updated\n",
    "\n",
    "def update_Q_i(Q_i_k, lambda_Q_i, rho_Q, alpha):\n",
    "    norm_Q = np.linalg.norm(Q_i_k, 'fro')\n",
    "    if norm_Q > alpha:\n",
    "        Q_i_updated = (1 - (alpha / norm_Q)) * (Q_i_k - (1 / rho_Q) * lambda_Q_i)\n",
    "    else:\n",
    "        Q_i_updated = np.zeros_like(Q_i_k)\n",
    "    return Q_i_updated\n",
    "\n",
    "def update_W_i(W_i_k, lambda_W_i, rho_W, alpha):\n",
    "    norm_W = np.linalg.norm(W_i_k, 'fro')\n",
    "    if norm_W > alpha:\n",
    "        W_i_updated = (1 - (alpha / norm_W)) * (W_i_k - (1 / rho_W) * lambda_W_i)\n",
    "    else:\n",
    "        W_i_updated = np.zeros_like(W_i_k)\n",
    "    return W_i_updated\n",
    "\n",
    "def update_lambda_Q(lambda_Q_i_k, rho_Q, Q_i_k_plus_1, LL_i, Sigma_L_k_plus_1):\n",
    "    term = np.sqrt(LL_i @ Sigma_L_k_plus_1 @ LL_i.T)\n",
    "    lambda_Q_i_updated = lambda_Q_i_k + rho_Q * (Q_i_k_plus_1 - term)\n",
    "    return lambda_Q_i_updated\n",
    "\n",
    "def update_lambda_W(lambda_W_i_k, rho_W, W_i_k_plus_1, HL_i, Sigma_H_k_plus_1):\n",
    "    term = np.sqrt(HL_i @ Sigma_H_k_plus_1 @ HL_i.T)\n",
    "    lambda_W_i_updated = lambda_W_i_k + rho_W * (W_i_k_plus_1 - term)\n",
    "    return lambda_W_i_updated\n",
    "\n",
    "def update_lambda_epsilon(lambda_eps_k, mu_L_k_plus_1, mu_U_ll_hat, Sigma_L_k_plus_1, Sigma_U_ll_hat, epsilon):\n",
    "    lambda_eps_updated = lambda_eps_k + (epsilon**2 - np.linalg.norm(mu_L_k_plus_1 - mu_U_ll_hat)**2 - np.linalg.norm(np.sqrt(Sigma_L_k_plus_1) - np.sqrt(Sigma_U_ll_hat))**2)\n",
    "    return lambda_eps_updated\n",
    "\n",
    "def update_lambda_delta(lambda_del_k, mu_H_k_plus_1, mu_U_hl_hat, Sigma_H_k_plus_1, Sigma_U_hl_hat, delta):\n",
    "    lambda_del_updated = lambda_del_k + (delta**2 - np.linalg.norm(mu_H_k_plus_1 - mu_U_hl_hat)**2 - np.linalg.norm(np.sqrt(Sigma_H_k_plus_1) - np.sqrt(Sigma_U_hl_hat))**2)\n",
    "    return lambda_del_updated\n",
    "\n",
    "def compute_monge_map(mu_a, Sigma_a, mu_b, Sigma_b):\n",
    "    # Check dimensions\n",
    "    if mu_a.shape[0] != 3 or mu_b.shape[0] != 2:\n",
    "        raise ValueError(\"Mean vectors must be of shape (3,) for Gaussian A and (2,) for Gaussian B.\")\n",
    "    \n",
    "    if Sigma_a.shape != (3, 3) or Sigma_b.shape != (2, 2):\n",
    "        raise ValueError(\"Covariance matrices must be of shape (3, 3) for Gaussian A and (2, 2) for Gaussian B.\")\n",
    "\n",
    "    # Compute the square root of the covariance matrices\n",
    "    Sigma_a_sqrt = np.linalg.cholesky(Sigma_a)\n",
    "\n",
    "    # Use a 2D projection from 3D to 2D, we need a method to match the dimensions\n",
    "    # Here we assume a simple linear map for projection\n",
    "    A = Sigma_a_sqrt[:2, :2]  # Take the first two rows/columns for the projection\n",
    "\n",
    "    # Compute the transformation matrix\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    B_inv = np.linalg.inv(Sigma_b)\n",
    "    \n",
    "    # Calculate the optimal transformation using the covariance matrices\n",
    "    transformation_matrix = A_inv @ Sigma_b @ A_inv\n",
    "\n",
    "    # Compute the inverse square root of the transformation matrix\n",
    "    transformation_matrix_sqrt_inv = np.linalg.inv(np.linalg.cholesky(transformation_matrix))\n",
    "\n",
    "    def T(x):\n",
    "        # Ensure the input x is in the expected shape (3,)\n",
    "        if x.shape[0] != 3:\n",
    "            raise ValueError(\"Input x must be a 3D vector (shape: (3,)).\")\n",
    "        # Map the 3D vector to 2D\n",
    "        return transformation_matrix_sqrt_inv @ (x - mu_a)[:2] + mu_b\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ddc20a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final mu_L: [0.55675283 0.06727336 0.64644982]\n",
      "Final Sigma_L:\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Final mu_H: [0.38240578 0.64187901]\n",
      "Final Sigma_H:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "Mapped point: [1.32565294 3.07460566]\n"
     ]
    }
   ],
   "source": [
    "# Optimization Parameters Initialization\n",
    "num_iterations = 100000  # Number of iterations for optimization\n",
    "rho_Q = 1.0  # Penalty parameter for Q updates\n",
    "rho_W = 1.0  # Penalty parameter for W updates\n",
    "alpha = 0.5  # Proximal parameter\n",
    "epsilon = 0.1  # Epsilon constraint value\n",
    "delta = 0.1  # Delta constraint value\n",
    "\n",
    "# Example initialization (replace these with actual data)\n",
    "LLmodels = [np.random.rand(4, 3) for _ in range(5)]  # Example L_i matrices\n",
    "HLmodels = [np.random.rand(4, 2) for _ in range(5)]  # Example H_i matrices\n",
    "mu_U_ll_hat = np.random.rand(3)  # Target mean for mu_L\n",
    "mu_U_hl_hat = np.random.rand(2)  # Target mean for mu_H\n",
    "Sigma_U_ll_hat = np.eye(3)  # Target covariance for Sigma_L\n",
    "Sigma_U_hl_hat = np.eye(2)  # Target covariance for Sigma_H\n",
    "\n",
    "# Initialize variables\n",
    "mu_L = np.random.rand(3)  # Initial estimate for mu_L\n",
    "mu_H = np.random.rand(2)  # Initial estimate for mu_H\n",
    "Sigma_L = np.eye(3)  # Initial estimate for Sigma_L\n",
    "Sigma_H = np.eye(2)  # Initial estimate for Sigma_H\n",
    "Q_vars = [np.random.rand(4, 4) for _ in range(5)]  # Initial Q matrices\n",
    "W_vars = [np.random.rand(4, 4) for _ in range(5)]  # Initial W matrices\n",
    "lambda_Q = [np.zeros((4, 4)) for _ in range(5)]  # Initial lambda_Q\n",
    "lambda_W = [np.zeros((4, 4)) for _ in range(5)]  # Initial lambda_W\n",
    "lambda_eps = 0.0  # Initial lambda_eps\n",
    "lambda_del = 0.0  # Initial lambda_delta\n",
    "\n",
    "# Optimization Loop\n",
    "for k in range(num_iterations):\n",
    "    # Update mu_L and mu_H\n",
    "    mu_L = update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat)\n",
    "    mu_H = update_mu_H(HLmodels, LLmodels, mu_L, lambda_delta, mu_U_hl_hat)\n",
    "\n",
    "    # Update Sigma_L and Sigma_H\n",
    "    Sigma_L = Sigma_U_ll_hat #update_Sigma_L(LLmodels, rho_Q, Sigma_U_ll_hat, lambda_Q, Sigma_L)\n",
    "    Sigma_H = Sigma_U_hl_hat #update_Sigma_H(HLmodels, rho_W, Sigma_U_hl_hat, lambda_W, Sigma_H)\n",
    "\n",
    "    # Update Q and W\n",
    "    for iota in range(len(Q_vars)):\n",
    "        Q_vars[iota] = update_Q_i(Q_vars[iota], lambda_Q[iota], rho_Q, alpha)\n",
    "        W_vars[iota] = update_W_i(W_vars[iota], lambda_W[iota], rho_W, alpha)\n",
    "\n",
    "    # Update lambda multipliers\n",
    "    for iota in range(len(Q_vars)):\n",
    "        lambda_Q[iota] = update_lambda_Q(lambda_Q[iota], rho_Q, Q_vars[iota], LLmodels[iota], Sigma_L)\n",
    "        lambda_W[iota] = update_lambda_W(lambda_W[iota], rho_W, W_vars[iota], HLmodels[iota], Sigma_H)\n",
    "\n",
    "    # Update lambda_eps and lambda_delta\n",
    "    lambda_eps = update_lambda_epsilon(lambda_eps, mu_L, mu_U_ll_hat, Sigma_L, Sigma_U_ll_hat, epsilon)\n",
    "    lambda_del = update_lambda_delta(lambda_del, mu_H, mu_U_hl_hat, Sigma_H, Sigma_U_hl_hat, delta)\n",
    "\n",
    "# Print the final results\n",
    "print(\"Final mu_L:\", mu_L)\n",
    "print(\"Final Sigma_L:\\n\", Sigma_L)\n",
    "print(\"Final mu_H:\", mu_H)\n",
    "print(\"Final Sigma_H:\\n\", Sigma_H)\n",
    "print( )\n",
    "tau = compute_monge_map(mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "x_sample = np.array([1.5, 2.5, 3.5])\n",
    "mapped_point = tau(x_sample)\n",
    "\n",
    "print(\"Mapped point:\", mapped_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb4ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d88fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "61ba49cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x3 and 2x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     47\u001b[0m     lagrangian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(L[i] \u001b[38;5;241m@\u001b[39m mu_L \u001b[38;5;241m-\u001b[39m H[i] \u001b[38;5;241m@\u001b[39m mu_H)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     lagrangian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(L[i] \u001b[38;5;241m@\u001b[39m Sigma_L \u001b[38;5;241m@\u001b[39m L[i]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m Q_vars[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     49\u001b[0m     lagrangian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(H[i] \u001b[38;5;241m@\u001b[39m Sigma_H \u001b[38;5;241m@\u001b[39m H[i]\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m W_vars[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     51\u001b[0m lagrangian \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_eps \u001b[38;5;241m*\u001b[39m (epsilon\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(mu_L \u001b[38;5;241m-\u001b[39m mu_L_hat)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(Sigma_L \u001b[38;5;241m-\u001b[39m Sigma_L_hat)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x3 and 2x2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the dimensions\n",
    "dim_mu_L = 3\n",
    "dim_mu_H = 2\n",
    "dim_Sigma = 2\n",
    "N = 5  # Number of samples\n",
    "\n",
    "# Initialize parameters\n",
    "L = [torch.randn(dim_Sigma, dim_mu_L) for _ in range(N)]\n",
    "H = [torch.randn(dim_Sigma, dim_mu_H) for _ in range(N)]\n",
    "Q = [torch.randn(dim_Sigma, dim_mu_L) for _ in range(N)]\n",
    "W = [torch.randn(dim_Sigma, dim_mu_H) for _ in range(N)]\n",
    "epsilon = 0.1\n",
    "delta = 0.1\n",
    "mu_L_hat = torch.randn(dim_mu_L)\n",
    "Sigma_L_hat = torch.randn(dim_Sigma, dim_Sigma)\n",
    "mu_H_hat = torch.randn(dim_mu_H)\n",
    "Sigma_H_hat = torch.randn(dim_Sigma, dim_Sigma)\n",
    "alpha = 0.5\n",
    "rho_Q = 1.0\n",
    "rho_W = 1.0\n",
    "num_iterations = 100\n",
    "\n",
    "# Initialize variables\n",
    "mu_L = torch.zeros(dim_mu_L, requires_grad=True)\n",
    "mu_H = torch.zeros(dim_mu_H, requires_grad=True)\n",
    "Sigma_L = torch.eye(dim_Sigma, requires_grad=True)\n",
    "Sigma_H = torch.eye(dim_Sigma, requires_grad=True)\n",
    "\n",
    "Q_vars = [torch.zeros(Q[i].shape, requires_grad=True) for i in range(N)]\n",
    "W_vars = [torch.zeros(W[i].shape, requires_grad=True) for i in range(N)]\n",
    "\n",
    "lambda_Q = [torch.zeros(Q[i].shape) for i in range(N)]\n",
    "lambda_W = [torch.zeros(W[i].shape) for i in range(N)]\n",
    "lambda_eps = torch.tensor(0.0, requires_grad=True)\n",
    "lambda_del = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([mu_L, mu_H, Sigma_L, Sigma_H, lambda_eps, lambda_del] + Q_vars + W_vars, lr=0.01)\n",
    "\n",
    "for k in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the augmented Lagrangian\n",
    "    lagrangian = 0\n",
    "    for i in range(N):\n",
    "        lagrangian += torch.norm(L[i] @ mu_L - H[i] @ mu_H)**2\n",
    "        lagrangian += torch.norm(L[i] @ Sigma_L @ L[i].T - Q_vars[i])**2\n",
    "        lagrangian += torch.norm(H[i] @ Sigma_H @ H[i].T - W_vars[i])**2\n",
    "\n",
    "    lagrangian += lambda_eps * (epsilon**2 - torch.norm(mu_L - mu_L_hat)**2 - torch.norm(Sigma_L - Sigma_L_hat)**2)\n",
    "    lagrangian += lambda_del * (delta**2 - torch.norm(mu_H - mu_H_hat)**2 - torch.norm(Sigma_H - Sigma_H_hat)**2)\n",
    "\n",
    "    # Compute gradients\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # Update variables\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update Lagrange multipliers\n",
    "    with torch.no_grad():\n",
    "        for i in range(N):\n",
    "            lambda_Q[i] += rho_Q * (Q_vars[i] - torch.sqrt(L[i] @ Sigma_L @ L[i].T))\n",
    "            lambda_W[i] += rho_W * (W_vars[i] - torch.sqrt(H[i] @ Sigma_H @ H[i].T))\n",
    "\n",
    "        lambda_eps += (epsilon**2 - torch.norm(mu_L - mu_L_hat)**2 - torch.norm(Sigma_L - Sigma_L_hat)**2)\n",
    "        lambda_del += (delta**2 - torch.norm(mu_H - mu_H_hat)**2 - torch.norm(Sigma_H - Sigma_H_hat)**2)\n",
    "\n",
    "# Print the results\n",
    "print(\"mu_L:\", mu_L)\n",
    "print(\"Sigma_L:\", Sigma_L)\n",
    "print(\"mu_H:\", mu_H)\n",
    "print(\"Sigma_H:\", Sigma_H)\n",
    "print(\"Q_vars:\", Q_vars)\n",
    "print(\"W_vars:\", W_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee1999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4cb8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4919bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7066aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ec612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ffe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0cca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26bbb2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped point: [3.53163521 6.39094344]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "mu_a = np.array([1.0, 2.0, 3.0])  # Mean of the 3D Gaussian\n",
    "Sigma_a = np.array([[1.0, 0.5, 0.2], [0.5, 1.0, 0.3], [0.2, 0.3, 1.0]])  # Covariance of the 3D Gaussian\n",
    "\n",
    "mu_b = np.array([3.0, 4.0])  # Mean of the 2D Gaussian\n",
    "Sigma_b = np.array([[1.0, 0.2], [0.2, 1.0]])  # Covariance of the 2D Gaussian\n",
    "\n",
    "# Compute the Monge map\n",
    "monge_map = compute_monge_map(mu_a, Sigma_a, mu_b, Sigma_b)\n",
    "\n",
    "# Example input point from the 3D Gaussian\n",
    "x_sample = np.array([1.5, 2.5, 3.5])\n",
    "mapped_point = monge_map(x_sample)\n",
    "\n",
    "print(\"Mapped point:\", mapped_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ea402d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m mu_U_ll_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(d_LL)  \n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update mu_L\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m mu_L, mu_L_expr \u001b[38;5;241m=\u001b[39m update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Print the CVXPY expression for the updated mu_L\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdate expression for mu_L:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mu_L_expr)\n",
      "Cell \u001b[0;32mIn[137], line 28\u001b[0m, in \u001b[0;36mupdate_mu_L\u001b[0;34m(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat)\u001b[0m\n\u001b[1;32m     25\u001b[0m E_LL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(LL_i \u001b[38;5;241m@\u001b[39m LL_i\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m LL_i \u001b[38;5;129;01min\u001b[39;00m LLmodels) \u001b[38;5;241m/\u001b[39m N  \u001b[38;5;66;03m# Correct dimension handling\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute the expected value of L_i^T H_i mu_H\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m E_LH_mu_H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(LL_i \u001b[38;5;241m@\u001b[39m HL_i \u001b[38;5;241m@\u001b[39m mu_H \u001b[38;5;28;01mfor\u001b[39;00m LL_i, HL_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(LLmodels, HLmodels)) \u001b[38;5;241m/\u001b[39m N\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Regularization term\u001b[39;00m\n\u001b[1;32m     31\u001b[0m reg_term \u001b[38;5;241m=\u001b[39m (lambda_eps \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(mu_U_ll_hat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[137], line 28\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m E_LL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(LL_i \u001b[38;5;241m@\u001b[39m LL_i\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m LL_i \u001b[38;5;129;01min\u001b[39;00m LLmodels) \u001b[38;5;241m/\u001b[39m N  \u001b[38;5;66;03m# Correct dimension handling\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute the expected value of L_i^T H_i mu_H\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m E_LH_mu_H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(LL_i \u001b[38;5;241m@\u001b[39m HL_i \u001b[38;5;241m@\u001b[39m mu_H \u001b[38;5;28;01mfor\u001b[39;00m LL_i, HL_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(LLmodels, HLmodels)) \u001b[38;5;241m/\u001b[39m N\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Regularization term\u001b[39;00m\n\u001b[1;32m     31\u001b[0m reg_term \u001b[38;5;241m=\u001b[39m (lambda_eps \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(mu_U_ll_hat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "def update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat):\n",
    "    \"\"\"\n",
    "    Update mu_L using the specified formula in a CVXPY context.\n",
    "    \n",
    "    Parameters:\n",
    "    - LLmodels: List of L_i matrices (square numpy arrays).\n",
    "    - HLmodels: List of H_i matrices (square numpy arrays).\n",
    "    - mu_H: Current estimate of mu_H (cvxpy Variable).\n",
    "    - lambda_eps: Regularization parameter for epsilon.\n",
    "    - mu_U_ll_hat: Reference mean vector of the low-level model.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated mu_L as a cvxpy Variable.\n",
    "    \"\"\"\n",
    "    # Number of samples\n",
    "    N = len(LLmodels)\n",
    "\n",
    "    # CVXPY variable for mu_L (3-dimensional)\n",
    "    mu_L = cp.Variable(mu_U_ll_hat.shape[0])\n",
    "    \n",
    "    # Compute the expected value of L_i^T L_i\n",
    "    E_LL = sum(LL_i @ LL_i.T for LL_i in LLmodels) / N  # Correct dimension handling\n",
    "    \n",
    "    # Compute the expected value of L_i^T H_i mu_H\n",
    "    E_LH_mu_H = sum(LL_i @ HL_i @ mu_H for LL_i, HL_i in zip(LLmodels, HLmodels)) / N\n",
    "    \n",
    "    # Regularization term\n",
    "    reg_term = (lambda_eps / 2) * np.eye(mu_U_ll_hat.shape[0])\n",
    "    \n",
    "    # Construct the update expression using cp.inv for matrix inversion\n",
    "    mu_L_expr = cp.inv(E_LL + reg_term) @ (E_LH_mu_H + (lambda_eps / 2) * mu_U_ll_hat)\n",
    "\n",
    "    return mu_L, mu_L_expr  # Returning mu_L variable for optimization and its expression\n",
    "\n",
    "# Example usage\n",
    "d_LL = 3  # Dimension of low-level model\n",
    "d_HL = 2  # Dimension of high-level model\n",
    "\n",
    "# Create square matrices for LLmodels and HLmodels\n",
    "LLmodels = [np.random.rand(d_LL, d_LL) for _ in range(5)]  # Example L_i matrices (3x3)\n",
    "HLmodels = [np.random.rand(d_HL, d_HL) for _ in range(5)]  # Example H_i matrices (2x2)\n",
    "\n",
    "# Current estimate for mu_H as a CVXPY variable (2-dimensional)\n",
    "mu_H = cp.Variable(d_HL)  \n",
    "lambda_eps = 0.1  # Regularization parameter\n",
    "# Reference mean vector for mu_L (3-dimensional)\n",
    "mu_U_ll_hat = np.random.rand(d_LL)  \n",
    "\n",
    "# Update mu_L\n",
    "mu_L, mu_L_expr = update_mu_L(LLmodels, HLmodels, mu_H, lambda_eps, mu_U_ll_hat)\n",
    "\n",
    "# Print the CVXPY expression for the updated mu_L\n",
    "print(\"Update expression for mu_L:\", mu_L_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322db57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067bba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90689836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6fba199d",
   "metadata": {},
   "outputs": [
    {
     "ename": "DCPError",
     "evalue": "Problem does not follow DCP rules. Specifically:\nThe objective is not DCP, even though each sub-expression is.\nYou are trying to minimize a function that is concave.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDCPError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Update each variable in turn\u001b[39;00m\n\u001b[1;32m     74\u001b[0m T \u001b[38;5;241m=\u001b[39m update_T(L, H, U_L, U_H, Theta, Phi, T)\n\u001b[0;32m---> 75\u001b[0m Theta \u001b[38;5;241m=\u001b[39m update_Theta(L, H, U_L, U_H, T, Phi, Theta, epsilon, N)\n\u001b[1;32m     76\u001b[0m Phi \u001b[38;5;241m=\u001b[39m update_Phi(L, H, U_L, U_H, T, Theta, Phi, delta, N)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[166], line 51\u001b[0m, in \u001b[0;36mupdate_Theta\u001b[0;34m(L, H, U_L, U_H, T, Phi, Theta_prev, epsilon, N)\u001b[0m\n\u001b[1;32m     49\u001b[0m objective \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mMinimize(\u001b[38;5;241m-\u001b[39mobjective \u001b[38;5;241m/\u001b[39m num_samples)\n\u001b[1;32m     50\u001b[0m prob \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(objective, constraints)\n\u001b[0;32m---> 51\u001b[0m prob\u001b[38;5;241m.\u001b[39msolve()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Theta_var\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/problems/problem.py:503\u001b[0m, in \u001b[0;36mProblem.solve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     solve_func \u001b[38;5;241m=\u001b[39m Problem\u001b[38;5;241m.\u001b[39m_solve\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solve_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/problems/problem.py:1073\u001b[0m, in \u001b[0;36mProblem._solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack(chain\u001b[38;5;241m.\u001b[39mretrieve(soln))\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m-> 1073\u001b[0m data, solving_chain, inverse_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_problem_data(\n\u001b[1;32m   1074\u001b[0m     solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, kwargs\n\u001b[1;32m   1075\u001b[0m )\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_NUM_SOLVER_STR)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/problems/problem.py:646\u001b[0m, in \u001b[0;36mProblem.get_problem_data\u001b[0;34m(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mkey:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39minvalidate()\n\u001b[0;32m--> 646\u001b[0m     solving_chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_chain(\n\u001b[1;32m    647\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver, gp\u001b[38;5;241m=\u001b[39mgp,\n\u001b[1;32m    648\u001b[0m         enforce_dpp\u001b[38;5;241m=\u001b[39menforce_dpp,\n\u001b[1;32m    649\u001b[0m         ignore_dpp\u001b[38;5;241m=\u001b[39mignore_dpp,\n\u001b[1;32m    650\u001b[0m         canon_backend\u001b[38;5;241m=\u001b[39mcanon_backend,\n\u001b[1;32m    651\u001b[0m         solver_opts\u001b[38;5;241m=\u001b[39msolver_opts)\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39msolving_chain \u001b[38;5;241m=\u001b[39m solving_chain\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/problems/problem.py:898\u001b[0m, in \u001b[0;36mProblem._construct_chain\u001b[0;34m(self, solver, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\u001b[0m\n\u001b[1;32m    896\u001b[0m candidate_solvers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_candidate_solvers(solver\u001b[38;5;241m=\u001b[39msolver, gp\u001b[38;5;241m=\u001b[39mgp)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_candidate_solvers(candidate_solvers)\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m construct_solving_chain(\u001b[38;5;28mself\u001b[39m, candidate_solvers, gp\u001b[38;5;241m=\u001b[39mgp,\n\u001b[1;32m    899\u001b[0m                                enforce_dpp\u001b[38;5;241m=\u001b[39menforce_dpp,\n\u001b[1;32m    900\u001b[0m                                ignore_dpp\u001b[38;5;241m=\u001b[39mignore_dpp,\n\u001b[1;32m    901\u001b[0m                                canon_backend\u001b[38;5;241m=\u001b[39mcanon_backend,\n\u001b[1;32m    902\u001b[0m                                solver_opts\u001b[38;5;241m=\u001b[39msolver_opts,\n\u001b[1;32m    903\u001b[0m                                specified_solver\u001b[38;5;241m=\u001b[39msolver)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/reductions/solvers/solving_chain.py:228\u001b[0m, in \u001b[0;36mconstruct_solving_chain\u001b[0;34m(problem, candidates, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts, specified_solver)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(problem\u001b[38;5;241m.\u001b[39mvariables()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SolvingChain(reductions\u001b[38;5;241m=\u001b[39m[ConstantSolver()])\n\u001b[0;32m--> 228\u001b[0m reductions \u001b[38;5;241m=\u001b[39m _reductions_for_problem_class(problem, candidates, gp, solver_opts)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Process DPP status of the problem.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m dpp_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdcp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdgp\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/cvxpy/reductions/solvers/solving_chain.py:143\u001b[0m, in \u001b[0;36m_reductions_for_problem_class\u001b[0;34m(problem, candidates, gp, solver_opts)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m problem\u001b[38;5;241m.\u001b[39mis_dqcp():\n\u001b[1;32m    141\u001b[0m         append \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHowever, the problem does follow DQCP rules. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider calling solve() with `qcp=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DCPError(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem does not follow DCP rules. Specifically:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m append)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m gp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m problem\u001b[38;5;241m.\u001b[39mis_dgp():\n\u001b[1;32m    146\u001b[0m     append \u001b[38;5;241m=\u001b[39m build_non_disciplined_error_msg(problem, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDGP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mDCPError\u001b[0m: Problem does not follow DCP rules. Specifically:\nThe objective is not DCP, even though each sub-expression is.\nYou are trying to minimize a function that is concave."
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Define your problem data\n",
    "L = np.random.rand(10, 3, 3)  # Example data, replace with actual values\n",
    "H = np.random.rand(10, 2, 2)\n",
    "U_L = np.random.rand(5, 3)\n",
    "U_H = np.random.rand(5, 2)\n",
    "N = 5\n",
    "epsilon = 1.0\n",
    "delta = 1.0\n",
    "num_samples = L.shape[0]\n",
    "m, n = H.shape[1], L.shape[1]\n",
    "\n",
    "# Initialize variables\n",
    "T = np.random.rand(m, n)\n",
    "Theta = np.random.rand(N, n)\n",
    "Phi = np.random.rand(N, m)\n",
    "\n",
    "# Alternating optimization parameters\n",
    "max_iters = 100\n",
    "tol = 1e-4\n",
    "\n",
    "# Define the update functions\n",
    "def update_T(L, H, U_L, U_H, Theta, Phi, T_prev):\n",
    "    \"\"\"Update step for T using CVXPY with fixed Theta and Phi.\"\"\"\n",
    "    T_var = cp.Variable((m, n), nonneg=True)\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T_var @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "    return T_var.value\n",
    "\n",
    "def update_Theta(L, H, U_L, U_H, T, Phi, Theta_prev, epsilon, N):\n",
    "    \"\"\"Update step for Theta using CVXPY with fixed T and Phi.\"\"\"\n",
    "    Theta_var = cp.Variable((N, n))\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T @ Li @ (U_L.T + Theta_var.T) - Hi @ (U_H.T + Phi.T)\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "    constraints = [cp.norm(Theta_var, \"fro\") <= np.sqrt(N * epsilon**2)]\n",
    "    objective = cp.Minimize(-objective / num_samples)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    return Theta_var.value\n",
    "\n",
    "def update_Phi(L, H, U_L, U_H, T, Theta, Phi_prev, delta, N):\n",
    "    \"\"\"Update step for Phi using CVXPY with fixed T and Theta.\"\"\"\n",
    "    Phi_var = cp.Variable((N, m))\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi_var.T)\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "    constraints = [cp.norm(Phi_var, \"fro\") <= np.sqrt(N * delta**2)]\n",
    "    objective = cp.Minimize(-objective / num_samples)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    return Phi_var.value\n",
    "\n",
    "# Main optimization loop\n",
    "for iteration in range(max_iters):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Update each variable in turn\n",
    "    T = update_T(L, H, U_L, U_H, Theta, Phi, T)\n",
    "    Theta = update_Theta(L, H, U_L, U_H, T, Phi, Theta, epsilon, N)\n",
    "    Phi = update_Phi(L, H, U_L, U_H, T, Theta, Phi, delta, N)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d01101a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 17 iterations.\n",
      "Optimized T: [[1.76400242e-06 2.47997573e-06 2.61594958e-06]\n",
      " [1.72439876e-06 2.44073676e-06 2.54596391e-06]]\n",
      "Optimized Theta: [[-0.0015276  -0.00126539 -0.00157781]\n",
      " [-0.004604   -0.00381353 -0.00475515]\n",
      " [-0.00521074 -0.00431356 -0.00537965]\n",
      " [-0.00543193 -0.0044979  -0.00560906]\n",
      " [-0.00454818 -0.00376629 -0.00469665]]\n",
      "Optimized Phi: [[-0.10597937 -0.31120573]\n",
      " [-0.36405932 -0.86442975]\n",
      " [-0.9319195  -0.15612672]\n",
      " [-0.71643512 -0.56644337]\n",
      " [-0.56412914 -0.53022537]]\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Define your problem data\n",
    "L = np.random.rand(10, 3, 3)  # Example data, replace with actual values\n",
    "H = np.random.rand(10, 2, 2)\n",
    "U_L = np.random.rand(5, 3)\n",
    "U_H = np.random.rand(5, 2)\n",
    "N = 5\n",
    "epsilon = 1.0\n",
    "delta = 1.0\n",
    "num_samples = L.shape[0]\n",
    "m, n = H.shape[1], L.shape[1]\n",
    "\n",
    "# Initialize variables\n",
    "T = np.random.rand(m, n)\n",
    "Theta = np.random.rand(N, n)\n",
    "Phi = np.random.rand(N, m)\n",
    "\n",
    "# Alternating optimization parameters\n",
    "max_iters = 100\n",
    "tol = 1e-4\n",
    "\n",
    "# Define the update functions\n",
    "def update_T(L, H, U_L, U_H, Theta, Phi, T_prev):\n",
    "    \"\"\"Update step for T using CVXPY with fixed Theta and Phi.\"\"\"\n",
    "    T_var = cp.Variable((m, n), nonneg=True)\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T_var @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "    return T_var.value\n",
    "\n",
    "def update_Theta(L, H, U_L, U_H, T, Phi, Theta_prev, epsilon, N):\n",
    "    \"\"\"Update step for Theta using CVXPY with fixed T and Phi.\"\"\"\n",
    "    Theta_var = cp.Variable((N, n))\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T @ Li @ (U_L.T + Theta_var.T) - Hi @ (U_H.T + Phi.T)\n",
    "        objective = cp.norm(A, \"fro\")**2\n",
    "    constraints = [cp.norm(Theta_var, \"fro\") <= np.sqrt(N * epsilon**2)]\n",
    "    # Minimize the negative of the objective to handle maximization\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    return Theta_var.value\n",
    "\n",
    "def update_Phi(L, H, U_L, U_H, T, Theta, Phi_prev, delta, N):\n",
    "    \"\"\"Update step for Phi using CVXPY with fixed T and Theta.\"\"\"\n",
    "    Phi_var = cp.Variable((N, m))\n",
    "    objective = 0\n",
    "    for i in range(num_samples):\n",
    "        Li = L[i]\n",
    "        Hi = H[i]\n",
    "        A = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi_var.T)\n",
    "        objective = cp.norm(A, \"fro\")**2\n",
    "    constraints = [cp.norm(Phi_var, \"fro\") <= np.sqrt(N * delta**2)]\n",
    "    # Minimize the negative of the objective to handle maximization\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    return Phi_var.value\n",
    "\n",
    "# Main optimization loop\n",
    "for iteration in range(max_iters):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Update each variable in turn\n",
    "    T     = update_T(L, H, U_L, U_H, Theta, Phi, T)\n",
    "    Theta = update_Theta(L, H, U_L, U_H, T, Phi, Theta, epsilon, N)\n",
    "    Phi   = update_Phi(L, H, U_L, U_H, T, Theta, Phi, delta, N)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7af29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dbcf90db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized T: [[0.19407151 0.50730026 0.48335842]\n",
      " [0.19189463 0.13342697 0.2741195 ]]\n",
      "Optimized Theta: [[ 0.04010824  0.48980465  0.32861929]\n",
      " [ 0.4817851   0.13822311  0.41436349]\n",
      " [-0.95775434 -1.00583146 -0.69239412]\n",
      " [-0.17044718  0.71039676  0.5405109 ]\n",
      " [ 0.29193497  0.59663659  0.62026516]]\n",
      "Optimized Phi: [[ 0.38564357 -0.17346259]\n",
      " [ 0.04767077  0.3532042 ]\n",
      " [-1.68878638 -1.13244729]\n",
      " [ 0.2707614  -0.07152461]\n",
      " [ 0.19612339  0.34677054]]\n"
     ]
    }
   ],
   "source": [
    "U_L = U_ll_hat\n",
    "U_H = U_hl_hat\n",
    "\n",
    "num_samples, n = U_L.shape\n",
    "num_samples, m = U_H.shape\n",
    "\n",
    "epsilon     = 1.0\n",
    "delta       = 1.0\n",
    "alpha       = 0.01  # Learning rate for ascent steps in Theta and Phi\n",
    "\n",
    "# Initialize variables\n",
    "T     = np.random.rand(m, n)\n",
    "Theta = np.random.rand(N, n)\n",
    "Phi   = np.random.rand(N, m)\n",
    "\n",
    "# Project onto Frobenius ball function\n",
    "def project_onto_frobenius_ball(matrix, radius):\n",
    "    norm = np.linalg.norm(matrix, 'fro')\n",
    "    if norm > radius:\n",
    "        return matrix * (radius / norm)\n",
    "    return matrix\n",
    "\n",
    "# Update function for T \n",
    "def update_T(U_L, U_H, Theta, Phi):\n",
    "    T_var = cp.Variable((m, n), nonneg=True)\n",
    "    objective = 0\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A  = T_var @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        objective += cp.norm(A, \"fro\")**2\n",
    "\n",
    "    objective = cp.Minimize(objective / num_samples)\n",
    "    prob = cp.Problem(objective)\n",
    "    prob.solve()\n",
    "    return T_var.value\n",
    "\n",
    "# Gradient ascent step for Theta\n",
    "def ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, N, alpha):\n",
    "    gradient = np.zeros_like(Theta)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += ((T @ Li).T @ A).T  # Compute gradient wrt Theta\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Theta += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Theta, np.sqrt(N * epsilon**2))\n",
    "\n",
    "# Gradient ascent step for Phi\n",
    "def ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, N, alpha):\n",
    "    gradient = np.zeros_like(Phi)\n",
    "    for iota in Ill:\n",
    "        Li = LLmodels[iota].compute_mechanism() \n",
    "        Hi = HLmodels[omega[iota]].compute_mechanism()\n",
    "        A  = T @ Li @ (U_L.T + Theta.T) - Hi @ (U_H.T + Phi.T)\n",
    "\n",
    "        gradient += (Hi @ A).T  # Compute gradient wrt Phi\n",
    "\n",
    "    gradient /= num_samples\n",
    "    Phi += alpha * gradient  # Ascent step\n",
    "    return project_onto_frobenius_ball(Phi, np.sqrt(N * delta**2))\n",
    "\n",
    "# Main optimization loop\n",
    "max_iters = 100\n",
    "tol = 1e-4\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    T_prev, Theta_prev, Phi_prev = T.copy(), Theta.copy(), Phi.copy()\n",
    "\n",
    "    # Minimize wrt T\n",
    "    T = update_T(L, H, U_L, U_H, Theta, Phi)\n",
    "\n",
    "    # Maximize wrt Theta and Phi using gradient ascent\n",
    "    Theta = ascent_step_Theta(U_L, U_H, T, Phi, Theta, epsilon, N, alpha)\n",
    "    Phi   = ascent_step_Phi(U_L, U_H, T, Theta, Phi, delta, N, alpha)\n",
    "\n",
    "    # Check for convergence\n",
    "    if (np.linalg.norm(T - T_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Theta - Theta_prev, 'fro') < tol and\n",
    "        np.linalg.norm(Phi - Phi_prev, 'fro') < tol):\n",
    "        print(f\"Converged in {iteration + 1} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimized values of T, Theta, and Phi\n",
    "print(\"Optimized T:\", T)\n",
    "print(\"Optimized Theta:\", Theta)\n",
    "print(\"Optimized Phi:\", Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8cce5242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5 2.5 3.5] maps to [3.25111237 1.58082761]\n"
     ]
    }
   ],
   "source": [
    "x_sample = np.array([1.5, 2.5, 3.5])\n",
    "mapped_point = T @ x_sample\n",
    "print(f'{x_sample} maps to {mapped_point}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dfac823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barycenter Mean: [-0.2  2.8]\n",
      "Barycenter Covariance: [[0.88284666 0.23464565]\n",
      " [0.23464565 2.50711389]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402becb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265db04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
