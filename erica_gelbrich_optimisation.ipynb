{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon         = params.radius[experiment][0]\n",
    "ll_num_envs     = params.n_envs[experiment][0]\n",
    "\n",
    "delta           = params.radius[experiment][1]\n",
    "hl_num_envs     = params.n_envs[experiment][1]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples   = params.n_samples[experiment][0]\n",
    "num_hlsamples   = params.n_samples[experiment][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll = mut.load_samples(experiment)[None][0] \n",
    "Gll = mut.load_ll_model(experiment)[0]\n",
    "Ill = mut.load_ll_model(experiment)[1]\n",
    "\n",
    "\n",
    "Dhl = mut.load_samples(experiment)[None][1] \n",
    "Ghl = mut.load_hl_model(experiment)[0]\n",
    "Ihl = mut.load_hl_model(experiment)[1]\n",
    "\n",
    "omega = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c38736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambiguity set construction: Based on epsilon and delta include distribution (as many as the num_envs) that\n",
    "# # pass the \"gelbrich\" test.\n",
    "# ll_moments = mut.sample_moments_U(mu_hat    = mu_U_ll_hat,\n",
    "#                                   Sigma_hat = Sigma_U_ll_hat,\n",
    "#                                   bound     = epsilon,\n",
    "#                                   num_envs  = ll_num_envs)\n",
    "\n",
    "# A_ll       = mut.sample_distros_Gelbrich(ll_moments) #Low-level: A_epsilon\n",
    "\n",
    "\n",
    "# hl_moments = mut.sample_moments_U(mu_hat    = mu_U_hl_hat,\n",
    "#                                   Sigma_hat = Sigma_U_hl_hat,\n",
    "#                                   bound     = delta,\n",
    "#                                   num_envs  = hl_num_envs)\n",
    "\n",
    "# A_hl       = mut.sample_distros_Gelbrich(hl_moments) #High-level A_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd01dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstraction_errors             = {}\n",
    "# abstraction_env_errors         = {}\n",
    "# max_env_avg_interv_error_value = -np.inf\n",
    "# max_env_avg_interv_error_key   = None\n",
    "# distance_err                   = 'wass'\n",
    "\n",
    "# for lenv in A_ll:\n",
    "\n",
    "#     Dll_noise      = lenv.sample(num_llsamples)[0]\n",
    "#     ll_environment = mut.get_exogenous_distribution(Dll_noise)\n",
    "\n",
    "#     for henv in A_hl:\n",
    "#         Dhl_noise      = henv.sample(num_hlsamples)[0]\n",
    "#         hl_environment = mut.get_exogenous_distribution(Dhl_noise)\n",
    "\n",
    "#         total_ui_error = 0\n",
    "#         num_distros    = len(Ill)\n",
    "\n",
    "#         n, m  = len(LLmodels[None].endogenous_vars), len(HLmodels[None].endogenous_vars)\n",
    "\n",
    "#         T     = mut.sample_stoch_matrix(n, m)\n",
    "\n",
    "#         for iota in Ill:\n",
    "#             llcm   = LLmodels[iota]\n",
    "#             hlcm   = HLmodels[omega[iota]]\n",
    "#             llmech = llcm.compute_mechanism()\n",
    "#             hlmech = hlcm.compute_mechanism()\n",
    "#             error  = mut.ui_error_dist(distance_err, lenv, henv, llmech, hlmech, T)\n",
    "\n",
    "#             total_ui_error += error\n",
    "\n",
    "#         avg_interv_error = total_ui_error/num_distros\n",
    "\n",
    "#         if avg_interv_error > max_env_avg_interv_error_value:\n",
    "#             max_env_avg_interv_error_value = avg_interv_error\n",
    "#             max_env_avg_interv_error_key   = (lenv, henv)\n",
    "\n",
    "#         abstraction_errors[str(T)] = avg_interv_error\n",
    "#         abstraction_env_errors['ll: '+str(ll_environment.means_)+' hl: '+str(hl_environment.means_)] = avg_interv_error\n",
    "\n",
    "\n",
    "# max_tau   = max(abstraction_errors, key=abstraction_errors.get)\n",
    "# max_error = abstraction_errors[max_tau]\n",
    "\n",
    "# print(f\"Abstraction: {max_tau}, Error: {max_error}\")\n",
    "# print('==============================================================================' )\n",
    "# max_lenv = max_env_avg_interv_error_key[0]\n",
    "# max_henv = max_env_avg_interv_error_key[1]\n",
    "\n",
    "# print(f\"max LL mean vector = {max_lenv.means_}\")\n",
    "# print(f\"max LL covariance = {max_lenv.covariances_}\")\n",
    "# print( )\n",
    "\n",
    "# print(f\"max HL mean vector = {max_henv.means_}\")\n",
    "# print(f\"max HL covariance = {max_henv.covariances_}\")\n",
    "# print('==============================================================================' )\n",
    "# print(f\"max environment, average interventional abstraction error = {max_env_avg_interv_error_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf53d7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a7f3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta):\n",
    "    grad_mu_L = torch.zeros_like(mu_L, dtype=torch.float32) \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float() \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float() \n",
    "\n",
    "        grad_mu_L += torch.matmul(V_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float())) \n",
    "    \n",
    "    grad_mu_L = (2 / n) * grad_mu_L - 2 * lambda_L * (mu_L - hat_mu_L)\n",
    "    mu_L = mu_L + (eta * grad_mu_L)\n",
    "    return mu_L\n",
    "\n",
    "def update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta):\n",
    "    grad_mu_H = torch.zeros_like(mu_H, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "        grad_mu_H -= torch.matmul(H_i.T, torch.matmul(V_i, mu_L.float()) - torch.matmul(H_i, mu_H.float()))\n",
    "    \n",
    "    grad_mu_H = (2 / n) * grad_mu_H - 2 * lambda_H * (mu_H - hat_mu_H)\n",
    "    \n",
    "    mu_H = mu_H + (eta * grad_mu_H)\n",
    "    return mu_H\n",
    "\n",
    "\n",
    "def update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta):\n",
    "    grad_Sigma_L = torch.zeros_like(Sigma_L)\n",
    "    \n",
    "    # Term 1: (2/n) * sum_i(V_i^T * V_i)\n",
    "    term1 = torch.zeros_like(Sigma_L)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        term1 = term1 + torch.matmul(V_i.T, V_i)\n",
    "\n",
    "    # Term 2: -2 * lambda_L * (Sigma_L^(1/2) - hat_Sigma_L^(1/2)) * Sigma_L^(-1/2)\n",
    "    Sigma_L_sqrt = oput.sqrtm_svd(Sigma_L)  # Compute the square root of Sigma_L\n",
    "    #Sigma_L_sqrt = torch.linalg.matrix_power(Sigma_L, 0.5)\n",
    "\n",
    "    hat_Sigma_L_sqrt = oput.sqrtm_svd(hat_Sigma_L)  # Compute the square root of hat_Sigma_L\n",
    "\n",
    "    term2 = -2 * lambda_L * (Sigma_L_sqrt - hat_Sigma_L_sqrt) @ torch.inverse(Sigma_L_sqrt)\n",
    "\n",
    "    # Combine terms\n",
    "    grad_Sigma_L = (2 / n) * term1 + term2\n",
    "\n",
    "    # Update Sigma_L\n",
    "    Sigma_L_half = Sigma_L + eta * grad_Sigma_L\n",
    "    #Sigma_L_half  = diagonalize(Sigma_L_half)\n",
    "    return Sigma_L_half\n",
    "\n",
    "\n",
    "def update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param):\n",
    "    Sigma_L_final = torch.zeros_like(Sigma_L_half, dtype=torch.float32)  \n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "        V_i = T @ L_i  \n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "        \n",
    "        Sigma_L_half      = Sigma_L_half.float()\n",
    "        V_Sigma_V         = torch.matmul(V_i, torch.matmul(Sigma_L_half, V_i.T))\n",
    "        sqrtm_V_Sigma_V   = oput.sqrtm_svd(V_Sigma_V)\n",
    "        prox_Sigma_L_half = torch.matmul(oput.prox_operator(sqrtm_V_Sigma_V, lambda_param), oput.prox_operator(sqrtm_V_Sigma_V, lambda_param).T)\n",
    "        ll_term           = torch.matmul(torch.matmul(torch.linalg.pinv(V_i), prox_Sigma_L_half), torch.linalg.pinv(V_i).T)\n",
    "\n",
    "        Sigma_H   = Sigma_H.float()  \n",
    "        H_Sigma_H = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)).float()\n",
    "        hl_term   = torch.norm(oput.sqrtm_svd(H_Sigma_H), p='fro')\n",
    "\n",
    "        Sigma_L_final = Sigma_L_final + (ll_term * hl_term)\n",
    "\n",
    "    Sigma_L_final =  Sigma_L_final * (2 / n)\n",
    "    Sigma_L_final = oput.diagonalize(Sigma_L_final)\n",
    "\n",
    "    return Sigma_L_final\n",
    "\n",
    "\n",
    "def update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta):\n",
    "    grad_Sigma_H = torch.zeros_like(Sigma_H)\n",
    "    term1 = torch.zeros_like(Sigma_H)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "        term1 = term1 + torch.matmul(H_i.T, H_i)\n",
    "\n",
    "    Sigma_H_sqrt     = oput.sqrtm_svd(Sigma_H)  \n",
    "    hat_Sigma_H_sqrt = oput.sqrtm_svd(hat_Sigma_H) \n",
    "\n",
    "    term2 = -2 * lambda_H * (Sigma_H_sqrt - hat_Sigma_H_sqrt) @ torch.inverse(Sigma_H_sqrt)\n",
    "\n",
    "    grad_Sigma_H = (2 / n) * term1 + term2\n",
    "\n",
    "    Sigma_H_half = Sigma_H + eta * grad_Sigma_H\n",
    "    return Sigma_H_half\n",
    "\n",
    "def check_for_invalid_values(matrix):\n",
    "    if torch.isnan(matrix).any() or torch.isinf(matrix).any():\n",
    "        #print(\"Matrix contains NaN or Inf values!\")\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def handle_nans(matrix, replacement_value=0.0):\n",
    "    # Replace NaNs with a given value (default is 0)\n",
    "    if torch.isnan(matrix).any():\n",
    "        print(\"Warning: NaN values found! Replacing with zero.\")\n",
    "        matrix = torch.nan_to_num(matrix, nan=replacement_value)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param):\n",
    "    if check_for_invalid_values(Sigma_L):\n",
    "        print(\"Sigma_L contains NaN or Inf values!\")\n",
    "    Sigma_H_final = torch.zeros_like(Sigma_H_half)\n",
    "    for n, iota in enumerate(Ill):\n",
    "        L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "        V_i = T @ L_i.float()\n",
    "        H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "        H_Sigma_H         = torch.matmul(H_i, torch.matmul(Sigma_H_half, H_i.T))\n",
    "        sqrtm_H_Sigma_H   = oput.sqrtm_svd(H_Sigma_H)\n",
    "        prox_Sigma_H_half = torch.matmul(oput.prox_operator(sqrtm_H_Sigma_H, lambda_param), oput.prox_operator(sqrtm_H_Sigma_H, lambda_param).T)\n",
    "        hl_term           = torch.matmul(torch.matmul(torch.inverse(H_i), prox_Sigma_H_half), torch.inverse(H_i).T)  \n",
    "        \n",
    "        V_Sigma_V = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "        ll_term   = torch.norm(oput.sqrtm_svd(V_Sigma_V))\n",
    "\n",
    "        Sigma_H_final = Sigma_H_final + (ll_term * hl_term)\n",
    "    \n",
    "    Sigma_H_final = Sigma_H_final * (2 / n)\n",
    "    Sigma_H_final = oput.diagonalize(Sigma_H_final)\n",
    "    \n",
    "    return Sigma_H_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996b4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "    # Constraint 1: epsilon^2 - ||mu_L - hat_mu_L||_2^2 - ||Sigma_L^{1/2} - hat_Sigma_L^{1/2}||_2^2 >= 0\n",
    "    constraint_L = epsilon**2 - (torch.norm(mu_L - hat_mu_L)**2) - (torch.norm(oput.sqrtm_svd(Sigma_L) - oput.sqrtm_svd(hat_Sigma_L))**2)\n",
    "    \n",
    "    # Constraint 2: delta^2 - ||mu_H - hat_mu_H||_2^2 - ||Sigma_H^{1/2} - hat_Sigma_H^{1/2}||_2^2 >= 0\n",
    "    constraint_H = delta**2 - (torch.norm(mu_H - hat_mu_H)**2) - (torch.norm(oput.sqrtm_svd(Sigma_H) - oput.sqrtm_svd(hat_Sigma_H))**2)\n",
    "    \n",
    "    # Return whether constraints are satisfied (i.e., >= 0) and the constraint violations\n",
    "    return constraint_L, constraint_H\n",
    "\n",
    "\n",
    "def enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "    constraint_L, constraint_H = check_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "    \n",
    "    # Clip values if constraints are violated\n",
    "    if constraint_L < 0:\n",
    "        #print(f\"Constraint for mu_L and Sigma_L violated. Fixing...\")\n",
    "        mu_L = hat_mu_L + torch.clamp(mu_L - hat_mu_L, min=-epsilon, max=epsilon)\n",
    "        Sigma_L = hat_Sigma_L + torch.clamp(Sigma_L - hat_Sigma_L, min=-epsilon, max=epsilon)\n",
    "    \n",
    "    if constraint_H < 0:\n",
    "        #print(f\"Constraint for mu_H and Sigma_H violated. Fixing...\")\n",
    "        mu_H = hat_mu_H + torch.clamp(mu_H - hat_mu_H, min=-delta, max=delta)\n",
    "        Sigma_H = hat_Sigma_H + torch.clamp(Sigma_H - hat_Sigma_H, min=-delta, max=delta)\n",
    "    \n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db35b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max, epsilon, delta, seed):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    for t in range(num_steps_max): \n",
    "        mu_L         = update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "        mu_H         = update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "        Sigma_L_half = update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "        Sigma_L      = update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "        Sigma_H_half = update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "        Sigma_H      = update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "        \n",
    "        mu_L, Sigma_L, mu_H, Sigma_H = enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "        \n",
    "        obj = 0\n",
    "        \n",
    "        for i, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "            V_i = T @ L_i.float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "                        \n",
    "            L_i_mu_L = V_i @ mu_L\n",
    "            H_i_mu_H = H_i @ mu_H\n",
    "            term1 = torch.norm(L_i_mu_L.float() - H_i_mu_H.float())**2\n",
    "            \n",
    "            V_Sigma_V = V_i.float() @ Sigma_L.float() @ V_i.T.float()\n",
    "            H_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "\n",
    "            term2 = torch.trace(V_Sigma_V)\n",
    "            term3 = torch.trace(H_Sigma_H)\n",
    "            \n",
    "            sqrtVSV = oput.sqrtm_svd(V_Sigma_V)\n",
    "            sqrtHSH = oput.sqrtm_svd(H_Sigma_H)\n",
    "\n",
    "            term4 = -2*torch.norm(oput.sqrtm_svd(sqrtVSV) @ oput.sqrtm_svd(sqrtHSH), 'nuc')\n",
    "            \n",
    "            obj = obj + (term1 + term2 + term3 + term4)\n",
    "        \n",
    "        obj = obj/i\n",
    "        \n",
    "        #print(f\"Max step {t+1}/{num_steps_max}, Objective: {obj.item()}\")\n",
    "\n",
    "    return obj, mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "def optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    objective_T = 0 # Reset objective at the start of each step\n",
    "    for step in range(num_steps_min):\n",
    "        objective_T = 0  # Reset objective at the start of each step\n",
    "        for n, iota in enumerate(Ill):\n",
    "            L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()\n",
    "            H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "            L_i_mu_L = L_i @ mu_L  \n",
    "            H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "            term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "            term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "            term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "            \n",
    "            L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "            H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "\n",
    "            term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "            objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "        objective_T = objective_T/n\n",
    "\n",
    "        optimizer_T.zero_grad() # Clear previous gradients\n",
    "        objective_T.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "        optimizer_T.step()      # Update T using the optimizer\n",
    "\n",
    "        #print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "\n",
    "    return objective_T, T  # Return both the objective and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a99b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, \n",
    "                     hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                     epsilon, delta, lambda_L, lambda_H, lambda_param, \n",
    "                     eta, max_iter, num_steps_min, num_steps_max, tol_max, tol, seed):\n",
    "    \n",
    "    j = 0\n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    T           = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "    optimizer_T = torch.optim.Adam([T], lr=0.01)\n",
    "\n",
    "    previous_objective = float('inf')  # Initialize with a large number\n",
    "    previous_objective_theta = float('inf')  # Initialize with a large number\n",
    "    objective_theta = torch.tensor(float('inf'))\n",
    "    for epoch in range(max_iter):\n",
    "        #print('##########################################')\n",
    "        print(f\"Epoch {epoch+1}/{max_iter}\\n\")\n",
    "        #print(\"MINIMIZING T\")\n",
    "\n",
    "        # ---- Minimize T ----\n",
    "        objective_T, T = optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed)\n",
    "        \n",
    "        print()\n",
    "        #print(\"MAX mu_L, Sigma_L, mu_H, Sigma_H\")\n",
    "\n",
    "        if not abs(previous_objective_theta - objective_theta.item()) < tol_max:\n",
    "            objective_theta, mu_L, Sigma_L, mu_H, Sigma_H = optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H,\n",
    "                                                                         lambda_L, lambda_H, lambda_param, eta, num_steps_max, epsilon, delta, seed)\n",
    "        else:\n",
    "            if j==0:\n",
    "                print('MAX step skipped')\n",
    "                print(mu_L, Sigma_L, mu_H, Sigma_H)\n",
    "                j+=1\n",
    "            \n",
    "        previous_objective_theta = objective_theta.item()\n",
    "        # Check for convergence by comparing the difference in objective values\n",
    "        criterion = abs(previous_objective - objective_T.item())\n",
    "        print(f\"Objective difference: {criterion}\")\n",
    "        \n",
    "        if criterion < tol:\n",
    "            print(f\"Convergence reached at epoch {epoch+1} with objective {objective_T.item()}\")\n",
    "            break\n",
    "\n",
    "        # Update previous objective for the next check\n",
    "        previous_objective = objective_T.item()\n",
    "        #print('##########################################')\n",
    "\n",
    "    print(\"Final T:\", T)\n",
    "    print(\"Final mu_L:\", mu_L)\n",
    "    print(\"Final Sigma_L:\", Sigma_L)\n",
    "    print(\"Final mu_H:\", mu_H)\n",
    "    print(\"Final Sigma_H:\", Sigma_H)\n",
    "\n",
    "    return mu_L, Sigma_L, mu_H, Sigma_H, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3913bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "hat_Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "hat_mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "hat_Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = hat_mu_L.shape[0]\n",
    "h = hat_mu_H.shape[0]\n",
    "\n",
    "\n",
    "# Gelbrich initialization\n",
    "ll_moments      = mut.sample_moments_U(mu_hat = mu_U_ll_hat, Sigma_hat = Sigma_U_ll_hat, bound = epsilon, num_envs = 1)\n",
    "mu_L0, Sigma_L0 = ll_moments[0]\n",
    "#mu_L0, Sigma_L0 = torch.from_numpy(mu_L0), torch.from_numpy(Sigma_L0)\n",
    "\n",
    "hl_moments      = mut.sample_moments_U(mu_hat = mu_U_hl_hat, Sigma_hat = Sigma_U_hl_hat, bound = delta, num_envs = 1)\n",
    "mu_H0, Sigma_H0 = hl_moments[0]\n",
    "#mu_H0, Sigma_H0 = torch.from_numpy(mu_H0), torch.from_numpy(Sigma_H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6458c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "504a3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\n",
      "\n",
      "Objective difference: inf\n",
      "Epoch 2/500\n",
      "\n",
      "\n",
      "MAX step skipped\n",
      "tensor([-0.1889,  0.3310,  0.0501], grad_fn=<AddBackward0>) tensor([[0.4787, 0.0000, 0.0000],\n",
      "        [0.0000, 1.4741, 0.0000],\n",
      "        [0.0000, 0.0000, 1.4901]], grad_fn=<AddBackward0>) tensor([0.4611, 0.4914], grad_fn=<AddBackward0>) tensor([[1.3855, 0.0000],\n",
      "        [0.0000, 1.4701]], grad_fn=<AddBackward0>)\n",
      "Objective difference: 1.6845197677612305\n",
      "Epoch 3/500\n",
      "\n",
      "\n",
      "Objective difference: 0.19249367713928223\n",
      "Epoch 4/500\n",
      "\n",
      "\n",
      "Objective difference: 0.17441654205322266\n",
      "Epoch 5/500\n",
      "\n",
      "\n",
      "Objective difference: 0.16641926765441895\n",
      "Epoch 6/500\n",
      "\n",
      "\n",
      "Objective difference: 0.16215252876281738\n",
      "Epoch 7/500\n",
      "\n",
      "\n",
      "Objective difference: 0.158613920211792\n",
      "Epoch 8/500\n",
      "\n",
      "\n",
      "Objective difference: 0.15427398681640625\n",
      "Epoch 9/500\n",
      "\n",
      "\n",
      "Objective difference: 0.14841902256011963\n",
      "Epoch 10/500\n",
      "\n",
      "\n",
      "Objective difference: 0.14085781574249268\n",
      "Epoch 11/500\n",
      "\n",
      "\n",
      "Objective difference: 0.1317894458770752\n",
      "Epoch 12/500\n",
      "\n",
      "\n",
      "Objective difference: 0.12165462970733643\n",
      "Epoch 13/500\n",
      "\n",
      "\n",
      "Objective difference: 0.11097729206085205\n",
      "Epoch 14/500\n",
      "\n",
      "\n",
      "Objective difference: 0.1002054214477539\n",
      "Epoch 15/500\n",
      "\n",
      "\n",
      "Objective difference: 0.08965754508972168\n",
      "Epoch 16/500\n",
      "\n",
      "\n",
      "Objective difference: 0.07951271533966064\n",
      "Epoch 17/500\n",
      "\n",
      "\n",
      "Objective difference: 0.06985616683959961\n",
      "Epoch 18/500\n",
      "\n",
      "\n",
      "Objective difference: 0.060770392417907715\n",
      "Epoch 19/500\n",
      "\n",
      "\n",
      "Objective difference: 0.05235069990158081\n",
      "Epoch 20/500\n",
      "\n",
      "\n",
      "Objective difference: 0.04470092058181763\n",
      "Epoch 21/500\n",
      "\n",
      "\n",
      "Objective difference: 0.037931740283966064\n",
      "Epoch 22/500\n",
      "\n",
      "\n",
      "Objective difference: 0.032096922397613525\n",
      "Epoch 23/500\n",
      "\n",
      "\n",
      "Objective difference: 0.027205467224121094\n",
      "Epoch 24/500\n",
      "\n",
      "\n",
      "Objective difference: 0.02321094274520874\n",
      "Epoch 25/500\n",
      "\n",
      "\n",
      "Objective difference: 0.02001368999481201\n",
      "Epoch 26/500\n",
      "\n",
      "\n",
      "Objective difference: 0.017496585845947266\n",
      "Epoch 27/500\n",
      "\n",
      "\n",
      "Objective difference: 0.01552879810333252\n",
      "Epoch 28/500\n",
      "\n",
      "\n",
      "Objective difference: 0.013987541198730469\n",
      "Epoch 29/500\n",
      "\n",
      "\n",
      "Objective difference: 0.012766420841217041\n",
      "Epoch 30/500\n",
      "\n",
      "\n",
      "Objective difference: 0.011775970458984375\n",
      "Epoch 31/500\n",
      "\n",
      "\n",
      "Objective difference: 0.01095116138458252\n",
      "Epoch 32/500\n",
      "\n",
      "\n",
      "Objective difference: 0.010250449180603027\n",
      "Epoch 33/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00963050127029419\n",
      "Epoch 34/500\n",
      "\n",
      "\n",
      "Objective difference: 0.009075462818145752\n",
      "Epoch 35/500\n",
      "\n",
      "\n",
      "Objective difference: 0.008571326732635498\n",
      "Epoch 36/500\n",
      "\n",
      "\n",
      "Objective difference: 0.008107304573059082\n",
      "Epoch 37/500\n",
      "\n",
      "\n",
      "Objective difference: 0.007677853107452393\n",
      "Epoch 38/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0072784423828125\n",
      "Epoch 39/500\n",
      "\n",
      "\n",
      "Objective difference: 0.006910502910614014\n",
      "Epoch 40/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0065651535987854\n",
      "Epoch 41/500\n",
      "\n",
      "\n",
      "Objective difference: 0.006245076656341553\n",
      "Epoch 42/500\n",
      "\n",
      "\n",
      "Objective difference: 0.005943775177001953\n",
      "Epoch 43/500\n",
      "\n",
      "\n",
      "Objective difference: 0.005665957927703857\n",
      "Epoch 44/500\n",
      "\n",
      "\n",
      "Objective difference: 0.005405306816101074\n",
      "Epoch 45/500\n",
      "\n",
      "\n",
      "Objective difference: 0.005159318447113037\n",
      "Epoch 46/500\n",
      "\n",
      "\n",
      "Objective difference: 0.004930973052978516\n",
      "Epoch 47/500\n",
      "\n",
      "\n",
      "Objective difference: 0.004714667797088623\n",
      "Epoch 48/500\n",
      "\n",
      "\n",
      "Objective difference: 0.004510402679443359\n",
      "Epoch 49/500\n",
      "\n",
      "\n",
      "Objective difference: 0.004322171211242676\n",
      "Epoch 50/500\n",
      "\n",
      "\n",
      "Objective difference: 0.004141569137573242\n",
      "Epoch 51/500\n",
      "\n",
      "\n",
      "Objective difference: 0.003973722457885742\n",
      "Epoch 52/500\n",
      "\n",
      "\n",
      "Objective difference: 0.003817737102508545\n",
      "Epoch 53/500\n",
      "\n",
      "\n",
      "Objective difference: 0.003668665885925293\n",
      "Epoch 54/500\n",
      "\n",
      "\n",
      "Objective difference: 0.003529667854309082\n",
      "Epoch 55/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0034006237983703613\n",
      "Epoch 56/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0032787322998046875\n",
      "Epoch 57/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00316542387008667\n",
      "Epoch 58/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0030587315559387207\n",
      "Epoch 59/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0029562711715698242\n",
      "Epoch 60/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0028630495071411133\n",
      "Epoch 61/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002776026725769043\n",
      "Epoch 62/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0026901960372924805\n",
      "Epoch 63/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0026149749755859375\n",
      "Epoch 64/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0025394558906555176\n",
      "Epoch 65/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0024707913398742676\n",
      "Epoch 66/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0024067163467407227\n",
      "Epoch 67/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0023443102836608887\n",
      "Epoch 68/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0022864937782287598\n",
      "Epoch 69/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002232193946838379\n",
      "Epoch 70/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00218123197555542\n",
      "Epoch 71/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0021317601203918457\n",
      "Epoch 72/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0020885467529296875\n",
      "Epoch 73/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0020464658737182617\n",
      "Epoch 74/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0020064711570739746\n",
      "Epoch 75/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001970648765563965\n",
      "Epoch 76/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0019349455833435059\n",
      "Epoch 77/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0019034147262573242\n",
      "Epoch 78/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018717050552368164\n",
      "Epoch 79/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018467903137207031\n",
      "Epoch 80/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018187165260314941\n",
      "Epoch 81/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001796424388885498\n",
      "Epoch 82/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017735958099365234\n",
      "Epoch 83/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017546415328979492\n",
      "Epoch 84/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017334818840026855\n",
      "Epoch 85/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017189383506774902\n",
      "Epoch 86/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017037391662597656\n",
      "Epoch 87/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016902685165405273\n",
      "Epoch 88/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016768574714660645\n",
      "Epoch 89/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016676783561706543\n",
      "Epoch 90/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016568303108215332\n",
      "Epoch 91/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016508102416992188\n",
      "Epoch 92/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016436576843261719\n",
      "Epoch 93/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016379356384277344\n",
      "Epoch 94/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016336441040039062\n",
      "Epoch 95/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016330480575561523\n",
      "Epoch 96/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001629650592803955\n",
      "Epoch 97/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016293525695800781\n",
      "Epoch 98/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016307830810546875\n",
      "Epoch 99/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016325116157531738\n",
      "Epoch 100/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016353428363800049\n",
      "Epoch 101/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016407966613769531\n",
      "Epoch 102/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001644819974899292\n",
      "Epoch 103/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016502141952514648\n",
      "Epoch 104/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016576945781707764\n",
      "Epoch 105/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016685426235198975\n",
      "Epoch 106/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016779899597167969\n",
      "Epoch 107/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016865730285644531\n",
      "Epoch 108/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0016997456550598145\n",
      "Epoch 109/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017105937004089355\n",
      "Epoch 110/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017251968383789062\n",
      "Epoch 111/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017403662204742432\n",
      "Epoch 112/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017563700675964355\n",
      "Epoch 113/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0017732679843902588\n",
      "Epoch 114/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001792997121810913\n",
      "Epoch 115/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018113255500793457\n",
      "Epoch 116/500\n",
      "\n",
      "\n",
      "Objective difference: 0.001830756664276123\n",
      "Epoch 117/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018548071384429932\n",
      "Epoch 118/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0018785595893859863\n",
      "Epoch 119/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0019017159938812256\n",
      "Epoch 120/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0019268989562988281\n",
      "Epoch 121/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0019549429416656494\n",
      "Epoch 122/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00198325514793396\n",
      "Epoch 123/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002013593912124634\n",
      "Epoch 124/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0020450353622436523\n",
      "Epoch 125/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0020765364170074463\n",
      "Epoch 126/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00211372971534729\n",
      "Epoch 127/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002147495746612549\n",
      "Epoch 128/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002188384532928467\n",
      "Epoch 129/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0022267401218414307\n",
      "Epoch 130/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0022679269313812256\n",
      "Epoch 131/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002312004566192627\n",
      "Epoch 132/500\n",
      "\n",
      "\n",
      "Objective difference: 0.00235748291015625\n",
      "Epoch 133/500\n",
      "\n",
      "\n",
      "Objective difference: 0.002406388521194458\n",
      "Epoch 134/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0024556219577789307\n",
      "Epoch 135/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0025094151496887207\n",
      "Epoch 136/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0025643110275268555\n",
      "Epoch 137/500\n",
      "\n",
      "\n",
      "Objective difference: 0.0026227235794067383\n",
      "Epoch 138/500\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m mu_H    \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mu_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      4\u001b[0m Sigma_H \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(Sigma_H0)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 6\u001b[0m mu_L, Sigma_L, mu_H, Sigma_H, T \u001b[38;5;241m=\u001b[39m optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, \n\u001b[1;32m      7\u001b[0m                                                     LLmodels, HLmodels, \n\u001b[1;32m      8\u001b[0m                                                     hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n\u001b[1;32m      9\u001b[0m                                                     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, lambda_L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lambda_H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lambda_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, \n\u001b[1;32m     10\u001b[0m                                                     eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,  num_steps_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_steps_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, tol_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m, in \u001b[0;36moptimize_min_max\u001b[0;34m(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, lambda_param, eta, max_iter, num_steps_min, num_steps_max, tol_max, tol, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#print(\"MINIMIZING T\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ---- Minimize T ----\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m objective_T, T \u001b[38;5;241m=\u001b[39m optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#print(\"MAX mu_L, Sigma_L, mu_H, Sigma_H\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 75\u001b[0m, in \u001b[0;36moptimize_min\u001b[0;34m(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T, seed)\u001b[0m\n\u001b[1;32m     72\u001b[0m objective_T \u001b[38;5;241m=\u001b[39m objective_T\u001b[38;5;241m/\u001b[39mn\n\u001b[1;32m     74\u001b[0m optimizer_T\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m objective_T\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Backpropagate to compute gradients\u001b[39;00m\n\u001b[1;32m     76\u001b[0m optimizer_T\u001b[38;5;241m.\u001b[39mstep()      \u001b[38;5;66;03m# Update T using the optimizer\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/erica/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mu_L    = torch.from_numpy(mu_L0).float()\n",
    "Sigma_L = torch.from_numpy(Sigma_L0).float()\n",
    "mu_H    = torch.from_numpy(mu_H0).float()\n",
    "Sigma_H = torch.from_numpy(Sigma_H0).float()\n",
    "\n",
    "mu_L, Sigma_L, mu_H, Sigma_H, T = optimize_min_max(mu_L, Sigma_L, mu_H, Sigma_H, \n",
    "                                                    LLmodels, HLmodels, \n",
    "                                                    hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, \n",
    "                                                    epsilon=0.5, delta=0.5, lambda_L=0.8, lambda_H=0.7, lambda_param=0.9, \n",
    "                                                    eta=0.01, max_iter=500,  num_steps_min=5, num_steps_max=5, tol_max=1e-5, tol=1e-5, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06cfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad31497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a76ea3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
