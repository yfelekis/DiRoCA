{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import modularised_utils as mut\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opt_utils as oput\n",
    "\n",
    "import Linear_Additive_Noise_Models as lanm\n",
    "import operations as ops\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import params\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'synth1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the radius of the Wasserstein balls (epsilon, delta) and the size for both models.\n",
    "epsilon, delta           = params.radius[experiment]\n",
    "ll_num_envs, hl_num_envs = params.n_envs[experiment]\n",
    "\n",
    "# Define the number of samples per environment. Currently every environment has the same number of samples\n",
    "num_llsamples, num_hlsamples  = params.n_samples[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dll      = mut.load_samples(experiment)[None][0] \n",
    "Gll, Ill = mut.load_model(experiment, 'LL')\n",
    "\n",
    "Dhl      = mut.load_samples(experiment)[None][1] \n",
    "Ghl, Ihl = mut.load_model(experiment, 'HL')\n",
    "\n",
    "omega    = mut.load_omega_map(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9fb9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_coeffs = mut.get_coefficients(Dll, Gll)\n",
    "hl_coeffs = mut.get_coefficients(Dhl, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e42545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [Not suggested] In case we want to explore also the interventional --> worse estimation!\n",
    "# Dlls, Dhls = [], []\n",
    "# for dpair in list(mut.load_samples(experiment).values()):\n",
    "#     Dlls.append(dpair[0])\n",
    "#     Dhls.append(dpair[1])\n",
    "    \n",
    "# ll_coeffs = mut.get_coefficients(Dlls, Gll)\n",
    "# hl_coeffs = mut.get_coefficients(Dhls, Ghl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75470de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_ll_hat, mu_U_ll_hat, Sigma_U_ll_hat = mut.lan_abduction(Dll, Gll, ll_coeffs)\n",
    "U_hl_hat, mu_U_hl_hat, Sigma_U_hl_hat = mut.lan_abduction(Dhl, Ghl, hl_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e18c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLmodels = {}\n",
    "for iota in Ill:\n",
    "    LLmodels[iota] = lanm.LinearAddSCM(Gll, ll_coeffs, iota)\n",
    "    \n",
    "HLmodels, Dhl_samples = {}, {}\n",
    "for eta in Ihl:\n",
    "    HLmodels[eta] = lanm.LinearAddSCM(Ghl, hl_coeffs, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "838bcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_U_ll_hat    = torch.from_numpy(np.array([0, 0, 0])).float()  \n",
    "Sigma_U_ll_hat = torch.from_numpy(np.diag([1, 2, 1])).float() \n",
    "\n",
    "mu_U_hl_hat    = torch.from_numpy(np.array([0, 0])).float()  \n",
    "Sigma_U_hl_hat = torch.from_numpy(np.diag([1, 1])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1907803",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_L    = mu_U_ll_hat\n",
    "Sigma_L = Sigma_U_ll_hat\n",
    "\n",
    "mu_H    = mu_U_hl_hat\n",
    "Sigma_H = Sigma_U_hl_hat\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "lambda_L =.2\n",
    "lambda_H =.3\n",
    "eta      = .01\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae33f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_L    = torch.from_numpy(mu_U_ll_hat).float()\n",
    "Sigma_L = torch.from_numpy(Sigma_U_ll_hat).float()\n",
    "\n",
    "mu_H    = torch.from_numpy(mu_U_hl_hat).float()\n",
    "Sigma_H = torch.from_numpy(Sigma_U_hl_hat).float()\n",
    "\n",
    "l = mu_L.shape[0]\n",
    "h = mu_H.shape[0]\n",
    "\n",
    "lambda_L =.2\n",
    "lambda_H =.3\n",
    "eta      = .01\n",
    "max_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5e43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the necessary functions using PyTorch for automatic differentiation\n",
    "# def F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta):\n",
    "#     term1 = 0\n",
    "#     term2 = 0\n",
    "#     term3 = 0\n",
    "\n",
    "#     # Loop to compute the sum of terms\n",
    "#     for n, iota in enumerate(Ill):\n",
    "#         L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  # Convert to float32\n",
    "#         V_i = T @ L_i  # Matrix multiplication, ensure V_i is float32\n",
    "#         H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  # Convert to float32\n",
    "\n",
    "#         term1 += torch.norm(torch.matmul(V_i, mu_L) - torch.matmul(H_i, mu_H))**2 + torch.trace(torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))) + torch.trace(torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T)))\n",
    "\n",
    "#     term2 = lambda_L * (epsilon**2 - torch.norm(mu_L - hat_mu_L)**2 - torch.norm(sqrtm_svd(Sigma_L) - sqrtm_svd(hat_Sigma_L))**2)\n",
    "#     term3 = lambda_H * (delta**2 - torch.norm(mu_H - hat_mu_H)**2 - torch.norm(sqrtm_svd(Sigma_H) - sqrtm_svd(hat_Sigma_H))**2)\n",
    "\n",
    "#     return term1 / n + term2 + term3\n",
    "\n",
    "# # Proximal operator for Sigma_L (using soft-thresholding)\n",
    "# def prox_Sigma_L(Sigma_L, lambda_L, LLmodels, HLmodels, Sigma_H):\n",
    "#     # Using the Frobenius norm as a soft-thresholding operator for Sigma_L\n",
    "#     prox = torch.zeros_like(Sigma_L)\n",
    "#     for n, iota in enumerate(Ill):\n",
    "#         L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "#         V_i = T @ L_i  \n",
    "#         H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "\n",
    "#         V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "#         sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "#         prox_i          = prox_operator(sqrtm_V_Sigma_V, lambda_L)\n",
    "#         ll_term         = torch.linalg.pinv(V_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.pinv(V_i).T\n",
    "\n",
    "#         H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "#         sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "#         hl_term         = torch.norm(sqrtm_H_Sigma_H, p='fro') \n",
    "       \n",
    "#         prox += ll_term * hl_term\n",
    "\n",
    "#     prox *= (2 / n)\n",
    "#     prox = diagonalize(prox)\n",
    "#     return prox\n",
    "\n",
    "# # Proximal operator for Sigma_H (using soft-thresholding)\n",
    "# def prox_Sigma_H(Sigma_H, lambda_H, LLmodels, HLmodels, Sigma_L):\n",
    "#     prox = torch.zeros_like(Sigma_H)\n",
    "#     for n, iota in enumerate(Ill):\n",
    "#         L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()  \n",
    "#         V_i = T @ L_i  \n",
    "#         H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()  \n",
    "       \n",
    "#         H_Sigma_H       = torch.matmul(H_i, torch.matmul(Sigma_H, H_i.T))\n",
    "#         sqrtm_H_Sigma_H = sqrtm_svd(H_Sigma_H)\n",
    "#         prox_i          = prox_operator(sqrtm_H_Sigma_H, lambda_H)\n",
    "#         hl_term         = torch.linalg.inv(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.linalg.inv(H_i).T\n",
    "#         #hl_term        = torch.inverse(H_i) @ torch.matmul(prox_i, prox_i.T) @ torch.inverse(H_i).T\n",
    "\n",
    "#         V_Sigma_V       = torch.matmul(V_i, torch.matmul(Sigma_L, V_i.T))\n",
    "#         sqrtm_V_Sigma_V = sqrtm_svd(V_Sigma_V)\n",
    "#         ll_term         = torch.norm(sqrtm_V_Sigma_V, p='fro') \n",
    "        \n",
    "#         prox     += ll_term * hl_term\n",
    "\n",
    "#     prox *= (2 / n)\n",
    "\n",
    "#     prox = diagonalize(prox)\n",
    "#     return prox\n",
    "\n",
    "# # Proximal operator of a matrix frobenious norm\n",
    "# def prox_operator(A, lambda_param):\n",
    "#     frobenius_norm = torch.norm(A, p='fro')\n",
    "#     scaling_factor = torch.max(1 - lambda_param / frobenius_norm, torch.zeros_like(frobenius_norm))\n",
    "#     return scaling_factor * A\n",
    "\n",
    "# def diagonalize(A):\n",
    "#     # Get eigenvalues and eigenvectors\n",
    "#     eigvals, eigvecs = torch.linalg.eig(A)  \n",
    "#     eigvals_real     = eigvals.real  \n",
    "#     eigvals_real     = torch.sqrt(eigvals_real)  # Take the square root of the eigenvalues\n",
    "\n",
    "#     return torch.diag(eigvals_real)\n",
    "\n",
    "# def sqrtm_svd(A):\n",
    "#     # Compute the SVD of A\n",
    "#     U, S, V = torch.svd(A)\n",
    "    \n",
    "#     # Take the square root of the singular values\n",
    "#     S_sqrt = torch.sqrt(torch.clamp(S, min=0.0))  # Ensure non-negative singular values\n",
    "    \n",
    "#     # Reconstruct the square root matrix\n",
    "#     sqrt_A = U @ torch.diag(S_sqrt) @ V.T\n",
    "    \n",
    "#     return sqrt_A\n",
    "\n",
    "# def sqrtm_eig(A):\n",
    "#     eigvals, eigvecs = torch.linalg.eig(A)\n",
    "#     eigvals_real = eigvals.real\n",
    "    \n",
    "#     # Ensure eigenvalues are non-negative for the square root to be valid\n",
    "#     eigvals_sqrt = torch.sqrt(torch.clamp(eigvals_real, min=0.0))  # Square root of non-negative eigenvalues\n",
    "\n",
    "#     # Reconstruct the square root of the matrix using the eigenvectors\n",
    "#     # Make sure the eigenvectors are also real\n",
    "#     eigvecs_real = eigvecs.real\n",
    "    \n",
    "#     # Reconstruct the matrix square root\n",
    "#     sqrt_A = eigvecs_real @ torch.diag(eigvals_sqrt) @ eigvecs_real.T\n",
    "    \n",
    "#     return sqrt_A\n",
    "\n",
    "\n",
    "# # Optimization loop using autograd and PyProximal (maximize using gradient ascent)\n",
    "# def optimize(LLmodels, HLmodels, mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta, lambda_L, lambda_H, eta, max_iter):\n",
    "#     mu_L.requires_grad_(True)  # Enable autograd for mu_L\n",
    "#     Sigma_L_half.requires_grad_(True)  # Enable autograd for Sigma_L\n",
    "#     mu_H.requires_grad_(True)  # Enable autograd for mu_H\n",
    "#     Sigma_H_half.requires_grad_(True)  # Enable autograd for Sigma_H\n",
    "\n",
    "#     for t in range(max_iter):\n",
    "#         print(f\"Iteration {t}\")\n",
    "        \n",
    "#         objective = F_func(mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, lambda_L, lambda_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "#         objective.backward()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             mu_L += eta * mu_L.grad  # Ascent for mu_L\n",
    "#             mu_H += eta * mu_H.grad  # Ascent for mu_H\n",
    "            \n",
    "#             print(f\"Sigma_L: {Sigma_L.grad}\")\n",
    "#             Sigma_L_half += eta * Sigma_L.grad  # Ascent for Sigma_L\n",
    "#             Sigma_H += eta * Sigma_H.grad  # Ascent for Sigma_H\n",
    "#             Sigma_L = prox_Sigma_L(Sigma_L_half, lambda_L, LLmodels, HLmodels, Sigma_H)\n",
    "#             print(Sigma_L)  \n",
    "#             Sigma_H = prox_Sigma_H(Sigma_H_half, lambda_H, LLmodels, HLmodels, Sigma_L)\n",
    "            \n",
    "#             #Zero the gradients after the update\n",
    "#             mu_L.grad.zero_()\n",
    "#             mu_H.grad.zero_()\n",
    "#             Sigma_L.grad.zero_()\n",
    "#             Sigma_H.grad.zero_()\n",
    "\n",
    "#             # if mu_L.grad is not None:\n",
    "#             #     mu_L.grad.zero_()\n",
    "#             # if mu_H.grad is not None:\n",
    "#             #     mu_H.grad.zero_()\n",
    "#             # if Sigma_L.grad is not None:\n",
    "#             #     Sigma_L.grad.zero_()\n",
    "#             # if Sigma_H.grad is not None:\n",
    "#             #     Sigma_H.grad.zero_()\n",
    "\n",
    "#         # Print progress\n",
    "#         if t % 10 == 0:\n",
    "#             print(f\"Iteration {t}, Objective Value: {objective.item()}\")\n",
    "\n",
    "#     return mu_L, Sigma_L, mu_H, Sigma_H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125e88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_max(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, lambda_L, lambda_H, lambda_param, eta, num_steps_max):\n",
    "\n",
    "#     for t in range(num_steps_max): \n",
    "#         #print('mu_L before update:', mu_L)\n",
    "#         mu_L         = update_mu_L(T, mu_L, mu_H, LLmodels, HLmodels, lambda_L, hat_mu_L, eta)\n",
    "#         # print('mu_L after update:', mu_L)\n",
    "#         # print('mu_H before update:', mu_H)\n",
    "#         mu_H         = update_mu_H(T, mu_L, mu_H, LLmodels, HLmodels, lambda_H, hat_mu_H, eta)\n",
    "#         # print('mu_H after update:', mu_H)\n",
    "\n",
    "#         # print('Sigma_L before update:', Sigma_L)\n",
    "#         Sigma_L_half = update_Sigma_L_half(T, Sigma_L, LLmodels, lambda_L, hat_Sigma_L, eta)\n",
    "#         Sigma_L      = update_Sigma_L(T, Sigma_L_half, LLmodels, Sigma_H, HLmodels, lambda_param)\n",
    "#         # print('Sigma_L after update:', Sigma_L)\n",
    "        \n",
    "#         # print('Sigma_H before update:', Sigma_H)\n",
    "#         Sigma_H_half = update_Sigma_H_half(T, Sigma_H, HLmodels, lambda_H, hat_Sigma_H, eta)\n",
    "#         Sigma_H      = update_Sigma_H(T, Sigma_H_half, LLmodels, Sigma_L, HLmodels, lambda_param)\n",
    "#         # print('Sigma_H after update:', Sigma_H)\n",
    "        \n",
    "#         mu_L, Sigma_L, mu_H, Sigma_H = enforce_constraints(mu_L, Sigma_L, mu_H, Sigma_H, hat_mu_L, hat_Sigma_L, hat_mu_H, hat_Sigma_H, epsilon, delta)\n",
    "#         # print('mu_L after constraints:', mu_L)\n",
    "#         # print('Sigma_L after constraints:', Sigma_L)\n",
    "#         # print('mu_H after constraints:', mu_H)\n",
    "#         # print('Sigma_H after constraints:', Sigma_H)\n",
    "#         # print( )\n",
    "#         # Compute the objective function for the current iteration\n",
    "#         obj = 0\n",
    "        \n",
    "#         for i, iota in enumerate(Ill):\n",
    "#             L_i = torch.from_numpy(LLmodels[iota].compute_mechanism())\n",
    "#             V_i = T @ L_i.float()\n",
    "#             H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "                        \n",
    "#             L_i_mu_L = V_i @ mu_L\n",
    "#             H_i_mu_H = H_i @ mu_H\n",
    "#             term1 = torch.norm(L_i_mu_L.float() - H_i_mu_H.float())**2\n",
    "            \n",
    "#             V_Sigma_V = V_i.float() @ Sigma_L.float() @ V_i.T.float()\n",
    "#             H_Sigma_H = H_i.float() @ Sigma_H.float() @ H_i.T.float()\n",
    "\n",
    "#             term2 = torch.trace(V_Sigma_V)\n",
    "#             term3 = torch.trace(H_Sigma_H)\n",
    "            \n",
    "#             sqrtVSV = oput.sqrtm_svd(V_Sigma_V)\n",
    "#             sqrtHSH = oput.sqrtm_svd(H_Sigma_H)\n",
    "\n",
    "#             #term4 = -2*torch.trace(oput.sqrtm_svd(sqrtHSH @ V_Sigma_V @ sqrtHSH))\n",
    "#             term4 = -2*torch.norm(oput.sqrtm_svd(sqrtVSV) @ oput.sqrtm_svd(sqrtHSH), 'nuc')\n",
    "            \n",
    "#             obj = obj + (term1 + term2 + term3 + term4)\n",
    "        \n",
    "#         obj = obj/i\n",
    "        \n",
    "#         print(f\"Max step {t+1}/{num_steps_max}, Objective: {obj.item()}\")\n",
    "\n",
    "#     return mu_L, Sigma_L, mu_H, Sigma_H\n",
    "\n",
    "# def optimize_min(T, mu_L, Sigma_L, mu_H, Sigma_H, LLmodels, HLmodels, num_steps_min, optimizer_T):\n",
    "\n",
    "#     objective_T = 0  # Initialize the objective for this step\n",
    "\n",
    "#     for step in range(num_steps_min):\n",
    "#         objective_T = 0  # Reset objective at the start of each step\n",
    "#         for n, iota in enumerate(Ill):\n",
    "#             L_i = torch.from_numpy(LLmodels[iota].compute_mechanism()).float()\n",
    "#             H_i = torch.from_numpy(HLmodels[omega[iota]].compute_mechanism()).float()\n",
    "\n",
    "#             L_i_mu_L = L_i @ mu_L  \n",
    "#             H_i_mu_H = H_i @ mu_H \n",
    "\n",
    "#             term1 = torch.norm(T @ L_i_mu_L - H_i_mu_H) ** 2\n",
    "#             term2 = torch.trace(T @ L_i @ Sigma_L @ L_i.T @ T.T)\n",
    "#             term3 = torch.trace(H_i @ Sigma_H @ H_i.T)\n",
    "            \n",
    "#             L_i_Sigma_L = T @ L_i @ Sigma_L @ L_i.T @ T.T\n",
    "#             H_i_Sigma_H = H_i @ Sigma_H @ H_i.T\n",
    "\n",
    "#             # Using the SVD square root term\n",
    "#             term4 = -2 * torch.norm(oput.sqrtm_svd(L_i_Sigma_L) @ oput.sqrtm_svd(H_i_Sigma_H), 'nuc')\n",
    "\n",
    "#             objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "#         objective_T = objective_T/n\n",
    "\n",
    "#         optimizer_T.zero_grad() # Clear previous gradients\n",
    "#         objective_T.backward(retain_graph=True)  # Backpropagate to compute gradients\n",
    "#         optimizer_T.step()      # Update T using the optimizer\n",
    "\n",
    "#         print(f\"Min step {step+1}/{num_steps_min}, Objective: {objective_T.item()}\")\n",
    "\n",
    "#     return objective_T, T  # Return both the objective and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddacff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_struc_matrices(models, I):\n",
    "    matrices = []\n",
    "    for iota in I:\n",
    "        M_i = torch.from_numpy(models[iota]._compute_reduced_form()).float()  \n",
    "        matrices.append(M_i)\n",
    "\n",
    "    return matrices\n",
    "\n",
    "def compute_mu_bary(struc_matrices, mu):\n",
    "    struc_matrices_tensor = torch.stack(struc_matrices)\n",
    "    mu_barycenter         = torch.sum(struc_matrices_tensor @ mu, dim=0) / len(struc_matrices)\n",
    "\n",
    "    return mu_barycenter\n",
    "\n",
    "def compute_Sigma_bary(matrices, Sigma, initialization, max_iter, tol):\n",
    "\n",
    "    Sigma_matrices = []\n",
    "    for M in matrices:\n",
    "        Sigma_matrices.append(M @ Sigma @ M.T)\n",
    "\n",
    "    return covariance_bary_optim(Sigma_matrices, initialization, max_iter, tol)\n",
    "\n",
    "def covariance_bary_optim(Sigma_list, initialization, max_iter, tol):\n",
    "    \n",
    "    if initialization == 'psd':\n",
    "        S_0 = create_psd_matrix(Sigma_list[0].shape[0])\n",
    "    elif initialization == 'avg':\n",
    "        S_0 = sum(Sigma_list) / len(Sigma_list)\n",
    "    \n",
    "    S_n = S_0.clone()\n",
    "    n   = len(Sigma_list)  # Number of matrices\n",
    "    lambda_j = 1.0 / n   # Equal weights\n",
    "    \n",
    "    for n in range(max_iter):\n",
    "        S_n_old = S_n.clone()\n",
    "\n",
    "        S_n_inv_half = oput.sqrtm_svd(regmat(torch.inverse(S_n)))\n",
    "        \n",
    "        # Compute the sum of S_n^(1/2) Σ_j S_n^(1/2)\n",
    "        sum_term = torch.zeros_like(S_n)\n",
    "        for Sigma_j in Sigma_list:\n",
    "            S_n_half   = oput.sqrtm_svd(regmat(S_n))\n",
    "            inner_term = torch.matmul(torch.matmul(S_n_half, Sigma_j), S_n_half)\n",
    "            sqrt_term  = oput.sqrtm_svd(regmat(inner_term))\n",
    "            sum_term  += lambda_j * sqrt_term\n",
    "        # Square the sum term\n",
    "        squared_sum = torch.matmul(sum_term, sum_term.T)\n",
    "\n",
    "        S_n_next = torch.matmul(torch.matmul(S_n_inv_half, squared_sum), S_n_inv_half)\n",
    "        S_n = S_n_next\n",
    "\n",
    "        if torch.norm(S_n - S_n_old, p='fro') < tol:\n",
    "            print(f\"Converged after {n+1} iterations\")\n",
    "            break\n",
    "            \n",
    "    return S_n\n",
    "\n",
    "def monge(m1, S1, m2, S2):\n",
    "    inner      = torch.matmul(oput.sqrtm_svd(S1), torch.matmul(S2, oput.sqrtm_svd(S1)))\n",
    "    sqrt_inner = oput.sqrtm_svd(inner)\n",
    "    A          = torch.matmul(torch.inverse(oput.sqrtm_svd(regmat(S1))), torch.matmul(sqrt_inner, torch.inverse(oput.sqrtm_svd(regmat(S1)))))  \n",
    "\n",
    "    # Define the Monge map as a function τ(x) = m_2 + A(x - m_1)\n",
    "    def tau(x):\n",
    "        return m2 + A @ (x - m1)\n",
    "\n",
    "    return tau, A\n",
    "\n",
    "def regmat(matrix, eps=1e-10):\n",
    "    # Replace NaN and Inf values with finite numbers\n",
    "    matrix = torch.nan_to_num(matrix, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "    \n",
    "    # Add a small epsilon to the diagonal for numerical stability\n",
    "    if matrix.dim() == 2 and matrix.size(0) == matrix.size(1):\n",
    "        matrix = matrix + eps * torch.eye(matrix.size(0), device=matrix.device)\n",
    "    \n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7850a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_psd_matrix(size):\n",
    "    A = torch.randn(size, size).float()\n",
    "\n",
    "    return torch.matmul(A, A.T)\n",
    "\n",
    "# PCA Projection from higher to lower dimension\n",
    "def pca_projection(Sigma, target_dim):\n",
    "    \"\"\"\n",
    "    Project a d×d matrix to a k×k matrix where k < d\n",
    "    Args:\n",
    "        Sigma: source matrix (d×d)\n",
    "        target_dim: target dimension k\n",
    "    Returns:\n",
    "        k×k projected matrix\n",
    "    \"\"\"\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(Sigma)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = torch.argsort(eigenvalues, descending=True)\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    \n",
    "    # Take only the top target_dim eigenvectors\n",
    "    V = eigenvectors[:, :target_dim]  # d×k matrix\n",
    "    \n",
    "    # Project the covariance matrix\n",
    "    Sigma_projected = torch.matmul(torch.matmul(V.T, Sigma), V)  # k×k matrix\n",
    "    \n",
    "    return Sigma_projected, V\n",
    "\n",
    "# SVD Projection from higher to lower dimension\n",
    "def svd_projection(Sigma, target_dim):\n",
    "    \"\"\"\n",
    "    Project a d×d matrix to a k×k matrix where k < d using SVD\n",
    "    Args:\n",
    "        Sigma: source matrix (d×d)\n",
    "        target_dim: target dimension k\n",
    "    Returns:\n",
    "        k×k projected matrix\n",
    "    \"\"\"\n",
    "    # Perform SVD\n",
    "    U, S, V = torch.svd(Sigma)\n",
    "    \n",
    "    # Take only the first target_dim components\n",
    "    U_k = U[:, :target_dim]  # d×k matrix\n",
    "    S_k = S[:target_dim]     # k singular values\n",
    "    \n",
    "    # Project the covariance matrix\n",
    "    Sigma_projected = torch.matmul(torch.matmul(U_k.T, Sigma), U_k)  # k×k matrix\n",
    "    \n",
    "    return Sigma_projected, U_k\n",
    "\n",
    "def project_covariance(Sigma, n, method):\n",
    "    if method == 'pca':\n",
    "        return pca_projection(Sigma, n)\n",
    "    elif method == 'svd':\n",
    "        return svd_projection(Sigma, n)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown projection method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "739e4f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H, LLmodels, HLmodels, Ill, Ihl, projection_method, initialization, max_iter, tol):\n",
    "\n",
    "    h, l = mu_H.shape[0], mu_L.shape[0]\n",
    "\n",
    "    # Initialize the structural matrices    \n",
    "    L_matrices = compute_struc_matrices(LLmodels, Ill)\n",
    "    H_matrices = compute_struc_matrices(HLmodels, Ihl)\n",
    "\n",
    "    # Initilize the barycenteric means and covariances\n",
    "    print(\"Computing barycentric mu_L\")\n",
    "    mu_bary_L = compute_mu_bary(L_matrices, mu_L)\n",
    "    print(\"mu_bary_L:\", mu_bary_L)  \n",
    "    print(\"\\nComputing barycentric mu_H\")\n",
    "    mu_bary_H = compute_mu_bary(H_matrices, mu_H)\n",
    "    print(\"mu_bary_H:\", mu_bary_H)  \n",
    "\n",
    "    print(\"\\nComputing barycentric Sigma_L\")\n",
    "    Sigma_bary_L = compute_Sigma_bary(L_matrices, Sigma_L, initialization, max_iter, tol)\n",
    "    print(Sigma_bary_L)\n",
    "    print(\"\\nComputing barycentric Sigma_H\")\n",
    "    Sigma_bary_H = compute_Sigma_bary(H_matrices, Sigma_H, initialization, max_iter, tol)\n",
    "    print(Sigma_bary_H)\n",
    "    \n",
    "    proj_Sigma_bary_L, Tp = project_covariance(Sigma_bary_L, h, projection_method)\n",
    "    proj_mu_bary_L        = torch.matmul(Tp.T, mu_bary_L)\n",
    "\n",
    "    tau, A = monge(proj_mu_bary_L, proj_Sigma_bary_L, mu_bary_H, Sigma_bary_H)\n",
    "\n",
    "    T = torch.matmul(A, Tp.T)\n",
    "\n",
    "    return tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e21785be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing barycentric mu_L\n",
      "mu_bary_L: tensor([-0.0068, -0.0107, -0.0002])\n",
      "\n",
      "Computing barycentric mu_H\n",
      "mu_bary_H: tensor([ 0.0043, -0.0086])\n",
      "\n",
      "Computing barycentric Sigma_L\n",
      "Converged after 17 iterations\n",
      "tensor([[1.0397, 0.2818, 0.0300],\n",
      "        [0.2818, 2.0118, 0.2141],\n",
      "        [0.0300, 0.2141, 0.9900]])\n",
      "\n",
      "Computing barycentric Sigma_H\n",
      "Converged after 1 iterations\n",
      "tensor([[1.3537, 0.5822],\n",
      "        [0.5822, 0.9700]])\n"
     ]
    }
   ],
   "source": [
    "tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H = barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H,\n",
    "                                                                                        LLmodels, HLmodels, Ill, Ihl,\n",
    "                                                                                        'svd', 'avg', 100, 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5701cf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0171, -0.7649, -0.2792],\n",
       "        [ 0.6901, -0.3023, -0.5995]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9d01224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_number(matrix):\n",
    "    \"\"\"\n",
    "    Computes the condition number of a matrix using the 2-norm.\n",
    "\n",
    "    Parameters:\n",
    "        matrix (np.ndarray): Input matrix (can be square or rectangular).\n",
    "\n",
    "    Returns:\n",
    "        float: The condition number of the matrix.\n",
    "    \"\"\"\n",
    "    # Compute the singular values of the matrix\n",
    "    singular_values = np.linalg.svd(matrix, compute_uv=False)\n",
    "\n",
    "    # Condition number is the ratio of the largest to smallest singular value\n",
    "    cond_number = singular_values.max() / singular_values.min()\n",
    "\n",
    "    return cond_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef1c3315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7649595"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_number(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "046fa251",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tau  = mut.load_T(experiment)\n",
    "Tau  = torch.from_numpy(Tau).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15c44f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.73872"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_number(Tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0687b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292e446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "596e70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0775,  0.3862])\n",
      "tensor([-0.2403,  0.9754])\n",
      "tensor([ 0.2032, -1.0131])\n",
      "tensor([ 0.0389, -1.6508])\n",
      "tensor([0.4560, 1.6856])\n",
      "tensor([-1.3838,  0.2738])\n",
      "tensor([0.0904, 0.9283])\n",
      "tensor([-0.5994, -0.2864])\n",
      "tensor([-1.1619, -0.5414])\n",
      "tensor([-0.9447, -2.3564])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, l)\n",
    "for x_i in x:\n",
    "    print(T@x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbd34121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6622, -0.9276])\n",
      "tensor([-0.6622, -0.9276])\n"
     ]
    }
   ],
   "source": [
    "y = Tp.T@x[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bac833c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Barycentric Optimization at 2024-12-02 16:20:10.238005\n",
      "Maximum iterations: 100\n",
      "Parameters:\n",
      "- Projection method: svd\n",
      "- Initialization: psd\n",
      "- Tolerance: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Barycentric Optimization:  20%|██        | 20/100 [00:00<00:00, 694.66it/s, stage=Computing barycentric covariances]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing barycentric mu_L\n",
      "mu_bary_L: tensor([0., 0., 0.])\n",
      "\n",
      "Computing barycentric mu_H\n",
      "mu_bary_H: tensor([0., 0.])\n",
      "\n",
      "Computing barycentric Sigma_L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Barycentric Optimization: 100%|██████████| 100/100 [00:01<00:00, 78.40it/s, stage=Finalizing, tau=N/A]              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8460, -0.0126, -1.2985],\n",
      "        [-1.9967, -1.2195, -1.6254],\n",
      "        [-1.0787, -0.1480,  0.2046]])\n",
      "\n",
      "Computing barycentric Sigma_H\n",
      "Converged after 2 iterations\n",
      "tensor([[1.3602, 0.6002],\n",
      "        [0.6002, 1.0000]])\n",
      "\n",
      "Optimization Complete!\n",
      "Started at: 2024-12-02 16:20:10.238005\n",
      "Finished at: 2024-12-02 16:20:11.525951\n",
      "Total execution time: 0:00:01\n",
      "\n",
      "Final Results:\n",
      "tau: <function monge.<locals>.tau at 0x19a759580>\n",
      "T shape: torch.Size([2, 3])\n",
      "Tp shape: torch.Size([3, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "# import torch\n",
    "\n",
    "# def barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H, \n",
    "#                            LLmodels, HLmodels, Ill, Ihl, \n",
    "#                            projection_method, initialization, \n",
    "#                            max_iter, tol, pbar=None):\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     h, l = mu_H.shape[0], mu_L.shape[0]\n",
    "\n",
    "#     # Initialize progress tracking\n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({'stage': 'Initializing'})\n",
    "#         pbar.update(0)\n",
    "\n",
    "#     # Initialize the structural matrices    \n",
    "#     L_matrices = compute_struc_matrices(LLmodels, Ill)\n",
    "#     H_matrices = compute_struc_matrices(HLmodels, Ihl)\n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({'stage': 'Computing barycentric means'})\n",
    "#         pbar.update(10)  # Update progress by 10%\n",
    "\n",
    "#     # Initialize the barycentric means and covariances\n",
    "#     print(\"Computing barycentric mu_L\")\n",
    "#     mu_bary_L = compute_mu_bary(L_matrices, mu_L)\n",
    "#     print(\"mu_bary_L:\", mu_bary_L)  \n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.update(10)\n",
    "\n",
    "#     print(\"\\nComputing barycentric mu_H\")\n",
    "#     mu_bary_H = compute_mu_bary(H_matrices, mu_H)\n",
    "#     print(\"mu_bary_H:\", mu_bary_H)  \n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({'stage': 'Computing barycentric covariances'})\n",
    "#         pbar.update(20)\n",
    "\n",
    "#     print(\"\\nComputing barycentric Sigma_L\")\n",
    "#     Sigma_bary_L = compute_Sigma_bary(L_matrices, Sigma_L, initialization, max_iter, tol)\n",
    "#     print(Sigma_bary_L)\n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.update(20)\n",
    "\n",
    "#     print(\"\\nComputing barycentric Sigma_H\")\n",
    "#     Sigma_bary_H = compute_Sigma_bary(H_matrices, Sigma_H, initialization, max_iter, tol)\n",
    "#     print(Sigma_bary_H)\n",
    "    \n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({'stage': 'Projecting covariance'})\n",
    "#         pbar.update(20)\n",
    "\n",
    "#     proj_Sigma_bary_L, Tp = project_covariance(Sigma_bary_L, h, projection_method)\n",
    "#     proj_mu_bary_L = torch.matmul(Tp.T, mu_bary_L)\n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({'stage': 'Computing Monge map'})\n",
    "#         pbar.update(10)\n",
    "\n",
    "#     tau, A = monge(proj_mu_bary_L, proj_Sigma_bary_L, mu_bary_H, Sigma_bary_H)\n",
    "\n",
    "#     if pbar:\n",
    "#         pbar.set_postfix({\n",
    "#             'stage': 'Finalizing',\n",
    "#             'tau': f'{tau:.4f}' if isinstance(tau, (int, float)) else 'N/A'\n",
    "#         })\n",
    "#         pbar.update(10)\n",
    "\n",
    "#     T = torch.matmul(A, Tp.T)\n",
    "\n",
    "#     return tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H\n",
    "\n",
    "# # Main execution wrapper\n",
    "# def run_barycentric_optimization(mu_L, mu_H, Sigma_L, Sigma_H,\n",
    "#                                LLmodels, HLmodels, Ill, Ihl,\n",
    "#                                projection_method='svd', \n",
    "#                                initialization='avg', \n",
    "#                                max_iter=100, \n",
    "#                                tol=1e-5):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     start_datetime = datetime.now()\n",
    "\n",
    "#     print(f\"\\nStarting Barycentric Optimization at {start_datetime}\")\n",
    "#     print(f\"Maximum iterations: {max_iter}\")\n",
    "#     print(\"Parameters:\")\n",
    "#     print(f\"- Projection method: {projection_method}\")\n",
    "#     print(f\"- Initialization: {initialization}\")\n",
    "#     print(f\"- Tolerance: {tol}\")\n",
    "\n",
    "#     # Create progress bar (100 total steps for all stages)\n",
    "#     with tqdm(total=100, desc=\"Barycentric Optimization\") as pbar:\n",
    "#         try:\n",
    "#             # Run optimization with progress tracking\n",
    "#             tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H = barycentric_optimization(\n",
    "#                 mu_L=mu_L, \n",
    "#                 mu_H=mu_H, \n",
    "#                 Sigma_L=Sigma_L, \n",
    "#                 Sigma_H=Sigma_H,\n",
    "#                 LLmodels=LLmodels, \n",
    "#                 HLmodels=HLmodels, \n",
    "#                 Ill=Ill, \n",
    "#                 Ihl=Ihl,\n",
    "#                 projection_method=projection_method, \n",
    "#                 initialization=initialization, \n",
    "#                 max_iter=max_iter, \n",
    "#                 tol=tol,\n",
    "#                 pbar=pbar\n",
    "#             )\n",
    "            \n",
    "#             # Calculate execution time\n",
    "#             end_time = time.time()\n",
    "#             execution_time = end_time - start_time\n",
    "            \n",
    "#             # Print timing information\n",
    "#             print(\"\\nOptimization Complete!\")\n",
    "#             print(f\"Started at: {start_datetime}\")\n",
    "#             print(f\"Finished at: {datetime.now()}\")\n",
    "#             print(f\"Total execution time: {timedelta(seconds=int(execution_time))}\")\n",
    "            \n",
    "#             # Print final results\n",
    "#             print(\"\\nFinal Results:\")\n",
    "#             print(f\"tau: {tau}\")\n",
    "#             print(f\"T shape: {T.shape}\")\n",
    "#             print(f\"Tp shape: {Tp.shape}\")\n",
    "            \n",
    "#             return tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nError during optimization: {e}\")\n",
    "#             end_time = time.time()\n",
    "#             print(f\"Time until error: {timedelta(seconds=int(end_time - start_time))}\")\n",
    "#             raise\n",
    "\n",
    "# # Usage\n",
    "# tau, T, Tp, mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H = run_barycentric_optimization(\n",
    "#     mu_L, mu_H, Sigma_L, Sigma_H,\n",
    "#     LLmodels, HLmodels, Ill, Ihl,\n",
    "#     projection_method='svd',\n",
    "#     initialization='psd',\n",
    "#     max_iter=100,\n",
    "#     tol=1e-5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18bb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187d073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea3fa7c",
   "metadata": {},
   "source": [
    "Barycentric gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc473f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_min(mu_L, Sigma_L, mu_H, Sigma_H, num_steps, seed, tol=1e-2):\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    T = torch.randn(mu_H.shape[0], mu_L.shape[0], requires_grad=True)\n",
    "\n",
    "    optimizer_T        = torch.optim.Adam([T], lr=0.01)\n",
    "    previous_objective = float('inf')\n",
    "    objective_T        = 0  # Reset objective at the start of each step\n",
    "    # Optimization loop\n",
    "    for step in range(num_steps):\n",
    "        objective_T = 0  # Reset objective at the start of each step\n",
    "\n",
    "        # Calculate each term of the Wasserstein distance\n",
    "        term1 = torch.norm(T @ mu_L - mu_H) ** 2  # Squared Euclidean distance between transformed means\n",
    "        term2 = torch.trace(T @ Sigma_L @ T.T)   # Trace term for low-level covariance\n",
    "        term3 = torch.trace(Sigma_H)             # Trace term for high-level covariance\n",
    "        \n",
    "        # Compute the intermediate covariance matrices\n",
    "        T_Sigma_L_T      = torch.matmul(T, torch.matmul(Sigma_L, T.T))\n",
    "        T_Sigma_L_T_sqrt = oput.sqrtm_svd(T_Sigma_L_T)\n",
    "        Sigma_H_sqrt     = oput.sqrtm_svd(Sigma_H)\n",
    "        \n",
    "        # Coupling term using nuclear norm\n",
    "        term4 = -2 * torch.norm(T_Sigma_L_T_sqrt @ Sigma_H_sqrt, p='nuc')\n",
    "\n",
    "        # Total objective is the sum of terms\n",
    "        objective_T += term1 + term2 + term3 + term4\n",
    "\n",
    "        if abs(previous_objective - objective_T.item()) < tol:\n",
    "            print(f\"Converged at step {step + 1}/{num_steps} with objective: {objective_T.item()}\")\n",
    "            break\n",
    "\n",
    "        # Update previous objective\n",
    "        previous_objective = objective_T.item()\n",
    "\n",
    "        # Perform optimization step\n",
    "        optimizer_T.zero_grad()  # Clear gradients\n",
    "        objective_T.backward(retain_graph=True)  # Backpropagate\n",
    "        optimizer_T.step()  # Update T\n",
    "\n",
    "        print(f\"Min step {step+1}/{num_steps}, Objective: {objective_T.item()}\")\n",
    "\n",
    "    return objective_T.item(), T  # Return final objective and optimized T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92811913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min step 1/1000, Objective: 1.2971632480621338\n",
      "Min step 2/1000, Objective: 1.2380456924438477\n",
      "Min step 3/1000, Objective: 1.180325984954834\n",
      "Min step 4/1000, Objective: 1.124044418334961\n",
      "Min step 5/1000, Objective: 1.0692381858825684\n",
      "Min step 6/1000, Objective: 1.0159425735473633\n",
      "Min step 7/1000, Objective: 0.9641842842102051\n",
      "Min step 8/1000, Objective: 0.9139909744262695\n",
      "Min step 9/1000, Objective: 0.8653814792633057\n",
      "Min step 10/1000, Objective: 0.8183741569519043\n",
      "Min step 11/1000, Objective: 0.7729804515838623\n",
      "Min step 12/1000, Objective: 0.7292122840881348\n",
      "Min step 13/1000, Objective: 0.6870708465576172\n",
      "Min step 14/1000, Objective: 0.6465592384338379\n",
      "Min step 15/1000, Objective: 0.6076755523681641\n",
      "Min step 16/1000, Objective: 0.570408821105957\n",
      "Min step 17/1000, Objective: 0.5347499847412109\n",
      "Min step 18/1000, Objective: 0.5006833076477051\n",
      "Min step 19/1000, Objective: 0.46818971633911133\n",
      "Min step 20/1000, Objective: 0.43724489212036133\n",
      "Min step 21/1000, Objective: 0.4078254699707031\n",
      "Min step 22/1000, Objective: 0.3798971176147461\n",
      "Min step 23/1000, Objective: 0.3534278869628906\n",
      "Min step 24/1000, Objective: 0.3283839225769043\n",
      "Min step 25/1000, Objective: 0.30472373962402344\n",
      "Min step 26/1000, Objective: 0.2824087142944336\n",
      "Min step 27/1000, Objective: 0.26139020919799805\n",
      "Min step 28/1000, Objective: 0.24162817001342773\n",
      "Min step 29/1000, Objective: 0.22307252883911133\n",
      "Min step 30/1000, Objective: 0.20567607879638672\n",
      "Min step 31/1000, Objective: 0.1893911361694336\n",
      "Min step 32/1000, Objective: 0.1741657257080078\n",
      "Min step 33/1000, Objective: 0.15995359420776367\n",
      "Min step 34/1000, Objective: 0.14670276641845703\n",
      "Min step 35/1000, Objective: 0.1343674659729004\n",
      "Min step 36/1000, Objective: 0.12289667129516602\n",
      "Min step 37/1000, Objective: 0.1122431755065918\n",
      "Converged at step 38/1000 with objective: 0.10236310958862305\n",
      "Final Objective: 0.10236310958862305\n",
      "Optimized T: tensor([[ 0.6682, -0.1769,  0.6029],\n",
      "        [ 0.5016, -0.8019,  0.1245]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "num_steps = 1000\n",
    "seed      = 42\n",
    "\n",
    "final_objective, optimized_T = optimize_min(mu_bary_L, Sigma_bary_L, mu_bary_H, Sigma_bary_H, num_steps, seed)\n",
    "\n",
    "print(f\"Final Objective: {final_objective}\")\n",
    "print(f\"Optimized T: {optimized_T}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11a091",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
